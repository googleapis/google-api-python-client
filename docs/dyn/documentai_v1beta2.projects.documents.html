<html><body>
<style>

body, h1, h2, h3, div, span, p, pre, a {
  margin: 0;
  padding: 0;
  border: 0;
  font-weight: inherit;
  font-style: inherit;
  font-size: 100%;
  font-family: inherit;
  vertical-align: baseline;
}

body {
  font-size: 13px;
  padding: 1em;
}

h1 {
  font-size: 26px;
  margin-bottom: 1em;
}

h2 {
  font-size: 24px;
  margin-bottom: 1em;
}

h3 {
  font-size: 20px;
  margin-bottom: 1em;
  margin-top: 1em;
}

pre, code {
  line-height: 1.5;
  font-family: Monaco, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console', monospace;
}

pre {
  margin-top: 0.5em;
}

h1, h2, h3, p {
  font-family: Arial, sans serif;
}

h1, h2, h3 {
  border-bottom: solid #CCC 1px;
}

.toc_element {
  margin-top: 0.5em;
}

.firstline {
  margin-left: 2 em;
}

.method  {
  margin-top: 1em;
  border: solid 1px #CCC;
  padding: 1em;
  background: #EEE;
}

.details {
  font-weight: bold;
  font-size: 14px;
}

</style>

<h1><a href="documentai_v1beta2.html">Cloud Document AI API</a> . <a href="documentai_v1beta2.projects.html">projects</a> . <a href="documentai_v1beta2.projects.documents.html">documents</a></h1>
<h2>Instance Methods</h2>
<p class="toc_element">
  <code><a href="#batchProcess">batchProcess(parent, body=None, x__xgafv=None)</a></code></p>
<p class="firstline">LRO endpoint to batch process many documents. The output is written</p>
<p class="toc_element">
  <code><a href="#process">process(parent, body=None, x__xgafv=None)</a></code></p>
<p class="firstline">Processes a single document.</p>
<h3>Method Details</h3>
<div class="method">
    <code class="details" id="batchProcess">batchProcess(parent, body=None, x__xgafv=None)</code>
  <pre>LRO endpoint to batch process many documents. The output is written
to Cloud Storage as JSON in the [Document] format.

Args:
  parent: string, Target project and location to make a call.

Format: `projects/{project-id}/locations/{location-id}`.

If no location is specified, a region will be chosen automatically. (required)
  body: object, The request body.
    The object takes the form of:

{ # Request to batch process documents as an asynchronous operation. The output
      # is written to Cloud Storage as JSON in the [Document] format.
    &quot;requests&quot;: [ # Required. Individual requests for each document.
      { # Request to process one document.
          &quot;entityExtractionParams&quot;: { # Parameters to control entity extraction behavior. # Controls entity extraction behavior. If not specified, the system will
              # decide reasonable defaults.
            &quot;modelVersion&quot;: &quot;A String&quot;, # Model version of the entity extraction. Default is
                # &quot;builtin/stable&quot;. Specify &quot;builtin/latest&quot; for the latest model.
            &quot;enabled&quot;: True or False, # Whether to enable entity extraction.
          },
          &quot;documentType&quot;: &quot;A String&quot;, # Specifies a known document type for deeper structure detection. Valid
              # values are currently &quot;general&quot; and &quot;invoice&quot;. If not provided, &quot;general&quot;\
              # is used as default. If any other value is given, the request is rejected.
          &quot;outputConfig&quot;: { # The desired output location and metadata. # The desired output location. This field is only needed in
              # BatchProcessDocumentsRequest.
            &quot;pagesPerShard&quot;: 42, # The max number of pages to include into each output Document shard JSON on
                # Google Cloud Storage.
                #
                # The valid range is [1, 100]. If not specified, the default value is 20.
                #
                # For example, for one pdf file with 100 pages, 100 parsed pages will be
                # produced. If `pages_per_shard` = 20, then 5 Document shard JSON files each
                # containing 20 parsed pages will be written under the prefix
                # OutputConfig.gcs_destination.uri and suffix pages-x-to-y.json where
                # x and y are 1-indexed page numbers.
                #
                # Example GCS outputs with 157 pages and pages_per_shard = 50:
                #
                # &lt;prefix&gt;pages-001-to-050.json
                # &lt;prefix&gt;pages-051-to-100.json
                # &lt;prefix&gt;pages-101-to-150.json
                # &lt;prefix&gt;pages-151-to-157.json
            &quot;gcsDestination&quot;: { # The Google Cloud Storage location where the output file will be written to. # The Google Cloud Storage location to write the output to.
              &quot;uri&quot;: &quot;A String&quot;,
            },
          },
          &quot;automlParams&quot;: { # Parameters to control AutoML model prediction behavior. # Controls AutoML model prediction behavior. AutoMlParams cannot be used
              # together with other Params.
            &quot;model&quot;: &quot;A String&quot;, # Resource name of the AutoML model.
                #
                # Format: `projects/{project-id}/locations/{location-id}/models/{model-id}`.
          },
          &quot;ocrParams&quot;: { # Parameters to control Optical Character Recognition (OCR) behavior. # Controls OCR behavior. If not specified, the system will decide reasonable
              # defaults.
            &quot;languageHints&quot;: [ # List of languages to use for OCR. In most cases, an empty value
                # yields the best results since it enables automatic language detection. For
                # languages based on the Latin alphabet, setting `language_hints` is not
                # needed. In rare cases, when the language of the text in the image is known,
                # setting a hint will help get better results (although it will be a
                # significant hindrance if the hint is wrong). Document processing returns an
                # error if one or more of the specified languages is not one of the
                # supported languages.
              &quot;A String&quot;,
            ],
          },
          &quot;inputConfig&quot;: { # The desired input location and metadata. # Required. Information about the input file.
            &quot;gcsSource&quot;: { # The Google Cloud Storage location where the input file will be read from. # The Google Cloud Storage location to read the input from. This must be a
                # single file.
              &quot;uri&quot;: &quot;A String&quot;,
            },
            &quot;mimeType&quot;: &quot;A String&quot;, # Required. Mimetype of the input. Current supported mimetypes are application/pdf,
                # image/tiff, and image/gif.
                # In addition, application/json type is supported for requests with
                # ProcessDocumentRequest.automl_params field set. The JSON file needs to
                # be in Document format.
            &quot;contents&quot;: &quot;A String&quot;, # Content in bytes, represented as a stream of bytes.
                # Note: As with all `bytes` fields, proto buffer messages use a pure binary
                # representation, whereas JSON representations use base64.
                #
                # This field only works for synchronous ProcessDocument method.
          },
          &quot;tableExtractionParams&quot;: { # Parameters to control table extraction behavior. # Controls table extraction behavior. If not specified, the system will
              # decide reasonable defaults.
            &quot;enabled&quot;: True or False, # Whether to enable table extraction.
            &quot;headerHints&quot;: [ # Optional. Reserved for future use.
              &quot;A String&quot;,
            ],
            &quot;tableBoundHints&quot;: [ # Optional. Table bounding box hints that can be provided to complex cases
                # which our algorithm cannot locate the table(s) in.
              { # A hint for a table bounding box on the page for table parsing.
                &quot;pageNumber&quot;: 42, # Optional. Page number for multi-paged inputs this hint applies to. If not
                    # provided, this hint will apply to all pages by default. This value is
                    # 1-based.
                &quot;boundingBox&quot;: { # A bounding polygon for the detected image annotation. # Bounding box hint for a table on this page. The coordinates must be
                    # normalized to [0,1] and the bounding box must be an axis-aligned rectangle.
                  &quot;vertices&quot;: [ # The bounding polygon vertices.
                    { # A vertex represents a 2D point in the image.
                        # NOTE: the vertex coordinates are in the same scale as the original image.
                      &quot;x&quot;: 42, # X coordinate.
                      &quot;y&quot;: 42, # Y coordinate.
                    },
                  ],
                  &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                    { # A vertex represents a 2D point in the image.
                        # NOTE: the normalized vertex coordinates are relative to the original image
                        # and range from 0 to 1.
                      &quot;y&quot;: 3.14, # Y coordinate.
                      &quot;x&quot;: 3.14, # X coordinate.
                    },
                  ],
                },
              },
            ],
            &quot;modelVersion&quot;: &quot;A String&quot;, # Model version of the table extraction system. Default is &quot;builtin/stable&quot;.
                # Specify &quot;builtin/latest&quot; for the latest model.
          },
          &quot;parent&quot;: &quot;A String&quot;, # Target project and location to make a call.
              #
              # Format: `projects/{project-id}/locations/{location-id}`.
              #
              # If no location is specified, a region will be chosen automatically.
              # This field is only populated when used in ProcessDocument method.
          &quot;formExtractionParams&quot;: { # Parameters to control form extraction behavior. # Controls form extraction behavior. If not specified, the system will
              # decide reasonable defaults.
            &quot;keyValuePairHints&quot;: [ # Reserved for future use.
              { # Reserved for future use.
                &quot;key&quot;: &quot;A String&quot;, # The key text for the hint.
                &quot;valueTypes&quot;: [ # Type of the value. This is case-insensitive, and could be one of:
                    # ADDRESS, LOCATION, ORGANIZATION, PERSON, PHONE_NUMBER,
                    # ID, NUMBER, EMAIL, PRICE, TERMS, DATE, NAME. Types not in this list will
                    # be ignored.
                  &quot;A String&quot;,
                ],
              },
            ],
            &quot;modelVersion&quot;: &quot;A String&quot;, # Model version of the form extraction system. Default is
                # &quot;builtin/stable&quot;. Specify &quot;builtin/latest&quot; for the latest model.
                # For custom form models, specify: “custom/{model_name}&quot;. Model name
                # format is &quot;bucket_name/path/to/modeldir&quot; corresponding to
                # &quot;gs://bucket_name/path/to/modeldir&quot; where annotated examples are stored.
            &quot;enabled&quot;: True or False, # Whether to enable form extraction.
          },
        },
    ],
  }

  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # This resource represents a long-running operation that is the result of a
      # network API call.
    &quot;error&quot;: { # The `Status` type defines a logical error model that is suitable for # The error result of the operation in case of failure or cancellation.
        # different programming environments, including REST APIs and RPC APIs. It is
        # used by [gRPC](https://github.com/grpc). Each `Status` message contains
        # three pieces of data: error code, error message, and error details.
        #
        # You can find out more about this error model and how to work with it in the
        # [API Design Guide](https://cloud.google.com/apis/design/errors).
      &quot;code&quot;: 42, # The status code, which should be an enum value of google.rpc.Code.
      &quot;message&quot;: &quot;A String&quot;, # A developer-facing error message, which should be in English. Any
          # user-facing error message should be localized and sent in the
          # google.rpc.Status.details field, or localized by the client.
      &quot;details&quot;: [ # A list of messages that carry the error details.  There is a common set of
          # message types for APIs to use.
        {
          &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
        },
      ],
    },
    &quot;response&quot;: { # The normal response of the operation in case of success.  If the original
        # method returns no data on success, such as `Delete`, the response is
        # `google.protobuf.Empty`.  If the original method is standard
        # `Get`/`Create`/`Update`, the response should be the resource.  For other
        # methods, the response should have the type `XxxResponse`, where `Xxx`
        # is the original method name.  For example, if the original method name
        # is `TakeSnapshot()`, the inferred response type is
        # `TakeSnapshotResponse`.
      &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
    },
    &quot;metadata&quot;: { # Service-specific metadata associated with the operation.  It typically
        # contains progress information and common metadata such as create time.
        # Some services might not provide such metadata.  Any method that returns a
        # long-running operation should document the metadata type, if any.
      &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
    },
    &quot;done&quot;: True or False, # If the value is `false`, it means the operation is still in progress.
        # If `true`, the operation is completed, and either `error` or `response` is
        # available.
    &quot;name&quot;: &quot;A String&quot;, # The server-assigned name, which is only unique within the same service that
        # originally returns it. If you use the default HTTP mapping, the
        # `name` should be a resource name ending with `operations/{unique_id}`.
  }</pre>
</div>

<div class="method">
    <code class="details" id="process">process(parent, body=None, x__xgafv=None)</code>
  <pre>Processes a single document.

Args:
  parent: string, Target project and location to make a call.

Format: `projects/{project-id}/locations/{location-id}`.

If no location is specified, a region will be chosen automatically.
This field is only populated when used in ProcessDocument method. (required)
  body: object, The request body.
    The object takes the form of:

{ # Request to process one document.
    &quot;entityExtractionParams&quot;: { # Parameters to control entity extraction behavior. # Controls entity extraction behavior. If not specified, the system will
        # decide reasonable defaults.
      &quot;modelVersion&quot;: &quot;A String&quot;, # Model version of the entity extraction. Default is
          # &quot;builtin/stable&quot;. Specify &quot;builtin/latest&quot; for the latest model.
      &quot;enabled&quot;: True or False, # Whether to enable entity extraction.
    },
    &quot;documentType&quot;: &quot;A String&quot;, # Specifies a known document type for deeper structure detection. Valid
        # values are currently &quot;general&quot; and &quot;invoice&quot;. If not provided, &quot;general&quot;\
        # is used as default. If any other value is given, the request is rejected.
    &quot;outputConfig&quot;: { # The desired output location and metadata. # The desired output location. This field is only needed in
        # BatchProcessDocumentsRequest.
      &quot;pagesPerShard&quot;: 42, # The max number of pages to include into each output Document shard JSON on
          # Google Cloud Storage.
          #
          # The valid range is [1, 100]. If not specified, the default value is 20.
          #
          # For example, for one pdf file with 100 pages, 100 parsed pages will be
          # produced. If `pages_per_shard` = 20, then 5 Document shard JSON files each
          # containing 20 parsed pages will be written under the prefix
          # OutputConfig.gcs_destination.uri and suffix pages-x-to-y.json where
          # x and y are 1-indexed page numbers.
          #
          # Example GCS outputs with 157 pages and pages_per_shard = 50:
          #
          # &lt;prefix&gt;pages-001-to-050.json
          # &lt;prefix&gt;pages-051-to-100.json
          # &lt;prefix&gt;pages-101-to-150.json
          # &lt;prefix&gt;pages-151-to-157.json
      &quot;gcsDestination&quot;: { # The Google Cloud Storage location where the output file will be written to. # The Google Cloud Storage location to write the output to.
        &quot;uri&quot;: &quot;A String&quot;,
      },
    },
    &quot;automlParams&quot;: { # Parameters to control AutoML model prediction behavior. # Controls AutoML model prediction behavior. AutoMlParams cannot be used
        # together with other Params.
      &quot;model&quot;: &quot;A String&quot;, # Resource name of the AutoML model.
          #
          # Format: `projects/{project-id}/locations/{location-id}/models/{model-id}`.
    },
    &quot;ocrParams&quot;: { # Parameters to control Optical Character Recognition (OCR) behavior. # Controls OCR behavior. If not specified, the system will decide reasonable
        # defaults.
      &quot;languageHints&quot;: [ # List of languages to use for OCR. In most cases, an empty value
          # yields the best results since it enables automatic language detection. For
          # languages based on the Latin alphabet, setting `language_hints` is not
          # needed. In rare cases, when the language of the text in the image is known,
          # setting a hint will help get better results (although it will be a
          # significant hindrance if the hint is wrong). Document processing returns an
          # error if one or more of the specified languages is not one of the
          # supported languages.
        &quot;A String&quot;,
      ],
    },
    &quot;inputConfig&quot;: { # The desired input location and metadata. # Required. Information about the input file.
      &quot;gcsSource&quot;: { # The Google Cloud Storage location where the input file will be read from. # The Google Cloud Storage location to read the input from. This must be a
          # single file.
        &quot;uri&quot;: &quot;A String&quot;,
      },
      &quot;mimeType&quot;: &quot;A String&quot;, # Required. Mimetype of the input. Current supported mimetypes are application/pdf,
          # image/tiff, and image/gif.
          # In addition, application/json type is supported for requests with
          # ProcessDocumentRequest.automl_params field set. The JSON file needs to
          # be in Document format.
      &quot;contents&quot;: &quot;A String&quot;, # Content in bytes, represented as a stream of bytes.
          # Note: As with all `bytes` fields, proto buffer messages use a pure binary
          # representation, whereas JSON representations use base64.
          #
          # This field only works for synchronous ProcessDocument method.
    },
    &quot;tableExtractionParams&quot;: { # Parameters to control table extraction behavior. # Controls table extraction behavior. If not specified, the system will
        # decide reasonable defaults.
      &quot;enabled&quot;: True or False, # Whether to enable table extraction.
      &quot;headerHints&quot;: [ # Optional. Reserved for future use.
        &quot;A String&quot;,
      ],
      &quot;tableBoundHints&quot;: [ # Optional. Table bounding box hints that can be provided to complex cases
          # which our algorithm cannot locate the table(s) in.
        { # A hint for a table bounding box on the page for table parsing.
          &quot;pageNumber&quot;: 42, # Optional. Page number for multi-paged inputs this hint applies to. If not
              # provided, this hint will apply to all pages by default. This value is
              # 1-based.
          &quot;boundingBox&quot;: { # A bounding polygon for the detected image annotation. # Bounding box hint for a table on this page. The coordinates must be
              # normalized to [0,1] and the bounding box must be an axis-aligned rectangle.
            &quot;vertices&quot;: [ # The bounding polygon vertices.
              { # A vertex represents a 2D point in the image.
                  # NOTE: the vertex coordinates are in the same scale as the original image.
                &quot;x&quot;: 42, # X coordinate.
                &quot;y&quot;: 42, # Y coordinate.
              },
            ],
            &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
              { # A vertex represents a 2D point in the image.
                  # NOTE: the normalized vertex coordinates are relative to the original image
                  # and range from 0 to 1.
                &quot;y&quot;: 3.14, # Y coordinate.
                &quot;x&quot;: 3.14, # X coordinate.
              },
            ],
          },
        },
      ],
      &quot;modelVersion&quot;: &quot;A String&quot;, # Model version of the table extraction system. Default is &quot;builtin/stable&quot;.
          # Specify &quot;builtin/latest&quot; for the latest model.
    },
    &quot;parent&quot;: &quot;A String&quot;, # Target project and location to make a call.
        # 
        # Format: `projects/{project-id}/locations/{location-id}`.
        # 
        # If no location is specified, a region will be chosen automatically.
        # This field is only populated when used in ProcessDocument method.
    &quot;formExtractionParams&quot;: { # Parameters to control form extraction behavior. # Controls form extraction behavior. If not specified, the system will
        # decide reasonable defaults.
      &quot;keyValuePairHints&quot;: [ # Reserved for future use.
        { # Reserved for future use.
          &quot;key&quot;: &quot;A String&quot;, # The key text for the hint.
          &quot;valueTypes&quot;: [ # Type of the value. This is case-insensitive, and could be one of:
              # ADDRESS, LOCATION, ORGANIZATION, PERSON, PHONE_NUMBER,
              # ID, NUMBER, EMAIL, PRICE, TERMS, DATE, NAME. Types not in this list will
              # be ignored.
            &quot;A String&quot;,
          ],
        },
      ],
      &quot;modelVersion&quot;: &quot;A String&quot;, # Model version of the form extraction system. Default is
          # &quot;builtin/stable&quot;. Specify &quot;builtin/latest&quot; for the latest model.
          # For custom form models, specify: “custom/{model_name}&quot;. Model name
          # format is &quot;bucket_name/path/to/modeldir&quot; corresponding to
          # &quot;gs://bucket_name/path/to/modeldir&quot; where annotated examples are stored.
      &quot;enabled&quot;: True or False, # Whether to enable form extraction.
    },
  }

  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # Document represents the canonical document resource in Document Understanding
      # AI.
      # It is an interchange format that provides insights into documents and allows
      # for collaboration between users and Document Understanding AI to iterate and
      # optimize for quality.
    &quot;translations&quot;: [ # A list of translations on Document.text. For document shards,
        # translations in this list may cross shard boundaries.
      { # A translation of the text segment.
        &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
            # information, see
            # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
        &quot;translatedText&quot;: &quot;A String&quot;, # Text translated into the target language.
        &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Provenance of the translation.
            # Text anchor indexing into the Document.text.
          &quot;textSegments&quot;: [ # The text segments from the Document.text.
            { # A text segment in the Document.text. The indices may be out of bounds
                # which indicate that the text extends into another document shard for
                # large sharded documents. See ShardInfo.text_offset
              &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                  # Document.text.
              &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
            },
          ],
        },
      },
    ],
    &quot;pages&quot;: [ # Visual page layout for the Document.
      { # A page in a Document.
        &quot;lines&quot;: [ # A list of visually detected text lines on the page.
            # A collection of tokens that a human would perceive as a line.
          { # A collection of tokens that a human would perceive as a line.
              # Does not cross column boundaries, can be horizontal, vertical, etc.
            &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for Line.
              &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                &quot;textSegments&quot;: [ # The text segments from the Document.text.
                  { # A text segment in the Document.text. The indices may be out of bounds
                      # which indicate that the text extends into another document shard for
                      # large sharded documents. See ShardInfo.text_offset
                    &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                        # Document.text.
                    &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                  },
                ],
              },
              &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                  # layout is for. e.g. confidence can be for a single token, a table,
                  # a visual element, etc. depending on context. Range [0, 1].
              &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                &quot;vertices&quot;: [ # The bounding polygon vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the vertex coordinates are in the same scale as the original image.
                    &quot;x&quot;: 42, # X coordinate.
                    &quot;y&quot;: 42, # Y coordinate.
                  },
                ],
                &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the normalized vertex coordinates are relative to the original image
                      # and range from 0 to 1.
                    &quot;y&quot;: 3.14, # Y coordinate.
                    &quot;x&quot;: 3.14, # X coordinate.
                  },
                ],
              },
              &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
            },
            &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
              { # Detected language for a structural component.
                &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                    # information, see
                    # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
              },
            ],
          },
        ],
        &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for the page.
          &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
            &quot;textSegments&quot;: [ # The text segments from the Document.text.
              { # A text segment in the Document.text. The indices may be out of bounds
                  # which indicate that the text extends into another document shard for
                  # large sharded documents. See ShardInfo.text_offset
                &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                    # Document.text.
                &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
              },
            ],
          },
          &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
              # layout is for. e.g. confidence can be for a single token, a table,
              # a visual element, etc. depending on context. Range [0, 1].
          &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
            &quot;vertices&quot;: [ # The bounding polygon vertices.
              { # A vertex represents a 2D point in the image.
                  # NOTE: the vertex coordinates are in the same scale as the original image.
                &quot;x&quot;: 42, # X coordinate.
                &quot;y&quot;: 42, # Y coordinate.
              },
            ],
            &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
              { # A vertex represents a 2D point in the image.
                  # NOTE: the normalized vertex coordinates are relative to the original image
                  # and range from 0 to 1.
                &quot;y&quot;: 3.14, # Y coordinate.
                &quot;x&quot;: 3.14, # X coordinate.
              },
            ],
          },
          &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
        },
        &quot;visualElements&quot;: [ # A list of detected non-text visual elements e.g. checkbox,
            # signature etc. on the page.
          { # Detected non-text visual elements e.g. checkbox, signature etc. on the
              # page.
            &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
              { # Detected language for a structural component.
                &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                    # information, see
                    # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
              },
            ],
            &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for VisualElement.
              &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                &quot;textSegments&quot;: [ # The text segments from the Document.text.
                  { # A text segment in the Document.text. The indices may be out of bounds
                      # which indicate that the text extends into another document shard for
                      # large sharded documents. See ShardInfo.text_offset
                    &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                        # Document.text.
                    &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                  },
                ],
              },
              &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                  # layout is for. e.g. confidence can be for a single token, a table,
                  # a visual element, etc. depending on context. Range [0, 1].
              &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                &quot;vertices&quot;: [ # The bounding polygon vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the vertex coordinates are in the same scale as the original image.
                    &quot;x&quot;: 42, # X coordinate.
                    &quot;y&quot;: 42, # Y coordinate.
                  },
                ],
                &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the normalized vertex coordinates are relative to the original image
                      # and range from 0 to 1.
                    &quot;y&quot;: 3.14, # Y coordinate.
                    &quot;x&quot;: 3.14, # X coordinate.
                  },
                ],
              },
              &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
            },
            &quot;type&quot;: &quot;A String&quot;, # Type of the VisualElement.
          },
        ],
        &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
          { # Detected language for a structural component.
            &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                # information, see
                # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
            &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
          },
        ],
        &quot;blocks&quot;: [ # A list of visually detected text blocks on the page.
            # A block has a set of lines (collected into paragraphs) that have a common
            # line-spacing and orientation.
          { # A block has a set of lines (collected into paragraphs) that have a
              # common line-spacing and orientation.
            &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
              { # Detected language for a structural component.
                &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                    # information, see
                    # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
              },
            ],
            &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for Block.
              &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                &quot;textSegments&quot;: [ # The text segments from the Document.text.
                  { # A text segment in the Document.text. The indices may be out of bounds
                      # which indicate that the text extends into another document shard for
                      # large sharded documents. See ShardInfo.text_offset
                    &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                        # Document.text.
                    &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                  },
                ],
              },
              &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                  # layout is for. e.g. confidence can be for a single token, a table,
                  # a visual element, etc. depending on context. Range [0, 1].
              &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                &quot;vertices&quot;: [ # The bounding polygon vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the vertex coordinates are in the same scale as the original image.
                    &quot;x&quot;: 42, # X coordinate.
                    &quot;y&quot;: 42, # Y coordinate.
                  },
                ],
                &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the normalized vertex coordinates are relative to the original image
                      # and range from 0 to 1.
                    &quot;y&quot;: 3.14, # Y coordinate.
                    &quot;x&quot;: 3.14, # X coordinate.
                  },
                ],
              },
              &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
            },
          },
        ],
        &quot;tables&quot;: [ # A list of visually detected tables on the page.
          { # A table representation similar to HTML table structure.
            &quot;headerRows&quot;: [ # Header rows of the table.
              { # A row of table cells.
                &quot;cells&quot;: [ # Cells that make up this row.
                  { # A cell representation inside the table.
                    &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
                      { # Detected language for a structural component.
                        &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                            # information, see
                            # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                        &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
                      },
                    ],
                    &quot;rowSpan&quot;: 42, # How many rows this cell spans.
                    &quot;colSpan&quot;: 42, # How many columns this cell spans.
                    &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for TableCell.
                      &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                        &quot;textSegments&quot;: [ # The text segments from the Document.text.
                          { # A text segment in the Document.text. The indices may be out of bounds
                              # which indicate that the text extends into another document shard for
                              # large sharded documents. See ShardInfo.text_offset
                            &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                                # Document.text.
                            &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                          },
                        ],
                      },
                      &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                          # layout is for. e.g. confidence can be for a single token, a table,
                          # a visual element, etc. depending on context. Range [0, 1].
                      &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                        &quot;vertices&quot;: [ # The bounding polygon vertices.
                          { # A vertex represents a 2D point in the image.
                              # NOTE: the vertex coordinates are in the same scale as the original image.
                            &quot;x&quot;: 42, # X coordinate.
                            &quot;y&quot;: 42, # Y coordinate.
                          },
                        ],
                        &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                          { # A vertex represents a 2D point in the image.
                              # NOTE: the normalized vertex coordinates are relative to the original image
                              # and range from 0 to 1.
                            &quot;y&quot;: 3.14, # Y coordinate.
                            &quot;x&quot;: 3.14, # X coordinate.
                          },
                        ],
                      },
                      &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
                    },
                  },
                ],
              },
            ],
            &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
              { # Detected language for a structural component.
                &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                    # information, see
                    # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
              },
            ],
            &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for Table.
              &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                &quot;textSegments&quot;: [ # The text segments from the Document.text.
                  { # A text segment in the Document.text. The indices may be out of bounds
                      # which indicate that the text extends into another document shard for
                      # large sharded documents. See ShardInfo.text_offset
                    &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                        # Document.text.
                    &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                  },
                ],
              },
              &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                  # layout is for. e.g. confidence can be for a single token, a table,
                  # a visual element, etc. depending on context. Range [0, 1].
              &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                &quot;vertices&quot;: [ # The bounding polygon vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the vertex coordinates are in the same scale as the original image.
                    &quot;x&quot;: 42, # X coordinate.
                    &quot;y&quot;: 42, # Y coordinate.
                  },
                ],
                &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the normalized vertex coordinates are relative to the original image
                      # and range from 0 to 1.
                    &quot;y&quot;: 3.14, # Y coordinate.
                    &quot;x&quot;: 3.14, # X coordinate.
                  },
                ],
              },
              &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
            },
            &quot;bodyRows&quot;: [ # Body rows of the table.
              { # A row of table cells.
                &quot;cells&quot;: [ # Cells that make up this row.
                  { # A cell representation inside the table.
                    &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
                      { # Detected language for a structural component.
                        &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                            # information, see
                            # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                        &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
                      },
                    ],
                    &quot;rowSpan&quot;: 42, # How many rows this cell spans.
                    &quot;colSpan&quot;: 42, # How many columns this cell spans.
                    &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for TableCell.
                      &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                        &quot;textSegments&quot;: [ # The text segments from the Document.text.
                          { # A text segment in the Document.text. The indices may be out of bounds
                              # which indicate that the text extends into another document shard for
                              # large sharded documents. See ShardInfo.text_offset
                            &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                                # Document.text.
                            &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                          },
                        ],
                      },
                      &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                          # layout is for. e.g. confidence can be for a single token, a table,
                          # a visual element, etc. depending on context. Range [0, 1].
                      &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                        &quot;vertices&quot;: [ # The bounding polygon vertices.
                          { # A vertex represents a 2D point in the image.
                              # NOTE: the vertex coordinates are in the same scale as the original image.
                            &quot;x&quot;: 42, # X coordinate.
                            &quot;y&quot;: 42, # Y coordinate.
                          },
                        ],
                        &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                          { # A vertex represents a 2D point in the image.
                              # NOTE: the normalized vertex coordinates are relative to the original image
                              # and range from 0 to 1.
                            &quot;y&quot;: 3.14, # Y coordinate.
                            &quot;x&quot;: 3.14, # X coordinate.
                          },
                        ],
                      },
                      &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
                    },
                  },
                ],
              },
            ],
          },
        ],
        &quot;dimension&quot;: { # Dimension for the page. # Physical dimension of the page.
          &quot;unit&quot;: &quot;A String&quot;, # Dimension unit.
          &quot;height&quot;: 3.14, # Page height.
          &quot;width&quot;: 3.14, # Page width.
        },
        &quot;tokens&quot;: [ # A list of visually detected tokens on the page.
          { # A detected token.
            &quot;detectedBreak&quot;: { # Detected break at the end of a Token. # Detected break at the end of a Token.
              &quot;type&quot;: &quot;A String&quot;, # Detected break type.
            },
            &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
              { # Detected language for a structural component.
                &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                    # information, see
                    # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
              },
            ],
            &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for Token.
              &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                &quot;textSegments&quot;: [ # The text segments from the Document.text.
                  { # A text segment in the Document.text. The indices may be out of bounds
                      # which indicate that the text extends into another document shard for
                      # large sharded documents. See ShardInfo.text_offset
                    &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                        # Document.text.
                    &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                  },
                ],
              },
              &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                  # layout is for. e.g. confidence can be for a single token, a table,
                  # a visual element, etc. depending on context. Range [0, 1].
              &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                &quot;vertices&quot;: [ # The bounding polygon vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the vertex coordinates are in the same scale as the original image.
                    &quot;x&quot;: 42, # X coordinate.
                    &quot;y&quot;: 42, # Y coordinate.
                  },
                ],
                &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the normalized vertex coordinates are relative to the original image
                      # and range from 0 to 1.
                    &quot;y&quot;: 3.14, # Y coordinate.
                    &quot;x&quot;: 3.14, # X coordinate.
                  },
                ],
              },
              &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
            },
          },
        ],
        &quot;formFields&quot;: [ # A list of visually detected form fields on the page.
          { # A form field detected on the page.
            &quot;valueDetectedLanguages&quot;: [ # A list of detected languages for value together with confidence.
              { # Detected language for a structural component.
                &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                    # information, see
                    # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
              },
            ],
            &quot;valueType&quot;: &quot;A String&quot;, # If the value is non-textual, this field represents the type. Current
                # valid values are:
                # - blank (this indicates the field_value is normal text)
                # - &quot;unfilled_checkbox&quot;
                # - &quot;filled_checkbox&quot;
            &quot;nameDetectedLanguages&quot;: [ # A list of detected languages for name together with confidence.
              { # Detected language for a structural component.
                &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                    # information, see
                    # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
              },
            ],
            &quot;fieldName&quot;: { # Visual element describing a layout unit on a page. # Layout for the FormField name. e.g. `Address`, `Email`,
                # `Grand total`, `Phone number`, etc.
              &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                &quot;textSegments&quot;: [ # The text segments from the Document.text.
                  { # A text segment in the Document.text. The indices may be out of bounds
                      # which indicate that the text extends into another document shard for
                      # large sharded documents. See ShardInfo.text_offset
                    &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                        # Document.text.
                    &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                  },
                ],
              },
              &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                  # layout is for. e.g. confidence can be for a single token, a table,
                  # a visual element, etc. depending on context. Range [0, 1].
              &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                &quot;vertices&quot;: [ # The bounding polygon vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the vertex coordinates are in the same scale as the original image.
                    &quot;x&quot;: 42, # X coordinate.
                    &quot;y&quot;: 42, # Y coordinate.
                  },
                ],
                &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the normalized vertex coordinates are relative to the original image
                      # and range from 0 to 1.
                    &quot;y&quot;: 3.14, # Y coordinate.
                    &quot;x&quot;: 3.14, # X coordinate.
                  },
                ],
              },
              &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
            },
            &quot;fieldValue&quot;: { # Visual element describing a layout unit on a page. # Layout for the FormField value.
              &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                &quot;textSegments&quot;: [ # The text segments from the Document.text.
                  { # A text segment in the Document.text. The indices may be out of bounds
                      # which indicate that the text extends into another document shard for
                      # large sharded documents. See ShardInfo.text_offset
                    &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                        # Document.text.
                    &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                  },
                ],
              },
              &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                  # layout is for. e.g. confidence can be for a single token, a table,
                  # a visual element, etc. depending on context. Range [0, 1].
              &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                &quot;vertices&quot;: [ # The bounding polygon vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the vertex coordinates are in the same scale as the original image.
                    &quot;x&quot;: 42, # X coordinate.
                    &quot;y&quot;: 42, # Y coordinate.
                  },
                ],
                &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the normalized vertex coordinates are relative to the original image
                      # and range from 0 to 1.
                    &quot;y&quot;: 3.14, # Y coordinate.
                    &quot;x&quot;: 3.14, # X coordinate.
                  },
                ],
              },
              &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
            },
          },
        ],
        &quot;pageNumber&quot;: 42, # 1-based index for current Page in a parent Document.
            # Useful when a page is taken out of a Document for individual
            # processing.
        &quot;paragraphs&quot;: [ # A list of visually detected text paragraphs on the page.
            # A collection of lines that a human would perceive as a paragraph.
          { # A collection of lines that a human would perceive as a paragraph.
            &quot;detectedLanguages&quot;: [ # A list of detected languages together with confidence.
              { # Detected language for a structural component.
                &quot;languageCode&quot;: &quot;A String&quot;, # The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more
                    # information, see
                    # http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
                &quot;confidence&quot;: 3.14, # Confidence of detected language. Range [0, 1].
              },
            ],
            &quot;layout&quot;: { # Visual element describing a layout unit on a page. # Layout for Paragraph.
              &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
                &quot;textSegments&quot;: [ # The text segments from the Document.text.
                  { # A text segment in the Document.text. The indices may be out of bounds
                      # which indicate that the text extends into another document shard for
                      # large sharded documents. See ShardInfo.text_offset
                    &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                        # Document.text.
                    &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
                  },
                ],
              },
              &quot;confidence&quot;: 3.14, # Confidence of the current Layout within context of the object this
                  # layout is for. e.g. confidence can be for a single token, a table,
                  # a visual element, etc. depending on context. Range [0, 1].
              &quot;boundingPoly&quot;: { # A bounding polygon for the detected image annotation. # The bounding polygon for the Layout.
                &quot;vertices&quot;: [ # The bounding polygon vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the vertex coordinates are in the same scale as the original image.
                    &quot;x&quot;: 42, # X coordinate.
                    &quot;y&quot;: 42, # Y coordinate.
                  },
                ],
                &quot;normalizedVertices&quot;: [ # The bounding polygon normalized vertices.
                  { # A vertex represents a 2D point in the image.
                      # NOTE: the normalized vertex coordinates are relative to the original image
                      # and range from 0 to 1.
                    &quot;y&quot;: 3.14, # Y coordinate.
                    &quot;x&quot;: 3.14, # X coordinate.
                  },
                ],
              },
              &quot;orientation&quot;: &quot;A String&quot;, # Detected orientation for the Layout.
            },
          },
        ],
      },
    ],
    &quot;error&quot;: { # The `Status` type defines a logical error model that is suitable for # Any error that occurred while processing this document.
        # different programming environments, including REST APIs and RPC APIs. It is
        # used by [gRPC](https://github.com/grpc). Each `Status` message contains
        # three pieces of data: error code, error message, and error details.
        #
        # You can find out more about this error model and how to work with it in the
        # [API Design Guide](https://cloud.google.com/apis/design/errors).
      &quot;code&quot;: 42, # The status code, which should be an enum value of google.rpc.Code.
      &quot;message&quot;: &quot;A String&quot;, # A developer-facing error message, which should be in English. Any
          # user-facing error message should be localized and sent in the
          # google.rpc.Status.details field, or localized by the client.
      &quot;details&quot;: [ # A list of messages that carry the error details.  There is a common set of
          # message types for APIs to use.
        {
          &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
        },
      ],
    },
    &quot;entityRelations&quot;: [ # Relationship among Document.entities.
      { # Relationship between Entities.
        &quot;subjectId&quot;: &quot;A String&quot;, # Subject entity id.
        &quot;objectId&quot;: &quot;A String&quot;, # Object entity id.
        &quot;relation&quot;: &quot;A String&quot;, # Relationship description.
      },
    ],
    &quot;entities&quot;: [ # A list of entities detected on Document.text. For document shards,
        # entities in this list may cross shard boundaries.
      { # A phrase in the text that is a known entity type, such as a person, an
          # organization, or location.
        &quot;type&quot;: &quot;A String&quot;, # Entity type from a schema e.g. `Address`.
        &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Provenance of the entity.
            # Text anchor indexing into the Document.text.
          &quot;textSegments&quot;: [ # The text segments from the Document.text.
            { # A text segment in the Document.text. The indices may be out of bounds
                # which indicate that the text extends into another document shard for
                # large sharded documents. See ShardInfo.text_offset
              &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                  # Document.text.
              &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
            },
          ],
        },
        &quot;confidence&quot;: 3.14, # Optional. Confidence of detected Schema entity. Range [0, 1].
        &quot;mentionId&quot;: &quot;A String&quot;, # Deprecated.  Use `id` field instead.
        &quot;mentionText&quot;: &quot;A String&quot;, # Text value in the document e.g. `1600 Amphitheatre Pkwy`.
      },
    ],
    &quot;content&quot;: &quot;A String&quot;, # Inline document content, represented as a stream of bytes.
        # Note: As with all `bytes` fields, protobuffers use a pure binary
        # representation, whereas JSON representations use base64.
    &quot;mimeType&quot;: &quot;A String&quot;, # An IANA published MIME type (also referred to as media type). For more
        # information, see
        # https://www.iana.org/assignments/media-types/media-types.xhtml.
    &quot;text&quot;: &quot;A String&quot;, # UTF-8 encoded text in reading order from the document.
    &quot;labels&quot;: [ # Labels for this document.
      { # Label attaches schema information and/or other metadata to segments within
          # a Document. Multiple Labels on a single field can denote either
          # different labels, different instances of the same label created at
          # different times, or some combination of both.
        &quot;confidence&quot;: 3.14, # Confidence score between 0 and 1 for label assignment.
        &quot;name&quot;: &quot;A String&quot;, # Name of the label.
            #
            # When the label is generated from AutoML Text Classification model, this
            # field represents the name of the category.
        &quot;automlModel&quot;: &quot;A String&quot;, # Label is generated AutoML model. This field stores the full resource
            # name of the AutoML model.
            #
            # Format:
            # `projects/{project-id}/locations/{location-id}/models/{model-id}`
      },
    ],
    &quot;shardInfo&quot;: { # For a large document, sharding may be performed to produce several # Information about the sharding if this document is sharded part of a larger
        # document. If the document is not sharded, this message is not specified.
        # document shards. Each document shard contains this field to detail which
        # shard it is.
      &quot;shardIndex&quot;: &quot;A String&quot;, # The 0-based index of this shard.
      &quot;shardCount&quot;: &quot;A String&quot;, # Total number of shards.
      &quot;textOffset&quot;: &quot;A String&quot;, # The index of the first character in Document.text in the overall
          # document global text.
    },
    &quot;textStyles&quot;: [ # Styles for the Document.text.
      { # Annotation for common text style attributes. This adheres to CSS
          # conventions as much as possible.
        &quot;textDecoration&quot;: &quot;A String&quot;, # Text decoration. Follows CSS standard.
            # &lt;text-decoration-line&gt; &lt;text-decoration-color&gt; &lt;text-decoration-style&gt;
            # https://www.w3schools.com/cssref/pr_text_text-decoration.asp
        &quot;fontSize&quot;: { # Font size with unit. # Font size.
          &quot;unit&quot;: &quot;A String&quot;, # Unit for the font size. Follows CSS naming (in, px, pt, etc.).
          &quot;size&quot;: 3.14, # Font size for the text.
        },
        &quot;textAnchor&quot;: { # Text reference indexing into the Document.text. # Text anchor indexing into the Document.text.
          &quot;textSegments&quot;: [ # The text segments from the Document.text.
            { # A text segment in the Document.text. The indices may be out of bounds
                # which indicate that the text extends into another document shard for
                # large sharded documents. See ShardInfo.text_offset
              &quot;endIndex&quot;: &quot;A String&quot;, # TextSegment half open end UTF-8 char index in the
                  # Document.text.
              &quot;startIndex&quot;: &quot;A String&quot;, # TextSegment start UTF-8 char index in the Document.text.
            },
          ],
        },
        &quot;textStyle&quot;: &quot;A String&quot;, # Text style. Possible values are normal, italic, and oblique.
            # https://www.w3schools.com/cssref/pr_font_font-style.asp
        &quot;backgroundColor&quot;: { # Represents a color in the RGBA color space. This representation is designed # Text background color.
            # for simplicity of conversion to/from color representations in various
            # languages over compactness; for example, the fields of this representation
            # can be trivially provided to the constructor of &quot;java.awt.Color&quot; in Java; it
            # can also be trivially provided to UIColor&#x27;s &quot;+colorWithRed:green:blue:alpha&quot;
            # method in iOS; and, with just a little work, it can be easily formatted into
            # a CSS &quot;rgba()&quot; string in JavaScript, as well.
            #
            # Note: this proto does not carry information about the absolute color space
            # that should be used to interpret the RGB value (e.g. sRGB, Adobe RGB,
            # DCI-P3, BT.2020, etc.). By default, applications SHOULD assume the sRGB color
            # space.
            #
            # Note: when color equality needs to be decided, implementations, unless
            # documented otherwise, will treat two colors to be equal if all their red,
            # green, blue and alpha values each differ by at most 1e-5.
            #
            # Example (Java):
            #
            #      import com.google.type.Color;
            #
            #      // ...
            #      public static java.awt.Color fromProto(Color protocolor) {
            #        float alpha = protocolor.hasAlpha()
            #            ? protocolor.getAlpha().getValue()
            #            : 1.0;
            #
            #        return new java.awt.Color(
            #            protocolor.getRed(),
            #            protocolor.getGreen(),
            #            protocolor.getBlue(),
            #            alpha);
            #      }
            #
            #      public static Color toProto(java.awt.Color color) {
            #        float red = (float) color.getRed();
            #        float green = (float) color.getGreen();
            #        float blue = (float) color.getBlue();
            #        float denominator = 255.0;
            #        Color.Builder resultBuilder =
            #            Color
            #                .newBuilder()
            #                .setRed(red / denominator)
            #                .setGreen(green / denominator)
            #                .setBlue(blue / denominator);
            #        int alpha = color.getAlpha();
            #        if (alpha != 255) {
            #          result.setAlpha(
            #              FloatValue
            #                  .newBuilder()
            #                  .setValue(((float) alpha) / denominator)
            #                  .build());
            #        }
            #        return resultBuilder.build();
            #      }
            #      // ...
            #
            # Example (iOS / Obj-C):
            #
            #      // ...
            #      static UIColor* fromProto(Color* protocolor) {
            #         float red = [protocolor red];
            #         float green = [protocolor green];
            #         float blue = [protocolor blue];
            #         FloatValue* alpha_wrapper = [protocolor alpha];
            #         float alpha = 1.0;
            #         if (alpha_wrapper != nil) {
            #           alpha = [alpha_wrapper value];
            #         }
            #         return [UIColor colorWithRed:red green:green blue:blue alpha:alpha];
            #      }
            #
            #      static Color* toProto(UIColor* color) {
            #          CGFloat red, green, blue, alpha;
            #          if (![color getRed:&amp;red green:&amp;green blue:&amp;blue alpha:&amp;alpha]) {
            #            return nil;
            #          }
            #          Color* result = [[Color alloc] init];
            #          [result setRed:red];
            #          [result setGreen:green];
            #          [result setBlue:blue];
            #          if (alpha &lt;= 0.9999) {
            #            [result setAlpha:floatWrapperWithValue(alpha)];
            #          }
            #          [result autorelease];
            #          return result;
            #     }
            #     // ...
            #
            #  Example (JavaScript):
            #
            #     // ...
            #
            #     var protoToCssColor = function(rgb_color) {
            #        var redFrac = rgb_color.red || 0.0;
            #        var greenFrac = rgb_color.green || 0.0;
            #        var blueFrac = rgb_color.blue || 0.0;
            #        var red = Math.floor(redFrac * 255);
            #        var green = Math.floor(greenFrac * 255);
            #        var blue = Math.floor(blueFrac * 255);
            #
            #        if (!(&#x27;alpha&#x27; in rgb_color)) {
            #           return rgbToCssColor_(red, green, blue);
            #        }
            #
            #        var alphaFrac = rgb_color.alpha.value || 0.0;
            #        var rgbParams = [red, green, blue].join(&#x27;,&#x27;);
            #        return [&#x27;rgba(&#x27;, rgbParams, &#x27;,&#x27;, alphaFrac, &#x27;)&#x27;].join(&#x27;&#x27;);
            #     };
            #
            #     var rgbToCssColor_ = function(red, green, blue) {
            #       var rgbNumber = new Number((red &lt;&lt; 16) | (green &lt;&lt; 8) | blue);
            #       var hexString = rgbNumber.toString(16);
            #       var missingZeros = 6 - hexString.length;
            #       var resultBuilder = [&#x27;#&#x27;];
            #       for (var i = 0; i &lt; missingZeros; i++) {
            #          resultBuilder.push(&#x27;0&#x27;);
            #       }
            #       resultBuilder.push(hexString);
            #       return resultBuilder.join(&#x27;&#x27;);
            #     };
            #
            #     // ...
          &quot;alpha&quot;: 3.14, # The fraction of this color that should be applied to the pixel. That is,
              # the final pixel color is defined by the equation:
              #
              #   pixel color = alpha * (this color) + (1.0 - alpha) * (background color)
              #
              # This means that a value of 1.0 corresponds to a solid color, whereas
              # a value of 0.0 corresponds to a completely transparent color. This
              # uses a wrapper message rather than a simple float scalar so that it is
              # possible to distinguish between a default value and the value being unset.
              # If omitted, this color object is to be rendered as a solid color
              # (as if the alpha value had been explicitly given with a value of 1.0).
          &quot;red&quot;: 3.14, # The amount of red in the color as a value in the interval [0, 1].
          &quot;green&quot;: 3.14, # The amount of green in the color as a value in the interval [0, 1].
          &quot;blue&quot;: 3.14, # The amount of blue in the color as a value in the interval [0, 1].
        },
        &quot;fontWeight&quot;: &quot;A String&quot;, # Font weight. Possible values are normal, bold, bolder, and lighter.
            # https://www.w3schools.com/cssref/pr_font_weight.asp
        &quot;color&quot;: { # Represents a color in the RGBA color space. This representation is designed # Text color.
            # for simplicity of conversion to/from color representations in various
            # languages over compactness; for example, the fields of this representation
            # can be trivially provided to the constructor of &quot;java.awt.Color&quot; in Java; it
            # can also be trivially provided to UIColor&#x27;s &quot;+colorWithRed:green:blue:alpha&quot;
            # method in iOS; and, with just a little work, it can be easily formatted into
            # a CSS &quot;rgba()&quot; string in JavaScript, as well.
            #
            # Note: this proto does not carry information about the absolute color space
            # that should be used to interpret the RGB value (e.g. sRGB, Adobe RGB,
            # DCI-P3, BT.2020, etc.). By default, applications SHOULD assume the sRGB color
            # space.
            #
            # Note: when color equality needs to be decided, implementations, unless
            # documented otherwise, will treat two colors to be equal if all their red,
            # green, blue and alpha values each differ by at most 1e-5.
            #
            # Example (Java):
            #
            #      import com.google.type.Color;
            #
            #      // ...
            #      public static java.awt.Color fromProto(Color protocolor) {
            #        float alpha = protocolor.hasAlpha()
            #            ? protocolor.getAlpha().getValue()
            #            : 1.0;
            #
            #        return new java.awt.Color(
            #            protocolor.getRed(),
            #            protocolor.getGreen(),
            #            protocolor.getBlue(),
            #            alpha);
            #      }
            #
            #      public static Color toProto(java.awt.Color color) {
            #        float red = (float) color.getRed();
            #        float green = (float) color.getGreen();
            #        float blue = (float) color.getBlue();
            #        float denominator = 255.0;
            #        Color.Builder resultBuilder =
            #            Color
            #                .newBuilder()
            #                .setRed(red / denominator)
            #                .setGreen(green / denominator)
            #                .setBlue(blue / denominator);
            #        int alpha = color.getAlpha();
            #        if (alpha != 255) {
            #          result.setAlpha(
            #              FloatValue
            #                  .newBuilder()
            #                  .setValue(((float) alpha) / denominator)
            #                  .build());
            #        }
            #        return resultBuilder.build();
            #      }
            #      // ...
            #
            # Example (iOS / Obj-C):
            #
            #      // ...
            #      static UIColor* fromProto(Color* protocolor) {
            #         float red = [protocolor red];
            #         float green = [protocolor green];
            #         float blue = [protocolor blue];
            #         FloatValue* alpha_wrapper = [protocolor alpha];
            #         float alpha = 1.0;
            #         if (alpha_wrapper != nil) {
            #           alpha = [alpha_wrapper value];
            #         }
            #         return [UIColor colorWithRed:red green:green blue:blue alpha:alpha];
            #      }
            #
            #      static Color* toProto(UIColor* color) {
            #          CGFloat red, green, blue, alpha;
            #          if (![color getRed:&amp;red green:&amp;green blue:&amp;blue alpha:&amp;alpha]) {
            #            return nil;
            #          }
            #          Color* result = [[Color alloc] init];
            #          [result setRed:red];
            #          [result setGreen:green];
            #          [result setBlue:blue];
            #          if (alpha &lt;= 0.9999) {
            #            [result setAlpha:floatWrapperWithValue(alpha)];
            #          }
            #          [result autorelease];
            #          return result;
            #     }
            #     // ...
            #
            #  Example (JavaScript):
            #
            #     // ...
            #
            #     var protoToCssColor = function(rgb_color) {
            #        var redFrac = rgb_color.red || 0.0;
            #        var greenFrac = rgb_color.green || 0.0;
            #        var blueFrac = rgb_color.blue || 0.0;
            #        var red = Math.floor(redFrac * 255);
            #        var green = Math.floor(greenFrac * 255);
            #        var blue = Math.floor(blueFrac * 255);
            #
            #        if (!(&#x27;alpha&#x27; in rgb_color)) {
            #           return rgbToCssColor_(red, green, blue);
            #        }
            #
            #        var alphaFrac = rgb_color.alpha.value || 0.0;
            #        var rgbParams = [red, green, blue].join(&#x27;,&#x27;);
            #        return [&#x27;rgba(&#x27;, rgbParams, &#x27;,&#x27;, alphaFrac, &#x27;)&#x27;].join(&#x27;&#x27;);
            #     };
            #
            #     var rgbToCssColor_ = function(red, green, blue) {
            #       var rgbNumber = new Number((red &lt;&lt; 16) | (green &lt;&lt; 8) | blue);
            #       var hexString = rgbNumber.toString(16);
            #       var missingZeros = 6 - hexString.length;
            #       var resultBuilder = [&#x27;#&#x27;];
            #       for (var i = 0; i &lt; missingZeros; i++) {
            #          resultBuilder.push(&#x27;0&#x27;);
            #       }
            #       resultBuilder.push(hexString);
            #       return resultBuilder.join(&#x27;&#x27;);
            #     };
            #
            #     // ...
          &quot;alpha&quot;: 3.14, # The fraction of this color that should be applied to the pixel. That is,
              # the final pixel color is defined by the equation:
              #
              #   pixel color = alpha * (this color) + (1.0 - alpha) * (background color)
              #
              # This means that a value of 1.0 corresponds to a solid color, whereas
              # a value of 0.0 corresponds to a completely transparent color. This
              # uses a wrapper message rather than a simple float scalar so that it is
              # possible to distinguish between a default value and the value being unset.
              # If omitted, this color object is to be rendered as a solid color
              # (as if the alpha value had been explicitly given with a value of 1.0).
          &quot;red&quot;: 3.14, # The amount of red in the color as a value in the interval [0, 1].
          &quot;green&quot;: 3.14, # The amount of green in the color as a value in the interval [0, 1].
          &quot;blue&quot;: 3.14, # The amount of blue in the color as a value in the interval [0, 1].
        },
      },
    ],
    &quot;uri&quot;: &quot;A String&quot;, # Currently supports Google Cloud Storage URI of the form
        #    `gs://bucket_name/object_name`. Object versioning is not supported.
        #    See [Google Cloud Storage Request
        #    URIs](https://cloud.google.com/storage/docs/reference-uris) for more
        #    info.
  }</pre>
</div>

</body></html>