<html><body>
<style>

body, h1, h2, h3, div, span, p, pre, a {
  margin: 0;
  padding: 0;
  border: 0;
  font-weight: inherit;
  font-style: inherit;
  font-size: 100%;
  font-family: inherit;
  vertical-align: baseline;
}

body {
  font-size: 13px;
  padding: 1em;
}

h1 {
  font-size: 26px;
  margin-bottom: 1em;
}

h2 {
  font-size: 24px;
  margin-bottom: 1em;
}

h3 {
  font-size: 20px;
  margin-bottom: 1em;
  margin-top: 1em;
}

pre, code {
  line-height: 1.5;
  font-family: Monaco, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console', monospace;
}

pre {
  margin-top: 0.5em;
}

h1, h2, h3, p {
  font-family: Arial, sans serif;
}

h1, h2, h3 {
  border-bottom: solid #CCC 1px;
}

.toc_element {
  margin-top: 0.5em;
}

.firstline {
  margin-left: 2 em;
}

.method  {
  margin-top: 1em;
  border: solid 1px #CCC;
  padding: 1em;
  background: #EEE;
}

.details {
  font-weight: bold;
  font-size: 14px;
}

</style>

<h1><a href="documentai_v1beta3.html">Cloud Document AI API</a> . <a href="documentai_v1beta3.projects.html">projects</a> . <a href="documentai_v1beta3.projects.locations.html">locations</a> . <a href="documentai_v1beta3.projects.locations.processors.html">processors</a> . <a href="documentai_v1beta3.projects.locations.processors.processorVersions.html">processorVersions</a> . <a href="documentai_v1beta3.projects.locations.processors.processorVersions.evaluations.html">evaluations</a></h1>
<h2>Instance Methods</h2>
<p class="toc_element">
  <code><a href="#close">close()</a></code></p>
<p class="firstline">Close httplib2 connections.</p>
<p class="toc_element">
  <code><a href="#get">get(name, x__xgafv=None)</a></code></p>
<p class="firstline">Retrieves a specific evaluation.</p>
<p class="toc_element">
  <code><a href="#list">list(parent, pageSize=None, pageToken=None, x__xgafv=None)</a></code></p>
<p class="firstline">Retrieves a set of evaluations for a given processor version.</p>
<p class="toc_element">
  <code><a href="#list_next">list_next()</a></code></p>
<p class="firstline">Retrieves the next page of results.</p>
<h3>Method Details</h3>
<div class="method">
    <code class="details" id="close">close()</code>
  <pre>Close httplib2 connections.</pre>
</div>

<div class="method">
    <code class="details" id="get">get(name, x__xgafv=None)</code>
  <pre>Retrieves a specific evaluation.

Args:
  name: string, Required. The resource name of the Evaluation to get. `projects/{project}/locations/{location}/processors/{processor}/processorVersions/{processorVersion}/evaluations/{evaluation}` (required)
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # An evaluation of a ProcessorVersion&#x27;s performance.
  &quot;allEntitiesMetrics&quot;: { # Metrics across multiple confidence levels. # Metrics for all the entities in aggregate.
    &quot;auprc&quot;: 3.14, # The calculated area under the precision recall curve (AUPRC), computed by integrating over all confidence thresholds.
    &quot;auprcExact&quot;: 3.14, # The AUPRC for metrics with fuzzy matching disabled, i.e., exact matching only.
    &quot;confidenceLevelMetrics&quot;: [ # Metrics across confidence levels with fuzzy matching enabled.
      { # Evaluations metrics, at a specific confidence level.
        &quot;confidenceLevel&quot;: 3.14, # The confidence level.
        &quot;metrics&quot;: { # Evaluation metrics, either in aggregate or about a specific entity. # The metrics at the specific confidence level.
          &quot;f1Score&quot;: 3.14, # The calculated f1 score.
          &quot;falseNegativesCount&quot;: 42, # The amount of false negatives.
          &quot;falsePositivesCount&quot;: 42, # The amount of false positives.
          &quot;groundTruthDocumentCount&quot;: 42, # The amount of documents with a ground truth occurrence.
          &quot;groundTruthOccurrencesCount&quot;: 42, # The amount of occurrences in ground truth documents.
          &quot;precision&quot;: 3.14, # The calculated precision.
          &quot;predictedDocumentCount&quot;: 42, # The amount of documents with a predicted occurrence.
          &quot;predictedOccurrencesCount&quot;: 42, # The amount of occurrences in predicted documents.
          &quot;recall&quot;: 3.14, # The calculated recall.
          &quot;totalDocumentsCount&quot;: 42, # The amount of documents that had an occurrence of this label.
          &quot;truePositivesCount&quot;: 42, # The amount of true positives.
        },
      },
    ],
    &quot;confidenceLevelMetricsExact&quot;: [ # Metrics across confidence levels with only exact matching.
      { # Evaluations metrics, at a specific confidence level.
        &quot;confidenceLevel&quot;: 3.14, # The confidence level.
        &quot;metrics&quot;: { # Evaluation metrics, either in aggregate or about a specific entity. # The metrics at the specific confidence level.
          &quot;f1Score&quot;: 3.14, # The calculated f1 score.
          &quot;falseNegativesCount&quot;: 42, # The amount of false negatives.
          &quot;falsePositivesCount&quot;: 42, # The amount of false positives.
          &quot;groundTruthDocumentCount&quot;: 42, # The amount of documents with a ground truth occurrence.
          &quot;groundTruthOccurrencesCount&quot;: 42, # The amount of occurrences in ground truth documents.
          &quot;precision&quot;: 3.14, # The calculated precision.
          &quot;predictedDocumentCount&quot;: 42, # The amount of documents with a predicted occurrence.
          &quot;predictedOccurrencesCount&quot;: 42, # The amount of occurrences in predicted documents.
          &quot;recall&quot;: 3.14, # The calculated recall.
          &quot;totalDocumentsCount&quot;: 42, # The amount of documents that had an occurrence of this label.
          &quot;truePositivesCount&quot;: 42, # The amount of true positives.
        },
      },
    ],
    &quot;estimatedCalibrationError&quot;: 3.14, # The Estimated Calibration Error (ECE) of the confidence of the predicted entities.
    &quot;estimatedCalibrationErrorExact&quot;: 3.14, # The ECE for the predicted entities with fuzzy matching disabled, i.e., exact matching only.
    &quot;metricsType&quot;: &quot;A String&quot;, # The metrics type for the label.
  },
  &quot;createTime&quot;: &quot;A String&quot;, # The time that the evaluation was created.
  &quot;documentCounters&quot;: { # Evaluation counters for the documents that were used. # Counters for the documents used in the evaluation.
    &quot;evaluatedDocumentsCount&quot;: 42, # How many documents were used in the evaluation.
    &quot;failedDocumentsCount&quot;: 42, # How many documents were not included in the evaluation as Document AI failed to process them.
    &quot;inputDocumentsCount&quot;: 42, # How many documents were sent for evaluation.
    &quot;invalidDocumentsCount&quot;: 42, # How many documents were not included in the evaluation as they didn&#x27;t pass validation.
  },
  &quot;entityMetrics&quot;: { # Metrics across confidence levels, for different entities.
    &quot;a_key&quot;: { # Metrics across multiple confidence levels.
      &quot;auprc&quot;: 3.14, # The calculated area under the precision recall curve (AUPRC), computed by integrating over all confidence thresholds.
      &quot;auprcExact&quot;: 3.14, # The AUPRC for metrics with fuzzy matching disabled, i.e., exact matching only.
      &quot;confidenceLevelMetrics&quot;: [ # Metrics across confidence levels with fuzzy matching enabled.
        { # Evaluations metrics, at a specific confidence level.
          &quot;confidenceLevel&quot;: 3.14, # The confidence level.
          &quot;metrics&quot;: { # Evaluation metrics, either in aggregate or about a specific entity. # The metrics at the specific confidence level.
            &quot;f1Score&quot;: 3.14, # The calculated f1 score.
            &quot;falseNegativesCount&quot;: 42, # The amount of false negatives.
            &quot;falsePositivesCount&quot;: 42, # The amount of false positives.
            &quot;groundTruthDocumentCount&quot;: 42, # The amount of documents with a ground truth occurrence.
            &quot;groundTruthOccurrencesCount&quot;: 42, # The amount of occurrences in ground truth documents.
            &quot;precision&quot;: 3.14, # The calculated precision.
            &quot;predictedDocumentCount&quot;: 42, # The amount of documents with a predicted occurrence.
            &quot;predictedOccurrencesCount&quot;: 42, # The amount of occurrences in predicted documents.
            &quot;recall&quot;: 3.14, # The calculated recall.
            &quot;totalDocumentsCount&quot;: 42, # The amount of documents that had an occurrence of this label.
            &quot;truePositivesCount&quot;: 42, # The amount of true positives.
          },
        },
      ],
      &quot;confidenceLevelMetricsExact&quot;: [ # Metrics across confidence levels with only exact matching.
        { # Evaluations metrics, at a specific confidence level.
          &quot;confidenceLevel&quot;: 3.14, # The confidence level.
          &quot;metrics&quot;: { # Evaluation metrics, either in aggregate or about a specific entity. # The metrics at the specific confidence level.
            &quot;f1Score&quot;: 3.14, # The calculated f1 score.
            &quot;falseNegativesCount&quot;: 42, # The amount of false negatives.
            &quot;falsePositivesCount&quot;: 42, # The amount of false positives.
            &quot;groundTruthDocumentCount&quot;: 42, # The amount of documents with a ground truth occurrence.
            &quot;groundTruthOccurrencesCount&quot;: 42, # The amount of occurrences in ground truth documents.
            &quot;precision&quot;: 3.14, # The calculated precision.
            &quot;predictedDocumentCount&quot;: 42, # The amount of documents with a predicted occurrence.
            &quot;predictedOccurrencesCount&quot;: 42, # The amount of occurrences in predicted documents.
            &quot;recall&quot;: 3.14, # The calculated recall.
            &quot;totalDocumentsCount&quot;: 42, # The amount of documents that had an occurrence of this label.
            &quot;truePositivesCount&quot;: 42, # The amount of true positives.
          },
        },
      ],
      &quot;estimatedCalibrationError&quot;: 3.14, # The Estimated Calibration Error (ECE) of the confidence of the predicted entities.
      &quot;estimatedCalibrationErrorExact&quot;: 3.14, # The ECE for the predicted entities with fuzzy matching disabled, i.e., exact matching only.
      &quot;metricsType&quot;: &quot;A String&quot;, # The metrics type for the label.
    },
  },
  &quot;kmsKeyName&quot;: &quot;A String&quot;, # The KMS key name used for encryption.
  &quot;kmsKeyVersionName&quot;: &quot;A String&quot;, # The KMS key version with which data is encrypted.
  &quot;name&quot;: &quot;A String&quot;, # The resource name of the evaluation. Format: `projects/{project}/locations/{location}/processors/{processor}/processorVersions/{processor_version}/evaluations/{evaluation}`
}</pre>
</div>

<div class="method">
    <code class="details" id="list">list(parent, pageSize=None, pageToken=None, x__xgafv=None)</code>
  <pre>Retrieves a set of evaluations for a given processor version.

Args:
  parent: string, Required. The resource name of the ProcessorVersion to list evaluations for. `projects/{project}/locations/{location}/processors/{processor}/processorVersions/{processorVersion}` (required)
  pageSize: integer, The standard list page size. If unspecified, at most 5 evaluations will be returned. The maximum value is 100; values above 100 will be coerced to 100.
  pageToken: string, A page token, received from a previous `ListEvaluations` call. Provide this to retrieve the subsequent page.
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # The response from ListEvaluations.
  &quot;evaluations&quot;: [ # The evaluations requested.
    { # An evaluation of a ProcessorVersion&#x27;s performance.
      &quot;allEntitiesMetrics&quot;: { # Metrics across multiple confidence levels. # Metrics for all the entities in aggregate.
        &quot;auprc&quot;: 3.14, # The calculated area under the precision recall curve (AUPRC), computed by integrating over all confidence thresholds.
        &quot;auprcExact&quot;: 3.14, # The AUPRC for metrics with fuzzy matching disabled, i.e., exact matching only.
        &quot;confidenceLevelMetrics&quot;: [ # Metrics across confidence levels with fuzzy matching enabled.
          { # Evaluations metrics, at a specific confidence level.
            &quot;confidenceLevel&quot;: 3.14, # The confidence level.
            &quot;metrics&quot;: { # Evaluation metrics, either in aggregate or about a specific entity. # The metrics at the specific confidence level.
              &quot;f1Score&quot;: 3.14, # The calculated f1 score.
              &quot;falseNegativesCount&quot;: 42, # The amount of false negatives.
              &quot;falsePositivesCount&quot;: 42, # The amount of false positives.
              &quot;groundTruthDocumentCount&quot;: 42, # The amount of documents with a ground truth occurrence.
              &quot;groundTruthOccurrencesCount&quot;: 42, # The amount of occurrences in ground truth documents.
              &quot;precision&quot;: 3.14, # The calculated precision.
              &quot;predictedDocumentCount&quot;: 42, # The amount of documents with a predicted occurrence.
              &quot;predictedOccurrencesCount&quot;: 42, # The amount of occurrences in predicted documents.
              &quot;recall&quot;: 3.14, # The calculated recall.
              &quot;totalDocumentsCount&quot;: 42, # The amount of documents that had an occurrence of this label.
              &quot;truePositivesCount&quot;: 42, # The amount of true positives.
            },
          },
        ],
        &quot;confidenceLevelMetricsExact&quot;: [ # Metrics across confidence levels with only exact matching.
          { # Evaluations metrics, at a specific confidence level.
            &quot;confidenceLevel&quot;: 3.14, # The confidence level.
            &quot;metrics&quot;: { # Evaluation metrics, either in aggregate or about a specific entity. # The metrics at the specific confidence level.
              &quot;f1Score&quot;: 3.14, # The calculated f1 score.
              &quot;falseNegativesCount&quot;: 42, # The amount of false negatives.
              &quot;falsePositivesCount&quot;: 42, # The amount of false positives.
              &quot;groundTruthDocumentCount&quot;: 42, # The amount of documents with a ground truth occurrence.
              &quot;groundTruthOccurrencesCount&quot;: 42, # The amount of occurrences in ground truth documents.
              &quot;precision&quot;: 3.14, # The calculated precision.
              &quot;predictedDocumentCount&quot;: 42, # The amount of documents with a predicted occurrence.
              &quot;predictedOccurrencesCount&quot;: 42, # The amount of occurrences in predicted documents.
              &quot;recall&quot;: 3.14, # The calculated recall.
              &quot;totalDocumentsCount&quot;: 42, # The amount of documents that had an occurrence of this label.
              &quot;truePositivesCount&quot;: 42, # The amount of true positives.
            },
          },
        ],
        &quot;estimatedCalibrationError&quot;: 3.14, # The Estimated Calibration Error (ECE) of the confidence of the predicted entities.
        &quot;estimatedCalibrationErrorExact&quot;: 3.14, # The ECE for the predicted entities with fuzzy matching disabled, i.e., exact matching only.
        &quot;metricsType&quot;: &quot;A String&quot;, # The metrics type for the label.
      },
      &quot;createTime&quot;: &quot;A String&quot;, # The time that the evaluation was created.
      &quot;documentCounters&quot;: { # Evaluation counters for the documents that were used. # Counters for the documents used in the evaluation.
        &quot;evaluatedDocumentsCount&quot;: 42, # How many documents were used in the evaluation.
        &quot;failedDocumentsCount&quot;: 42, # How many documents were not included in the evaluation as Document AI failed to process them.
        &quot;inputDocumentsCount&quot;: 42, # How many documents were sent for evaluation.
        &quot;invalidDocumentsCount&quot;: 42, # How many documents were not included in the evaluation as they didn&#x27;t pass validation.
      },
      &quot;entityMetrics&quot;: { # Metrics across confidence levels, for different entities.
        &quot;a_key&quot;: { # Metrics across multiple confidence levels.
          &quot;auprc&quot;: 3.14, # The calculated area under the precision recall curve (AUPRC), computed by integrating over all confidence thresholds.
          &quot;auprcExact&quot;: 3.14, # The AUPRC for metrics with fuzzy matching disabled, i.e., exact matching only.
          &quot;confidenceLevelMetrics&quot;: [ # Metrics across confidence levels with fuzzy matching enabled.
            { # Evaluations metrics, at a specific confidence level.
              &quot;confidenceLevel&quot;: 3.14, # The confidence level.
              &quot;metrics&quot;: { # Evaluation metrics, either in aggregate or about a specific entity. # The metrics at the specific confidence level.
                &quot;f1Score&quot;: 3.14, # The calculated f1 score.
                &quot;falseNegativesCount&quot;: 42, # The amount of false negatives.
                &quot;falsePositivesCount&quot;: 42, # The amount of false positives.
                &quot;groundTruthDocumentCount&quot;: 42, # The amount of documents with a ground truth occurrence.
                &quot;groundTruthOccurrencesCount&quot;: 42, # The amount of occurrences in ground truth documents.
                &quot;precision&quot;: 3.14, # The calculated precision.
                &quot;predictedDocumentCount&quot;: 42, # The amount of documents with a predicted occurrence.
                &quot;predictedOccurrencesCount&quot;: 42, # The amount of occurrences in predicted documents.
                &quot;recall&quot;: 3.14, # The calculated recall.
                &quot;totalDocumentsCount&quot;: 42, # The amount of documents that had an occurrence of this label.
                &quot;truePositivesCount&quot;: 42, # The amount of true positives.
              },
            },
          ],
          &quot;confidenceLevelMetricsExact&quot;: [ # Metrics across confidence levels with only exact matching.
            { # Evaluations metrics, at a specific confidence level.
              &quot;confidenceLevel&quot;: 3.14, # The confidence level.
              &quot;metrics&quot;: { # Evaluation metrics, either in aggregate or about a specific entity. # The metrics at the specific confidence level.
                &quot;f1Score&quot;: 3.14, # The calculated f1 score.
                &quot;falseNegativesCount&quot;: 42, # The amount of false negatives.
                &quot;falsePositivesCount&quot;: 42, # The amount of false positives.
                &quot;groundTruthDocumentCount&quot;: 42, # The amount of documents with a ground truth occurrence.
                &quot;groundTruthOccurrencesCount&quot;: 42, # The amount of occurrences in ground truth documents.
                &quot;precision&quot;: 3.14, # The calculated precision.
                &quot;predictedDocumentCount&quot;: 42, # The amount of documents with a predicted occurrence.
                &quot;predictedOccurrencesCount&quot;: 42, # The amount of occurrences in predicted documents.
                &quot;recall&quot;: 3.14, # The calculated recall.
                &quot;totalDocumentsCount&quot;: 42, # The amount of documents that had an occurrence of this label.
                &quot;truePositivesCount&quot;: 42, # The amount of true positives.
              },
            },
          ],
          &quot;estimatedCalibrationError&quot;: 3.14, # The Estimated Calibration Error (ECE) of the confidence of the predicted entities.
          &quot;estimatedCalibrationErrorExact&quot;: 3.14, # The ECE for the predicted entities with fuzzy matching disabled, i.e., exact matching only.
          &quot;metricsType&quot;: &quot;A String&quot;, # The metrics type for the label.
        },
      },
      &quot;kmsKeyName&quot;: &quot;A String&quot;, # The KMS key name used for encryption.
      &quot;kmsKeyVersionName&quot;: &quot;A String&quot;, # The KMS key version with which data is encrypted.
      &quot;name&quot;: &quot;A String&quot;, # The resource name of the evaluation. Format: `projects/{project}/locations/{location}/processors/{processor}/processorVersions/{processor_version}/evaluations/{evaluation}`
    },
  ],
  &quot;nextPageToken&quot;: &quot;A String&quot;, # A token, which can be sent as `page_token` to retrieve the next page. If this field is omitted, there are no subsequent pages.
}</pre>
</div>

<div class="method">
    <code class="details" id="list_next">list_next()</code>
  <pre>Retrieves the next page of results.

        Args:
          previous_request: The request for the previous page. (required)
          previous_response: The response from the request for the previous page. (required)

        Returns:
          A request object that you can call &#x27;execute()&#x27; on to request the next
          page. Returns None if there are no more items in the collection.
        </pre>
</div>

</body></html>