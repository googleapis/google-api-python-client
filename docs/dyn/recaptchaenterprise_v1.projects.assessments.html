<html><body>
<style>

body, h1, h2, h3, div, span, p, pre, a {
  margin: 0;
  padding: 0;
  border: 0;
  font-weight: inherit;
  font-style: inherit;
  font-size: 100%;
  font-family: inherit;
  vertical-align: baseline;
}

body {
  font-size: 13px;
  padding: 1em;
}

h1 {
  font-size: 26px;
  margin-bottom: 1em;
}

h2 {
  font-size: 24px;
  margin-bottom: 1em;
}

h3 {
  font-size: 20px;
  margin-bottom: 1em;
  margin-top: 1em;
}

pre, code {
  line-height: 1.5;
  font-family: Monaco, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console', monospace;
}

pre {
  margin-top: 0.5em;
}

h1, h2, h3, p {
  font-family: Arial, sans serif;
}

h1, h2, h3 {
  border-bottom: solid #CCC 1px;
}

.toc_element {
  margin-top: 0.5em;
}

.firstline {
  margin-left: 2 em;
}

.method  {
  margin-top: 1em;
  border: solid 1px #CCC;
  padding: 1em;
  background: #EEE;
}

.details {
  font-weight: bold;
  font-size: 14px;
}

</style>

<h1><a href="recaptchaenterprise_v1.html">reCAPTCHA Enterprise API</a> . <a href="recaptchaenterprise_v1.projects.html">projects</a> . <a href="recaptchaenterprise_v1.projects.assessments.html">assessments</a></h1>
<h2>Instance Methods</h2>
<p class="toc_element">
  <code><a href="#annotate">annotate(name, body=None, x__xgafv=None)</a></code></p>
<p class="firstline">Annotates a previously created Assessment to provide additional information on whether the event turned out to be authentic or fraudulent.</p>
<p class="toc_element">
  <code><a href="#close">close()</a></code></p>
<p class="firstline">Close httplib2 connections.</p>
<p class="toc_element">
  <code><a href="#create">create(parent, body=None, x__xgafv=None)</a></code></p>
<p class="firstline">Creates an Assessment of the likelihood an event is legitimate.</p>
<h3>Method Details</h3>
<div class="method">
    <code class="details" id="annotate">annotate(name, body=None, x__xgafv=None)</code>
  <pre>Annotates a previously created Assessment to provide additional information on whether the event turned out to be authentic or fraudulent.

Args:
  name: string, Required. The resource name of the Assessment, in the format &quot;projects/{project}/assessments/{assessment}&quot;. (required)
  body: object, The request body.
    The object takes the form of:

{ # The request message to annotate an Assessment.
  &quot;annotation&quot;: &quot;A String&quot;, # Optional. The annotation that will be assigned to the Event. This field can be left empty to provide reasons that apply to an event without concluding whether the event is legitimate or fraudulent.
  &quot;hashedAccountId&quot;: &quot;A String&quot;, # Optional. Optional unique stable hashed user identifier to apply to the assessment. This is an alternative to setting the hashed_account_id in CreateAssessment, for example when the account identifier is not yet known in the initial request. It is recommended that the identifier is hashed using hmac-sha256 with stable secret.
  &quot;reasons&quot;: [ # Optional. Optional reasons for the annotation that will be assigned to the Event.
    &quot;A String&quot;,
  ],
}

  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # Empty response for AnnotateAssessment.
}</pre>
</div>

<div class="method">
    <code class="details" id="close">close()</code>
  <pre>Close httplib2 connections.</pre>
</div>

<div class="method">
    <code class="details" id="create">create(parent, body=None, x__xgafv=None)</code>
  <pre>Creates an Assessment of the likelihood an event is legitimate.

Args:
  parent: string, Required. The name of the project in which the assessment will be created, in the format &quot;projects/{project}&quot;. (required)
  body: object, The request body.
    The object takes the form of:

{ # A recaptcha assessment resource.
  &quot;accountDefenderAssessment&quot;: { # Account Defender risk assessment. # Assessment returned by Account Defender when a hashed_account_id is provided.
    &quot;labels&quot;: [ # Labels for this request.
      &quot;A String&quot;,
    ],
    &quot;recommendedAction&quot;: &quot;A String&quot;, # Recommended action after this request.
  },
  &quot;event&quot;: { # The event being assessed.
    &quot;expectedAction&quot;: &quot;A String&quot;, # Optional. The expected action for this type of event. This should be the same action provided at token generation time on client-side platforms already integrated with recaptcha enterprise.
    &quot;siteKey&quot;: &quot;A String&quot;, # Optional. The site key that was used to invoke reCAPTCHA on your site and generate the token.
    &quot;token&quot;: &quot;A String&quot;, # Optional. The user response token provided by the reCAPTCHA client-side integration on your site.
    &quot;userAgent&quot;: &quot;A String&quot;, # Optional. The user agent present in the request from the user&#x27;s device related to this event.
    &quot;userIpAddress&quot;: &quot;A String&quot;, # Optional. The IP address in the request from the user&#x27;s device related to this event.
  },
  &quot;name&quot;: &quot;A String&quot;, # Output only. The resource name for the Assessment in the format &quot;projects/{project}/assessments/{assessment}&quot;.
  &quot;riskAnalysis&quot;: { # Risk analysis result for an event. # Output only. The risk analysis result for the event being assessed.
    &quot;reasons&quot;: [ # Reasons contributing to the risk analysis verdict.
      &quot;A String&quot;,
    ],
    &quot;score&quot;: 3.14, # Legitimate event score from 0.0 to 1.0. (1.0 means very likely legitimate traffic while 0.0 means very likely non-legitimate traffic).
  },
  &quot;tokenProperties&quot;: { # Output only. Properties of the provided event token.
    &quot;action&quot;: &quot;A String&quot;, # Action name provided at token generation.
    &quot;createTime&quot;: &quot;A String&quot;, # The timestamp corresponding to the generation of the token.
    &quot;hostname&quot;: &quot;A String&quot;, # The hostname of the page on which the token was generated.
    &quot;invalidReason&quot;: &quot;A String&quot;, # Reason associated with the response when valid = false.
    &quot;valid&quot;: True or False, # Whether the provided user response token is valid. When valid = false, the reason could be specified in invalid_reason or it could also be due to a user failing to solve a challenge or a sitekey mismatch (i.e the sitekey used to generate the token was different than the one specified in the assessment).
  },
}

  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # A recaptcha assessment resource.
  &quot;accountDefenderAssessment&quot;: { # Account Defender risk assessment. # Assessment returned by Account Defender when a hashed_account_id is provided.
    &quot;labels&quot;: [ # Labels for this request.
      &quot;A String&quot;,
    ],
    &quot;recommendedAction&quot;: &quot;A String&quot;, # Recommended action after this request.
  },
  &quot;event&quot;: { # The event being assessed.
    &quot;expectedAction&quot;: &quot;A String&quot;, # Optional. The expected action for this type of event. This should be the same action provided at token generation time on client-side platforms already integrated with recaptcha enterprise.
    &quot;siteKey&quot;: &quot;A String&quot;, # Optional. The site key that was used to invoke reCAPTCHA on your site and generate the token.
    &quot;token&quot;: &quot;A String&quot;, # Optional. The user response token provided by the reCAPTCHA client-side integration on your site.
    &quot;userAgent&quot;: &quot;A String&quot;, # Optional. The user agent present in the request from the user&#x27;s device related to this event.
    &quot;userIpAddress&quot;: &quot;A String&quot;, # Optional. The IP address in the request from the user&#x27;s device related to this event.
  },
  &quot;name&quot;: &quot;A String&quot;, # Output only. The resource name for the Assessment in the format &quot;projects/{project}/assessments/{assessment}&quot;.
  &quot;riskAnalysis&quot;: { # Risk analysis result for an event. # Output only. The risk analysis result for the event being assessed.
    &quot;reasons&quot;: [ # Reasons contributing to the risk analysis verdict.
      &quot;A String&quot;,
    ],
    &quot;score&quot;: 3.14, # Legitimate event score from 0.0 to 1.0. (1.0 means very likely legitimate traffic while 0.0 means very likely non-legitimate traffic).
  },
  &quot;tokenProperties&quot;: { # Output only. Properties of the provided event token.
    &quot;action&quot;: &quot;A String&quot;, # Action name provided at token generation.
    &quot;createTime&quot;: &quot;A String&quot;, # The timestamp corresponding to the generation of the token.
    &quot;hostname&quot;: &quot;A String&quot;, # The hostname of the page on which the token was generated.
    &quot;invalidReason&quot;: &quot;A String&quot;, # Reason associated with the response when valid = false.
    &quot;valid&quot;: True or False, # Whether the provided user response token is valid. When valid = false, the reason could be specified in invalid_reason or it could also be due to a user failing to solve a challenge or a sitekey mismatch (i.e the sitekey used to generate the token was different than the one specified in the assessment).
  },
}</pre>
</div>

</body></html>