<html><body>
<style>

body, h1, h2, h3, div, span, p, pre, a {
  margin: 0;
  padding: 0;
  border: 0;
  font-weight: inherit;
  font-style: inherit;
  font-size: 100%;
  font-family: inherit;
  vertical-align: baseline;
}

body {
  font-size: 13px;
  padding: 1em;
}

h1 {
  font-size: 26px;
  margin-bottom: 1em;
}

h2 {
  font-size: 24px;
  margin-bottom: 1em;
}

h3 {
  font-size: 20px;
  margin-bottom: 1em;
  margin-top: 1em;
}

pre, code {
  line-height: 1.5;
  font-family: Monaco, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console', monospace;
}

pre {
  margin-top: 0.5em;
}

h1, h2, h3, p {
  font-family: Arial, sans serif;
}

h1, h2, h3 {
  border-bottom: solid #CCC 1px;
}

.toc_element {
  margin-top: 0.5em;
}

.firstline {
  margin-left: 2 em;
}

.method  {
  margin-top: 1em;
  border: solid 1px #CCC;
  padding: 1em;
  background: #EEE;
}

.details {
  font-weight: bold;
  font-size: 14px;
}

</style>

<h1><a href="aiplatform_v1.html">Vertex AI API</a> . <a href="aiplatform_v1.projects.html">projects</a> . <a href="aiplatform_v1.projects.locations.html">locations</a> . <a href="aiplatform_v1.projects.locations.evaluationRuns.html">evaluationRuns</a></h1>
<h2>Instance Methods</h2>
<p class="toc_element">
  <code><a href="#cancel">cancel(name, body=None, x__xgafv=None)</a></code></p>
<p class="firstline">Cancels an Evaluation Run. Attempts to cancel a running Evaluation Run asynchronously. Status of run can be checked via GetEvaluationRun.</p>
<p class="toc_element">
  <code><a href="#close">close()</a></code></p>
<p class="firstline">Close httplib2 connections.</p>
<p class="toc_element">
  <code><a href="#create">create(parent, body=None, x__xgafv=None)</a></code></p>
<p class="firstline">Creates an Evaluation Run.</p>
<p class="toc_element">
  <code><a href="#delete">delete(name, x__xgafv=None)</a></code></p>
<p class="firstline">Deletes an Evaluation Run.</p>
<p class="toc_element">
  <code><a href="#get">get(name, x__xgafv=None)</a></code></p>
<p class="firstline">Gets an Evaluation Run.</p>
<p class="toc_element">
  <code><a href="#list">list(parent, filter=None, orderBy=None, pageSize=None, pageToken=None, x__xgafv=None)</a></code></p>
<p class="firstline">Lists Evaluation Runs.</p>
<p class="toc_element">
  <code><a href="#list_next">list_next()</a></code></p>
<p class="firstline">Retrieves the next page of results.</p>
<h3>Method Details</h3>
<div class="method">
    <code class="details" id="cancel">cancel(name, body=None, x__xgafv=None)</code>
  <pre>Cancels an Evaluation Run. Attempts to cancel a running Evaluation Run asynchronously. Status of run can be checked via GetEvaluationRun.

Args:
  name: string, Required. The name of the EvaluationRun resource to be cancelled. Format: `projects/{project}/locations/{location}/evaluationRuns/{evaluation_run}` (required)
  body: object, The request body.
    The object takes the form of:

{ # Request message for EvaluationManagementService.CancelEvaluationRun.
}

  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # A generic empty message that you can re-use to avoid defining duplicated empty messages in your APIs. A typical example is to use it as the request or the response type of an API method. For instance: service Foo { rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty); }
}</pre>
</div>

<div class="method">
    <code class="details" id="close">close()</code>
  <pre>Close httplib2 connections.</pre>
</div>

<div class="method">
    <code class="details" id="create">create(parent, body=None, x__xgafv=None)</code>
  <pre>Creates an Evaluation Run.

Args:
  parent: string, Required. The resource name of the Location to create the Evaluation Run in. Format: `projects/{project}/locations/{location}` (required)
  body: object, The request body.
    The object takes the form of:

{ # EvaluationRun is a resource that represents a single evaluation run, which includes a set of prompts, model responses, evaluation configuration and the resulting metrics.
  &quot;completionTime&quot;: &quot;A String&quot;, # Output only. Time when the evaluation run was completed.
  &quot;createTime&quot;: &quot;A String&quot;, # Output only. Time when the evaluation run was created.
  &quot;dataSource&quot;: { # The data source for the evaluation run. # Required. The data source for the evaluation run.
    &quot;bigqueryRequestSet&quot;: { # The request set for the evaluation run. # Evaluation data in bigquery.
      &quot;candidateResponseColumns&quot;: { # Optional. Map of candidate name to candidate response column name. The column will be in evaluation_item.CandidateResponse format.
        &quot;a_key&quot;: &quot;A String&quot;,
      },
      &quot;promptColumn&quot;: &quot;A String&quot;, # Optional. The name of the column that contains the requests to evaluate. This will be in evaluation_item.EvalPrompt format.
      &quot;rubricsColumn&quot;: &quot;A String&quot;, # Optional. The name of the column that contains the rubrics. This is in evaluation_rubric.RubricGroup format.
      &quot;samplingConfig&quot;: { # The sampling config. # Optional. The sampling config for the bigquery resource.
        &quot;samplingCount&quot;: 42, # Optional. The total number of logged data to import. If available data is less than the sampling count, all data will be imported. Default is 100.
        &quot;samplingDuration&quot;: &quot;A String&quot;, # Optional. How long to wait before sampling data from the BigQuery table. If not specified, defaults to 0.
        &quot;samplingMethod&quot;: &quot;A String&quot;, # Optional. The sampling method to use.
      },
      &quot;uri&quot;: &quot;A String&quot;, # Required. The URI of a BigQuery table. e.g. bq://projectId.bqDatasetId.bqTableId
    },
    &quot;evaluationSet&quot;: &quot;A String&quot;, # The EvaluationSet resource name. Format: `projects/{project}/locations/{location}/evaluationSets/{evaluation_set}`
  },
  &quot;displayName&quot;: &quot;A String&quot;, # Required. The display name of the Evaluation Run.
  &quot;error&quot;: { # The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors). # Output only. Only populated when the evaluation run&#x27;s state is FAILED or CANCELLED.
    &quot;code&quot;: 42, # The status code, which should be an enum value of google.rpc.Code.
    &quot;details&quot;: [ # A list of messages that carry the error details. There is a common set of message types for APIs to use.
      {
        &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
      },
    ],
    &quot;message&quot;: &quot;A String&quot;, # A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
  },
  &quot;evaluationConfig&quot;: { # The Evalution configuration used for the evaluation run. # Required. The configuration used for the evaluation.
    &quot;autoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. The autorater config for the evaluation run.
      &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
      &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
        &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
        &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
        &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
        &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
        &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
          &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
          &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
            &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
            &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
          },
          &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
        },
        &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
        &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
        &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
        &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
        &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
        &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
        &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
        &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
          &quot;A String&quot;,
        ],
        &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
          &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
          &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
            # Object with schema name: GoogleCloudAiplatformV1Schema
          ],
          &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
          &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
          &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
            &quot;A String&quot;,
          ],
          &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
          &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
          &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
          &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
          &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
          &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
          &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
          &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
          &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
          &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
          &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
          &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
          &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
          &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
            &quot;A String&quot;,
          ],
          &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
          &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
            &quot;A String&quot;,
          ],
          &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
          &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
        },
        &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
          &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
            &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
          },
          &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
            &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
          },
        },
        &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
        &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
          &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
          &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
            &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
              { # Configuration for a single speaker in a multi-speaker setup.
                &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
            ],
          },
          &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
            },
          },
        },
        &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
          &quot;A String&quot;,
        ],
        &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
        &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
          &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
          &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
        },
        &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
        &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
      },
      &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
    },
    &quot;metrics&quot;: [ # Required. The metrics to be calculated in the evaluation run.
      { # The metric used for evaluation runs.
        &quot;llmBasedMetricSpec&quot;: { # Specification for an LLM based metric. # Spec for an LLM based metric.
          &quot;additionalConfig&quot;: { # Optional. Optional additional configuration for the metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
          &quot;judgeAutoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Optional configuration for the judge LLM (Autorater).
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Template for the prompt sent to the judge model.
          &quot;predefinedRubricGenerationSpec&quot;: { # Specification for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
            &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
            &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
          },
          &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
            &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
            &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
            &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
              &quot;A String&quot;,
            ],
          },
          &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input. Refers to a key in the rubric_groups map of EvaluationInstance.
          &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for the judge model.
        },
        &quot;metric&quot;: &quot;A String&quot;, # Required. The name of the metric.
        &quot;metricConfig&quot;: { # The metric used for running evaluations. # The metric config.
          &quot;aggregationMetrics&quot;: [ # Optional. The aggregation metrics to use.
            &quot;A String&quot;,
          ],
          &quot;bleuSpec&quot;: { # Spec for bleu score metric - calculates the precision of n-grams in the prediction as compared to reference - returns a score ranging between 0 to 1. # Spec for bleu metric.
            &quot;useEffectiveOrder&quot;: True or False, # Optional. Whether to use_effective_order to compute bleu score.
          },
          &quot;customCodeExecutionSpec&quot;: { # Specificies a metric that is populated by evaluating user-defined Python code. # Spec for Custom Code Execution metric.
            &quot;evaluationFunction&quot;: &quot;A String&quot;, # Required. Python function. Expected user to define the following function, e.g.: def evaluate(instance: dict[str, Any]) -&gt; float: Please include this function signature in the code snippet. Instance is the evaluation instance, any fields populated in the instance are available to the function as instance[field_name]. Example: Example input: ``` instance= EvaluationInstance( response=EvaluationInstance.InstanceData(text=&quot;The answer is 4.&quot;), reference=EvaluationInstance.InstanceData(text=&quot;4&quot;) ) ``` Example converted input: ``` { &#x27;response&#x27;: {&#x27;text&#x27;: &#x27;The answer is 4.&#x27;}, &#x27;reference&#x27;: {&#x27;text&#x27;: &#x27;4&#x27;} } ``` Example python function: ``` def evaluate(instance: dict[str, Any]) -&gt; float: if instance&#x27;response&#x27; == instance&#x27;reference&#x27;: return 1.0 return 0.0 ```
          },
          &quot;exactMatchSpec&quot;: { # Spec for exact match metric - returns 1 if prediction and reference exactly matches, otherwise 0. # Spec for exact match metric.
          },
          &quot;llmBasedMetricSpec&quot;: { # Specification for an LLM based metric. # Spec for an LLM based metric.
            &quot;additionalConfig&quot;: { # Optional. Optional additional configuration for the metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
            &quot;judgeAutoraterConfig&quot;: { # The configs for autorater. This is applicable to both EvaluateInstances and EvaluateDataset. # Optional. Optional configuration for the judge LLM (Autorater).
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;flipEnabled&quot;: True or False, # Optional. Default is true. Whether to flip the candidate and baseline responses. This is only applicable to the pairwise metric. If enabled, also provide PairwiseMetricSpec.candidate_response_field_name and PairwiseMetricSpec.baseline_response_field_name. When rendering PairwiseMetricSpec.metric_prompt_template, the candidate and baseline fields will be flipped for half of the samples to reduce bias.
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;samplingCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Template for the prompt sent to the judge model.
            &quot;predefinedRubricGenerationSpec&quot;: { # The spec for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
              &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
              &quot;metricSpecParameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
                &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
              },
            },
            &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
              &quot;modelConfig&quot;: { # The configs for autorater. This is applicable to both EvaluateInstances and EvaluateDataset. # Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
                &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                &quot;flipEnabled&quot;: True or False, # Optional. Default is true. Whether to flip the candidate and baseline responses. This is only applicable to the pairwise metric. If enabled, also provide PairwiseMetricSpec.candidate_response_field_name and PairwiseMetricSpec.baseline_response_field_name. When rendering PairwiseMetricSpec.metric_prompt_template, the candidate and baseline fields will be flipped for half of the samples to reduce bias.
                &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                  &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                  &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                  &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                  &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                  &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                    &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                    &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                      &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                      &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                    },
                    &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                  },
                  &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                  &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                  &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                  &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                  &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                  &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                  &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                  &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                    &quot;A String&quot;,
                  ],
                  &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                    &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                    &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                      # Object with schema name: GoogleCloudAiplatformV1Schema
                    ],
                    &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                    &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                    &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                      &quot;A String&quot;,
                    ],
                    &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                    &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                    &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                    &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                    &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                    &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                    &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                    &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                    &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                    &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                    &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                    &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                    &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                    &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                      &quot;A String&quot;,
                    ],
                    &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                    &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                      &quot;A String&quot;,
                    ],
                    &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                    &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                  },
                  &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                    &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                      &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                    },
                    &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                      &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                    },
                  },
                  &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                  &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                    &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                    &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                      &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                        { # Configuration for a single speaker in a multi-speaker setup.
                          &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                          &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                            },
                          },
                        },
                      ],
                    },
                    &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                      &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                        &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                      },
                    },
                  },
                  &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                    &quot;A String&quot;,
                  ],
                  &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                  &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                    &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                    &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                  },
                  &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                  &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                },
                &quot;samplingCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
              },
              &quot;promptTemplate&quot;: &quot;A String&quot;, # Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
              &quot;rubricContentType&quot;: &quot;A String&quot;, # The type of rubric content to be generated.
              &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
                &quot;A String&quot;,
              ],
            },
            &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input. Refers to a key in the rubric_groups map of EvaluationInstance.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for the judge model.
          },
          &quot;pairwiseMetricSpec&quot;: { # Spec for pairwise metric. # Spec for pairwise metric.
            &quot;baselineResponseFieldName&quot;: &quot;A String&quot;, # Optional. The field name of the baseline response.
            &quot;candidateResponseFieldName&quot;: &quot;A String&quot;, # Optional. The field name of the candidate response.
            &quot;customOutputFormatConfig&quot;: { # Spec for custom output format configuration. # Optional. CustomOutputFormatConfig allows customization of metric output. When this config is set, the default output is replaced with the raw output string. If a custom format is chosen, the `pairwise_choice` and `explanation` fields in the corresponding metric result will be empty.
              &quot;returnRawOutput&quot;: True or False, # Optional. Whether to return raw output.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Metric prompt template for pairwise metric.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for pairwise metric.
          },
          &quot;pointwiseMetricSpec&quot;: { # Spec for pointwise metric. # Spec for pointwise metric.
            &quot;customOutputFormatConfig&quot;: { # Spec for custom output format configuration. # Optional. CustomOutputFormatConfig allows customization of metric output. By default, metrics return a score and explanation. When this config is set, the default output is replaced with either: - The raw output string. - A parsed output based on a user-defined schema. If a custom format is chosen, the `score` and `explanation` fields in the corresponding metric result will be empty.
              &quot;returnRawOutput&quot;: True or False, # Optional. Whether to return raw output.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Metric prompt template for pointwise metric.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for pointwise metric.
          },
          &quot;predefinedMetricSpec&quot;: { # The spec for a pre-defined metric. # The spec for a pre-defined metric.
            &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
            &quot;metricSpecParameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
          },
          &quot;rougeSpec&quot;: { # Spec for rouge score metric - calculates the recall of n-grams in prediction as compared to reference - returns a score ranging between 0 and 1. # Spec for rouge metric.
            &quot;rougeType&quot;: &quot;A String&quot;, # Optional. Supported rouge types are rougen[1-9], rougeL, and rougeLsum.
            &quot;splitSummaries&quot;: True or False, # Optional. Whether to split summaries while using rougeLsum.
            &quot;useStemmer&quot;: True or False, # Optional. Whether to use stemmer to compute rouge score.
          },
        },
        &quot;predefinedMetricSpec&quot;: { # Specification for a pre-defined metric. # Spec for a pre-defined metric.
          &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
          &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
        },
        &quot;rubricBasedMetricSpec&quot;: { # Specification for a metric that is based on rubrics. # Spec for rubric based metric.
          &quot;inlineRubrics&quot;: { # Defines a list of rubrics, used when providing rubrics inline. # Use rubrics provided directly in the spec.
            &quot;rubrics&quot;: [ # The list of rubrics.
              { # Message representing a single testable criterion for evaluation. One input prompt could have multiple rubrics.
                &quot;content&quot;: { # Content of the rubric, defining the testable criteria. # Required. The actual testable criteria for the rubric.
                  &quot;property&quot;: { # Defines criteria based on a specific property. # Evaluation criteria based on a specific property.
                    &quot;description&quot;: &quot;A String&quot;, # Description of the property being evaluated. Example: &quot;The model&#x27;s response is grammatically correct.&quot;
                  },
                },
                &quot;importance&quot;: &quot;A String&quot;, # Optional. The relative importance of this rubric.
                &quot;rubricId&quot;: &quot;A String&quot;, # Unique identifier for the rubric. This ID is used to refer to this rubric, e.g., in RubricVerdict.
                &quot;type&quot;: &quot;A String&quot;, # Optional. A type designator for the rubric, which can inform how it&#x27;s evaluated or interpreted by systems or users. It&#x27;s recommended to use consistent, well-defined, upper snake_case strings. Examples: &quot;SUMMARIZATION_QUALITY&quot;, &quot;SAFETY_HARMFUL_CONTENT&quot;, &quot;INSTRUCTION_ADHERENCE&quot;.
              },
            ],
          },
          &quot;judgeAutoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Optional configuration for the judge LLM (Autorater). The definition of AutoraterConfig needs to be provided.
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used by the judge model to evaluate against rubrics.
          &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics for evaluation using this specification.
            &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
            &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
            &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
              &quot;A String&quot;,
            ],
          },
          &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input content. This refers to a key in the `rubric_groups` map of `RubricEnhancedContents`.
        },
      },
    ],
    &quot;outputConfig&quot;: { # The output config for the evaluation run. # Optional. The output config for the evaluation run.
      &quot;bigqueryDestination&quot;: { # The BigQuery location for the output content. # BigQuery destination for evaluation output.
        &quot;outputUri&quot;: &quot;A String&quot;, # Required. BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: * BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
      },
      &quot;gcsDestination&quot;: { # The Google Cloud Storage location where the output is to be written to. # Cloud Storage destination for evaluation output.
        &quot;outputUriPrefix&quot;: &quot;A String&quot;, # Required. Google Cloud Storage URI to output directory. If the uri doesn&#x27;t end with &#x27;/&#x27;, a &#x27;/&#x27; will be automatically appended. The directory is created if it doesn&#x27;t exist.
      },
    },
    &quot;promptTemplate&quot;: { # Prompt template used for inference. # The prompt template used for inference. The values for variables in the prompt template are defined in EvaluationItem.EvaluationPrompt.PromptTemplateData.values.
      &quot;gcsUri&quot;: &quot;A String&quot;, # Prompt template stored in Cloud Storage. Format: &quot;gs://my-bucket/file-name.txt&quot;.
      &quot;promptTemplate&quot;: &quot;A String&quot;, # Inline prompt template. Template variables should be in the format &quot;{var_name}&quot;. Example: &quot;Translate the following from {source_lang} to {target_lang}: {text}&quot;
    },
    &quot;rubricConfigs&quot;: [ # Optional. The rubric configs for the evaluation run. They are used to generate rubrics which can be used by rubric-based metrics. Multiple rubric configs can be specified for rubric generation but only one rubric config can be used for a rubric-based metric. If more than one rubric config is provided, the evaluation metric must specify a rubric group key. Note that if a generation spec is specified on both a rubric config and an evaluation metric, the rubrics generated for the metric will be used for evaluation.
      { # Configuration for a rubric group to be generated/saved for evaluation.
        &quot;predefinedRubricGenerationSpec&quot;: { # Specification for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
          &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
          &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
        },
        &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
          &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
          &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
          &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
            &quot;A String&quot;,
          ],
        },
        &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Required. The key used to save the generated rubrics. If a generation spec is provided, this key will be used for the name of the generated rubric group. Otherwise, this key will be used to look up the existing rubric group on the evaluation item. Note that if a rubric group key is specified on both a rubric config and an evaluation metric, the key from the metric will be used to select the rubrics for evaluation.
      },
    ],
  },
  &quot;evaluationResults&quot;: { # The results of the evaluation run. # Output only. The results of the evaluation run. Only populated when the evaluation run&#x27;s state is SUCCEEDED.
    &quot;evaluationSet&quot;: &quot;A String&quot;, # The evaluation set where item level results are stored.
    &quot;summaryMetrics&quot;: { # The summary metrics for the evaluation run. # Optional. The summary metrics for the evaluation run.
      &quot;failedItems&quot;: 42, # Optional. The number of items that failed to be evaluated.
      &quot;metrics&quot;: { # Optional. Map of metric name to metric value.
        &quot;a_key&quot;: &quot;&quot;,
      },
      &quot;totalItems&quot;: 42, # Optional. The total number of items that were evaluated.
    },
  },
  &quot;evaluationSetSnapshot&quot;: &quot;A String&quot;, # Output only. The specific evaluation set of the evaluation run. For runs with an evaluation set input, this will be that same set. For runs with BigQuery input, it&#x27;s the sampled BigQuery dataset.
  &quot;inferenceConfigs&quot;: { # Optional. The candidate to inference config map for the evaluation run. The candidate can be up to 128 characters long and can consist of any UTF-8 characters.
    &quot;a_key&quot;: { # An inference config used for model inference during the evaluation run.
      &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Generation config.
        &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
        &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
        &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
        &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
        &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
          &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
          &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
            &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
            &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
          },
          &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
        },
        &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
        &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
        &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
        &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
        &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
        &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
        &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
        &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
          &quot;A String&quot;,
        ],
        &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
          &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
          &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
            # Object with schema name: GoogleCloudAiplatformV1Schema
          ],
          &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
          &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
          &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
            &quot;A String&quot;,
          ],
          &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
          &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
          &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
          &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
          &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
          &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
          &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
          &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
          &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
          &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
          &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
          &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
          &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
          &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
            &quot;A String&quot;,
          ],
          &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
          &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
            &quot;A String&quot;,
          ],
          &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
          &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
        },
        &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
          &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
            &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
          },
          &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
            &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
          },
        },
        &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
        &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
          &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
          &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
            &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
              { # Configuration for a single speaker in a multi-speaker setup.
                &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
            ],
          },
          &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
            },
          },
        },
        &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
          &quot;A String&quot;,
        ],
        &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
        &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
          &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
          &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
        },
        &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
        &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
      },
      &quot;model&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
    },
  },
  &quot;labels&quot;: { # Optional. Labels for the evaluation run.
    &quot;a_key&quot;: &quot;A String&quot;,
  },
  &quot;metadata&quot;: &quot;&quot;, # Optional. Metadata about the evaluation run, can be used by the caller to store additional tracking information about the evaluation run.
  &quot;name&quot;: &quot;A String&quot;, # Identifier. The resource name of the EvaluationRun. This is a unique identifier. Format: `projects/{project}/locations/{location}/evaluationRuns/{evaluation_run}`
  &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the evaluation run.
}

  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # EvaluationRun is a resource that represents a single evaluation run, which includes a set of prompts, model responses, evaluation configuration and the resulting metrics.
  &quot;completionTime&quot;: &quot;A String&quot;, # Output only. Time when the evaluation run was completed.
  &quot;createTime&quot;: &quot;A String&quot;, # Output only. Time when the evaluation run was created.
  &quot;dataSource&quot;: { # The data source for the evaluation run. # Required. The data source for the evaluation run.
    &quot;bigqueryRequestSet&quot;: { # The request set for the evaluation run. # Evaluation data in bigquery.
      &quot;candidateResponseColumns&quot;: { # Optional. Map of candidate name to candidate response column name. The column will be in evaluation_item.CandidateResponse format.
        &quot;a_key&quot;: &quot;A String&quot;,
      },
      &quot;promptColumn&quot;: &quot;A String&quot;, # Optional. The name of the column that contains the requests to evaluate. This will be in evaluation_item.EvalPrompt format.
      &quot;rubricsColumn&quot;: &quot;A String&quot;, # Optional. The name of the column that contains the rubrics. This is in evaluation_rubric.RubricGroup format.
      &quot;samplingConfig&quot;: { # The sampling config. # Optional. The sampling config for the bigquery resource.
        &quot;samplingCount&quot;: 42, # Optional. The total number of logged data to import. If available data is less than the sampling count, all data will be imported. Default is 100.
        &quot;samplingDuration&quot;: &quot;A String&quot;, # Optional. How long to wait before sampling data from the BigQuery table. If not specified, defaults to 0.
        &quot;samplingMethod&quot;: &quot;A String&quot;, # Optional. The sampling method to use.
      },
      &quot;uri&quot;: &quot;A String&quot;, # Required. The URI of a BigQuery table. e.g. bq://projectId.bqDatasetId.bqTableId
    },
    &quot;evaluationSet&quot;: &quot;A String&quot;, # The EvaluationSet resource name. Format: `projects/{project}/locations/{location}/evaluationSets/{evaluation_set}`
  },
  &quot;displayName&quot;: &quot;A String&quot;, # Required. The display name of the Evaluation Run.
  &quot;error&quot;: { # The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors). # Output only. Only populated when the evaluation run&#x27;s state is FAILED or CANCELLED.
    &quot;code&quot;: 42, # The status code, which should be an enum value of google.rpc.Code.
    &quot;details&quot;: [ # A list of messages that carry the error details. There is a common set of message types for APIs to use.
      {
        &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
      },
    ],
    &quot;message&quot;: &quot;A String&quot;, # A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
  },
  &quot;evaluationConfig&quot;: { # The Evalution configuration used for the evaluation run. # Required. The configuration used for the evaluation.
    &quot;autoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. The autorater config for the evaluation run.
      &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
      &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
        &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
        &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
        &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
        &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
        &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
          &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
          &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
            &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
            &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
          },
          &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
        },
        &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
        &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
        &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
        &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
        &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
        &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
        &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
        &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
          &quot;A String&quot;,
        ],
        &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
          &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
          &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
            # Object with schema name: GoogleCloudAiplatformV1Schema
          ],
          &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
          &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
          &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
            &quot;A String&quot;,
          ],
          &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
          &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
          &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
          &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
          &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
          &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
          &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
          &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
          &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
          &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
          &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
          &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
          &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
          &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
            &quot;A String&quot;,
          ],
          &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
          &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
            &quot;A String&quot;,
          ],
          &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
          &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
        },
        &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
          &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
            &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
          },
          &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
            &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
          },
        },
        &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
        &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
          &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
          &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
            &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
              { # Configuration for a single speaker in a multi-speaker setup.
                &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
            ],
          },
          &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
            },
          },
        },
        &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
          &quot;A String&quot;,
        ],
        &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
        &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
          &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
          &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
        },
        &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
        &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
      },
      &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
    },
    &quot;metrics&quot;: [ # Required. The metrics to be calculated in the evaluation run.
      { # The metric used for evaluation runs.
        &quot;llmBasedMetricSpec&quot;: { # Specification for an LLM based metric. # Spec for an LLM based metric.
          &quot;additionalConfig&quot;: { # Optional. Optional additional configuration for the metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
          &quot;judgeAutoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Optional configuration for the judge LLM (Autorater).
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Template for the prompt sent to the judge model.
          &quot;predefinedRubricGenerationSpec&quot;: { # Specification for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
            &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
            &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
          },
          &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
            &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
            &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
            &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
              &quot;A String&quot;,
            ],
          },
          &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input. Refers to a key in the rubric_groups map of EvaluationInstance.
          &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for the judge model.
        },
        &quot;metric&quot;: &quot;A String&quot;, # Required. The name of the metric.
        &quot;metricConfig&quot;: { # The metric used for running evaluations. # The metric config.
          &quot;aggregationMetrics&quot;: [ # Optional. The aggregation metrics to use.
            &quot;A String&quot;,
          ],
          &quot;bleuSpec&quot;: { # Spec for bleu score metric - calculates the precision of n-grams in the prediction as compared to reference - returns a score ranging between 0 to 1. # Spec for bleu metric.
            &quot;useEffectiveOrder&quot;: True or False, # Optional. Whether to use_effective_order to compute bleu score.
          },
          &quot;customCodeExecutionSpec&quot;: { # Specificies a metric that is populated by evaluating user-defined Python code. # Spec for Custom Code Execution metric.
            &quot;evaluationFunction&quot;: &quot;A String&quot;, # Required. Python function. Expected user to define the following function, e.g.: def evaluate(instance: dict[str, Any]) -&gt; float: Please include this function signature in the code snippet. Instance is the evaluation instance, any fields populated in the instance are available to the function as instance[field_name]. Example: Example input: ``` instance= EvaluationInstance( response=EvaluationInstance.InstanceData(text=&quot;The answer is 4.&quot;), reference=EvaluationInstance.InstanceData(text=&quot;4&quot;) ) ``` Example converted input: ``` { &#x27;response&#x27;: {&#x27;text&#x27;: &#x27;The answer is 4.&#x27;}, &#x27;reference&#x27;: {&#x27;text&#x27;: &#x27;4&#x27;} } ``` Example python function: ``` def evaluate(instance: dict[str, Any]) -&gt; float: if instance&#x27;response&#x27; == instance&#x27;reference&#x27;: return 1.0 return 0.0 ```
          },
          &quot;exactMatchSpec&quot;: { # Spec for exact match metric - returns 1 if prediction and reference exactly matches, otherwise 0. # Spec for exact match metric.
          },
          &quot;llmBasedMetricSpec&quot;: { # Specification for an LLM based metric. # Spec for an LLM based metric.
            &quot;additionalConfig&quot;: { # Optional. Optional additional configuration for the metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
            &quot;judgeAutoraterConfig&quot;: { # The configs for autorater. This is applicable to both EvaluateInstances and EvaluateDataset. # Optional. Optional configuration for the judge LLM (Autorater).
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;flipEnabled&quot;: True or False, # Optional. Default is true. Whether to flip the candidate and baseline responses. This is only applicable to the pairwise metric. If enabled, also provide PairwiseMetricSpec.candidate_response_field_name and PairwiseMetricSpec.baseline_response_field_name. When rendering PairwiseMetricSpec.metric_prompt_template, the candidate and baseline fields will be flipped for half of the samples to reduce bias.
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;samplingCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Template for the prompt sent to the judge model.
            &quot;predefinedRubricGenerationSpec&quot;: { # The spec for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
              &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
              &quot;metricSpecParameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
                &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
              },
            },
            &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
              &quot;modelConfig&quot;: { # The configs for autorater. This is applicable to both EvaluateInstances and EvaluateDataset. # Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
                &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                &quot;flipEnabled&quot;: True or False, # Optional. Default is true. Whether to flip the candidate and baseline responses. This is only applicable to the pairwise metric. If enabled, also provide PairwiseMetricSpec.candidate_response_field_name and PairwiseMetricSpec.baseline_response_field_name. When rendering PairwiseMetricSpec.metric_prompt_template, the candidate and baseline fields will be flipped for half of the samples to reduce bias.
                &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                  &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                  &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                  &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                  &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                  &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                    &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                    &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                      &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                      &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                    },
                    &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                  },
                  &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                  &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                  &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                  &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                  &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                  &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                  &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                  &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                    &quot;A String&quot;,
                  ],
                  &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                    &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                    &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                      # Object with schema name: GoogleCloudAiplatformV1Schema
                    ],
                    &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                    &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                    &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                      &quot;A String&quot;,
                    ],
                    &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                    &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                    &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                    &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                    &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                    &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                    &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                    &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                    &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                    &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                    &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                    &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                    &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                    &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                      &quot;A String&quot;,
                    ],
                    &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                    &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                      &quot;A String&quot;,
                    ],
                    &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                    &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                  },
                  &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                    &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                      &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                    },
                    &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                      &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                    },
                  },
                  &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                  &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                    &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                    &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                      &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                        { # Configuration for a single speaker in a multi-speaker setup.
                          &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                          &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                            },
                          },
                        },
                      ],
                    },
                    &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                      &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                        &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                      },
                    },
                  },
                  &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                    &quot;A String&quot;,
                  ],
                  &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                  &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                    &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                    &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                  },
                  &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                  &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                },
                &quot;samplingCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
              },
              &quot;promptTemplate&quot;: &quot;A String&quot;, # Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
              &quot;rubricContentType&quot;: &quot;A String&quot;, # The type of rubric content to be generated.
              &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
                &quot;A String&quot;,
              ],
            },
            &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input. Refers to a key in the rubric_groups map of EvaluationInstance.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for the judge model.
          },
          &quot;pairwiseMetricSpec&quot;: { # Spec for pairwise metric. # Spec for pairwise metric.
            &quot;baselineResponseFieldName&quot;: &quot;A String&quot;, # Optional. The field name of the baseline response.
            &quot;candidateResponseFieldName&quot;: &quot;A String&quot;, # Optional. The field name of the candidate response.
            &quot;customOutputFormatConfig&quot;: { # Spec for custom output format configuration. # Optional. CustomOutputFormatConfig allows customization of metric output. When this config is set, the default output is replaced with the raw output string. If a custom format is chosen, the `pairwise_choice` and `explanation` fields in the corresponding metric result will be empty.
              &quot;returnRawOutput&quot;: True or False, # Optional. Whether to return raw output.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Metric prompt template for pairwise metric.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for pairwise metric.
          },
          &quot;pointwiseMetricSpec&quot;: { # Spec for pointwise metric. # Spec for pointwise metric.
            &quot;customOutputFormatConfig&quot;: { # Spec for custom output format configuration. # Optional. CustomOutputFormatConfig allows customization of metric output. By default, metrics return a score and explanation. When this config is set, the default output is replaced with either: - The raw output string. - A parsed output based on a user-defined schema. If a custom format is chosen, the `score` and `explanation` fields in the corresponding metric result will be empty.
              &quot;returnRawOutput&quot;: True or False, # Optional. Whether to return raw output.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Metric prompt template for pointwise metric.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for pointwise metric.
          },
          &quot;predefinedMetricSpec&quot;: { # The spec for a pre-defined metric. # The spec for a pre-defined metric.
            &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
            &quot;metricSpecParameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
          },
          &quot;rougeSpec&quot;: { # Spec for rouge score metric - calculates the recall of n-grams in prediction as compared to reference - returns a score ranging between 0 and 1. # Spec for rouge metric.
            &quot;rougeType&quot;: &quot;A String&quot;, # Optional. Supported rouge types are rougen[1-9], rougeL, and rougeLsum.
            &quot;splitSummaries&quot;: True or False, # Optional. Whether to split summaries while using rougeLsum.
            &quot;useStemmer&quot;: True or False, # Optional. Whether to use stemmer to compute rouge score.
          },
        },
        &quot;predefinedMetricSpec&quot;: { # Specification for a pre-defined metric. # Spec for a pre-defined metric.
          &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
          &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
        },
        &quot;rubricBasedMetricSpec&quot;: { # Specification for a metric that is based on rubrics. # Spec for rubric based metric.
          &quot;inlineRubrics&quot;: { # Defines a list of rubrics, used when providing rubrics inline. # Use rubrics provided directly in the spec.
            &quot;rubrics&quot;: [ # The list of rubrics.
              { # Message representing a single testable criterion for evaluation. One input prompt could have multiple rubrics.
                &quot;content&quot;: { # Content of the rubric, defining the testable criteria. # Required. The actual testable criteria for the rubric.
                  &quot;property&quot;: { # Defines criteria based on a specific property. # Evaluation criteria based on a specific property.
                    &quot;description&quot;: &quot;A String&quot;, # Description of the property being evaluated. Example: &quot;The model&#x27;s response is grammatically correct.&quot;
                  },
                },
                &quot;importance&quot;: &quot;A String&quot;, # Optional. The relative importance of this rubric.
                &quot;rubricId&quot;: &quot;A String&quot;, # Unique identifier for the rubric. This ID is used to refer to this rubric, e.g., in RubricVerdict.
                &quot;type&quot;: &quot;A String&quot;, # Optional. A type designator for the rubric, which can inform how it&#x27;s evaluated or interpreted by systems or users. It&#x27;s recommended to use consistent, well-defined, upper snake_case strings. Examples: &quot;SUMMARIZATION_QUALITY&quot;, &quot;SAFETY_HARMFUL_CONTENT&quot;, &quot;INSTRUCTION_ADHERENCE&quot;.
              },
            ],
          },
          &quot;judgeAutoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Optional configuration for the judge LLM (Autorater). The definition of AutoraterConfig needs to be provided.
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used by the judge model to evaluate against rubrics.
          &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics for evaluation using this specification.
            &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
            &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
            &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
              &quot;A String&quot;,
            ],
          },
          &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input content. This refers to a key in the `rubric_groups` map of `RubricEnhancedContents`.
        },
      },
    ],
    &quot;outputConfig&quot;: { # The output config for the evaluation run. # Optional. The output config for the evaluation run.
      &quot;bigqueryDestination&quot;: { # The BigQuery location for the output content. # BigQuery destination for evaluation output.
        &quot;outputUri&quot;: &quot;A String&quot;, # Required. BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: * BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
      },
      &quot;gcsDestination&quot;: { # The Google Cloud Storage location where the output is to be written to. # Cloud Storage destination for evaluation output.
        &quot;outputUriPrefix&quot;: &quot;A String&quot;, # Required. Google Cloud Storage URI to output directory. If the uri doesn&#x27;t end with &#x27;/&#x27;, a &#x27;/&#x27; will be automatically appended. The directory is created if it doesn&#x27;t exist.
      },
    },
    &quot;promptTemplate&quot;: { # Prompt template used for inference. # The prompt template used for inference. The values for variables in the prompt template are defined in EvaluationItem.EvaluationPrompt.PromptTemplateData.values.
      &quot;gcsUri&quot;: &quot;A String&quot;, # Prompt template stored in Cloud Storage. Format: &quot;gs://my-bucket/file-name.txt&quot;.
      &quot;promptTemplate&quot;: &quot;A String&quot;, # Inline prompt template. Template variables should be in the format &quot;{var_name}&quot;. Example: &quot;Translate the following from {source_lang} to {target_lang}: {text}&quot;
    },
    &quot;rubricConfigs&quot;: [ # Optional. The rubric configs for the evaluation run. They are used to generate rubrics which can be used by rubric-based metrics. Multiple rubric configs can be specified for rubric generation but only one rubric config can be used for a rubric-based metric. If more than one rubric config is provided, the evaluation metric must specify a rubric group key. Note that if a generation spec is specified on both a rubric config and an evaluation metric, the rubrics generated for the metric will be used for evaluation.
      { # Configuration for a rubric group to be generated/saved for evaluation.
        &quot;predefinedRubricGenerationSpec&quot;: { # Specification for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
          &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
          &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
        },
        &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
          &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
          &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
          &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
            &quot;A String&quot;,
          ],
        },
        &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Required. The key used to save the generated rubrics. If a generation spec is provided, this key will be used for the name of the generated rubric group. Otherwise, this key will be used to look up the existing rubric group on the evaluation item. Note that if a rubric group key is specified on both a rubric config and an evaluation metric, the key from the metric will be used to select the rubrics for evaluation.
      },
    ],
  },
  &quot;evaluationResults&quot;: { # The results of the evaluation run. # Output only. The results of the evaluation run. Only populated when the evaluation run&#x27;s state is SUCCEEDED.
    &quot;evaluationSet&quot;: &quot;A String&quot;, # The evaluation set where item level results are stored.
    &quot;summaryMetrics&quot;: { # The summary metrics for the evaluation run. # Optional. The summary metrics for the evaluation run.
      &quot;failedItems&quot;: 42, # Optional. The number of items that failed to be evaluated.
      &quot;metrics&quot;: { # Optional. Map of metric name to metric value.
        &quot;a_key&quot;: &quot;&quot;,
      },
      &quot;totalItems&quot;: 42, # Optional. The total number of items that were evaluated.
    },
  },
  &quot;evaluationSetSnapshot&quot;: &quot;A String&quot;, # Output only. The specific evaluation set of the evaluation run. For runs with an evaluation set input, this will be that same set. For runs with BigQuery input, it&#x27;s the sampled BigQuery dataset.
  &quot;inferenceConfigs&quot;: { # Optional. The candidate to inference config map for the evaluation run. The candidate can be up to 128 characters long and can consist of any UTF-8 characters.
    &quot;a_key&quot;: { # An inference config used for model inference during the evaluation run.
      &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Generation config.
        &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
        &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
        &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
        &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
        &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
          &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
          &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
            &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
            &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
          },
          &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
        },
        &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
        &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
        &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
        &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
        &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
        &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
        &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
        &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
          &quot;A String&quot;,
        ],
        &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
          &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
          &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
            # Object with schema name: GoogleCloudAiplatformV1Schema
          ],
          &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
          &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
          &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
            &quot;A String&quot;,
          ],
          &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
          &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
          &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
          &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
          &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
          &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
          &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
          &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
          &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
          &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
          &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
          &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
          &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
          &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
            &quot;A String&quot;,
          ],
          &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
          &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
            &quot;A String&quot;,
          ],
          &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
          &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
        },
        &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
          &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
            &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
          },
          &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
            &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
          },
        },
        &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
        &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
          &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
          &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
            &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
              { # Configuration for a single speaker in a multi-speaker setup.
                &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
            ],
          },
          &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
            },
          },
        },
        &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
          &quot;A String&quot;,
        ],
        &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
        &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
          &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
          &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
        },
        &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
        &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
      },
      &quot;model&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
    },
  },
  &quot;labels&quot;: { # Optional. Labels for the evaluation run.
    &quot;a_key&quot;: &quot;A String&quot;,
  },
  &quot;metadata&quot;: &quot;&quot;, # Optional. Metadata about the evaluation run, can be used by the caller to store additional tracking information about the evaluation run.
  &quot;name&quot;: &quot;A String&quot;, # Identifier. The resource name of the EvaluationRun. This is a unique identifier. Format: `projects/{project}/locations/{location}/evaluationRuns/{evaluation_run}`
  &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the evaluation run.
}</pre>
</div>

<div class="method">
    <code class="details" id="delete">delete(name, x__xgafv=None)</code>
  <pre>Deletes an Evaluation Run.

Args:
  name: string, Required. The name of the EvaluationRun resource to be deleted. Format: `projects/{project}/locations/{location}/evaluationRuns/{evaluation_run}` (required)
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # This resource represents a long-running operation that is the result of a network API call.
  &quot;done&quot;: True or False, # If the value is `false`, it means the operation is still in progress. If `true`, the operation is completed, and either `error` or `response` is available.
  &quot;error&quot;: { # The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors). # The error result of the operation in case of failure or cancellation.
    &quot;code&quot;: 42, # The status code, which should be an enum value of google.rpc.Code.
    &quot;details&quot;: [ # A list of messages that carry the error details. There is a common set of message types for APIs to use.
      {
        &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
      },
    ],
    &quot;message&quot;: &quot;A String&quot;, # A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
  },
  &quot;metadata&quot;: { # Service-specific metadata associated with the operation. It typically contains progress information and common metadata such as create time. Some services might not provide such metadata. Any method that returns a long-running operation should document the metadata type, if any.
    &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
  },
  &quot;name&quot;: &quot;A String&quot;, # The server-assigned name, which is only unique within the same service that originally returns it. If you use the default HTTP mapping, the `name` should be a resource name ending with `operations/{unique_id}`.
  &quot;response&quot;: { # The normal, successful response of the operation. If the original method returns no data on success, such as `Delete`, the response is `google.protobuf.Empty`. If the original method is standard `Get`/`Create`/`Update`, the response should be the resource. For other methods, the response should have the type `XxxResponse`, where `Xxx` is the original method name. For example, if the original method name is `TakeSnapshot()`, the inferred response type is `TakeSnapshotResponse`.
    &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
  },
}</pre>
</div>

<div class="method">
    <code class="details" id="get">get(name, x__xgafv=None)</code>
  <pre>Gets an Evaluation Run.

Args:
  name: string, Required. The name of the EvaluationRun resource. Format: `projects/{project}/locations/{location}/evaluationRuns/{evaluation_run}` (required)
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # EvaluationRun is a resource that represents a single evaluation run, which includes a set of prompts, model responses, evaluation configuration and the resulting metrics.
  &quot;completionTime&quot;: &quot;A String&quot;, # Output only. Time when the evaluation run was completed.
  &quot;createTime&quot;: &quot;A String&quot;, # Output only. Time when the evaluation run was created.
  &quot;dataSource&quot;: { # The data source for the evaluation run. # Required. The data source for the evaluation run.
    &quot;bigqueryRequestSet&quot;: { # The request set for the evaluation run. # Evaluation data in bigquery.
      &quot;candidateResponseColumns&quot;: { # Optional. Map of candidate name to candidate response column name. The column will be in evaluation_item.CandidateResponse format.
        &quot;a_key&quot;: &quot;A String&quot;,
      },
      &quot;promptColumn&quot;: &quot;A String&quot;, # Optional. The name of the column that contains the requests to evaluate. This will be in evaluation_item.EvalPrompt format.
      &quot;rubricsColumn&quot;: &quot;A String&quot;, # Optional. The name of the column that contains the rubrics. This is in evaluation_rubric.RubricGroup format.
      &quot;samplingConfig&quot;: { # The sampling config. # Optional. The sampling config for the bigquery resource.
        &quot;samplingCount&quot;: 42, # Optional. The total number of logged data to import. If available data is less than the sampling count, all data will be imported. Default is 100.
        &quot;samplingDuration&quot;: &quot;A String&quot;, # Optional. How long to wait before sampling data from the BigQuery table. If not specified, defaults to 0.
        &quot;samplingMethod&quot;: &quot;A String&quot;, # Optional. The sampling method to use.
      },
      &quot;uri&quot;: &quot;A String&quot;, # Required. The URI of a BigQuery table. e.g. bq://projectId.bqDatasetId.bqTableId
    },
    &quot;evaluationSet&quot;: &quot;A String&quot;, # The EvaluationSet resource name. Format: `projects/{project}/locations/{location}/evaluationSets/{evaluation_set}`
  },
  &quot;displayName&quot;: &quot;A String&quot;, # Required. The display name of the Evaluation Run.
  &quot;error&quot;: { # The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors). # Output only. Only populated when the evaluation run&#x27;s state is FAILED or CANCELLED.
    &quot;code&quot;: 42, # The status code, which should be an enum value of google.rpc.Code.
    &quot;details&quot;: [ # A list of messages that carry the error details. There is a common set of message types for APIs to use.
      {
        &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
      },
    ],
    &quot;message&quot;: &quot;A String&quot;, # A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
  },
  &quot;evaluationConfig&quot;: { # The Evalution configuration used for the evaluation run. # Required. The configuration used for the evaluation.
    &quot;autoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. The autorater config for the evaluation run.
      &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
      &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
        &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
        &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
        &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
        &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
        &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
          &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
          &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
            &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
            &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
          },
          &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
        },
        &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
        &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
        &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
        &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
        &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
        &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
        &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
        &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
          &quot;A String&quot;,
        ],
        &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
          &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
          &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
            # Object with schema name: GoogleCloudAiplatformV1Schema
          ],
          &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
          &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
          &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
            &quot;A String&quot;,
          ],
          &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
          &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
          &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
          &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
          &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
          &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
          &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
          &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
          &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
          &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
          &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
          &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
          &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
          &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
            &quot;A String&quot;,
          ],
          &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
          &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
            &quot;A String&quot;,
          ],
          &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
          &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
        },
        &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
          &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
            &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
          },
          &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
            &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
          },
        },
        &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
        &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
          &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
          &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
            &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
              { # Configuration for a single speaker in a multi-speaker setup.
                &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
            ],
          },
          &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
            },
          },
        },
        &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
          &quot;A String&quot;,
        ],
        &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
        &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
          &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
          &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
        },
        &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
        &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
      },
      &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
    },
    &quot;metrics&quot;: [ # Required. The metrics to be calculated in the evaluation run.
      { # The metric used for evaluation runs.
        &quot;llmBasedMetricSpec&quot;: { # Specification for an LLM based metric. # Spec for an LLM based metric.
          &quot;additionalConfig&quot;: { # Optional. Optional additional configuration for the metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
          &quot;judgeAutoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Optional configuration for the judge LLM (Autorater).
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Template for the prompt sent to the judge model.
          &quot;predefinedRubricGenerationSpec&quot;: { # Specification for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
            &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
            &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
          },
          &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
            &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
            &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
            &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
              &quot;A String&quot;,
            ],
          },
          &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input. Refers to a key in the rubric_groups map of EvaluationInstance.
          &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for the judge model.
        },
        &quot;metric&quot;: &quot;A String&quot;, # Required. The name of the metric.
        &quot;metricConfig&quot;: { # The metric used for running evaluations. # The metric config.
          &quot;aggregationMetrics&quot;: [ # Optional. The aggregation metrics to use.
            &quot;A String&quot;,
          ],
          &quot;bleuSpec&quot;: { # Spec for bleu score metric - calculates the precision of n-grams in the prediction as compared to reference - returns a score ranging between 0 to 1. # Spec for bleu metric.
            &quot;useEffectiveOrder&quot;: True or False, # Optional. Whether to use_effective_order to compute bleu score.
          },
          &quot;customCodeExecutionSpec&quot;: { # Specificies a metric that is populated by evaluating user-defined Python code. # Spec for Custom Code Execution metric.
            &quot;evaluationFunction&quot;: &quot;A String&quot;, # Required. Python function. Expected user to define the following function, e.g.: def evaluate(instance: dict[str, Any]) -&gt; float: Please include this function signature in the code snippet. Instance is the evaluation instance, any fields populated in the instance are available to the function as instance[field_name]. Example: Example input: ``` instance= EvaluationInstance( response=EvaluationInstance.InstanceData(text=&quot;The answer is 4.&quot;), reference=EvaluationInstance.InstanceData(text=&quot;4&quot;) ) ``` Example converted input: ``` { &#x27;response&#x27;: {&#x27;text&#x27;: &#x27;The answer is 4.&#x27;}, &#x27;reference&#x27;: {&#x27;text&#x27;: &#x27;4&#x27;} } ``` Example python function: ``` def evaluate(instance: dict[str, Any]) -&gt; float: if instance&#x27;response&#x27; == instance&#x27;reference&#x27;: return 1.0 return 0.0 ```
          },
          &quot;exactMatchSpec&quot;: { # Spec for exact match metric - returns 1 if prediction and reference exactly matches, otherwise 0. # Spec for exact match metric.
          },
          &quot;llmBasedMetricSpec&quot;: { # Specification for an LLM based metric. # Spec for an LLM based metric.
            &quot;additionalConfig&quot;: { # Optional. Optional additional configuration for the metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
            &quot;judgeAutoraterConfig&quot;: { # The configs for autorater. This is applicable to both EvaluateInstances and EvaluateDataset. # Optional. Optional configuration for the judge LLM (Autorater).
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;flipEnabled&quot;: True or False, # Optional. Default is true. Whether to flip the candidate and baseline responses. This is only applicable to the pairwise metric. If enabled, also provide PairwiseMetricSpec.candidate_response_field_name and PairwiseMetricSpec.baseline_response_field_name. When rendering PairwiseMetricSpec.metric_prompt_template, the candidate and baseline fields will be flipped for half of the samples to reduce bias.
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;samplingCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Template for the prompt sent to the judge model.
            &quot;predefinedRubricGenerationSpec&quot;: { # The spec for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
              &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
              &quot;metricSpecParameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
                &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
              },
            },
            &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
              &quot;modelConfig&quot;: { # The configs for autorater. This is applicable to both EvaluateInstances and EvaluateDataset. # Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
                &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                &quot;flipEnabled&quot;: True or False, # Optional. Default is true. Whether to flip the candidate and baseline responses. This is only applicable to the pairwise metric. If enabled, also provide PairwiseMetricSpec.candidate_response_field_name and PairwiseMetricSpec.baseline_response_field_name. When rendering PairwiseMetricSpec.metric_prompt_template, the candidate and baseline fields will be flipped for half of the samples to reduce bias.
                &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                  &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                  &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                  &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                  &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                  &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                    &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                    &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                      &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                      &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                    },
                    &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                  },
                  &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                  &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                  &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                  &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                  &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                  &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                  &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                  &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                    &quot;A String&quot;,
                  ],
                  &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                    &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                    &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                      # Object with schema name: GoogleCloudAiplatformV1Schema
                    ],
                    &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                    &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                    &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                      &quot;A String&quot;,
                    ],
                    &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                    &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                    &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                    &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                    &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                    &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                    &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                    &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                    &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                    &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                    &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                    &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                    &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                    &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                      &quot;A String&quot;,
                    ],
                    &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                    &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                      &quot;A String&quot;,
                    ],
                    &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                    &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                  },
                  &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                    &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                      &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                    },
                    &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                      &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                    },
                  },
                  &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                  &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                    &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                    &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                      &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                        { # Configuration for a single speaker in a multi-speaker setup.
                          &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                          &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                            },
                          },
                        },
                      ],
                    },
                    &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                      &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                        &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                      },
                    },
                  },
                  &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                    &quot;A String&quot;,
                  ],
                  &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                  &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                    &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                    &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                  },
                  &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                  &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                },
                &quot;samplingCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
              },
              &quot;promptTemplate&quot;: &quot;A String&quot;, # Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
              &quot;rubricContentType&quot;: &quot;A String&quot;, # The type of rubric content to be generated.
              &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
                &quot;A String&quot;,
              ],
            },
            &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input. Refers to a key in the rubric_groups map of EvaluationInstance.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for the judge model.
          },
          &quot;pairwiseMetricSpec&quot;: { # Spec for pairwise metric. # Spec for pairwise metric.
            &quot;baselineResponseFieldName&quot;: &quot;A String&quot;, # Optional. The field name of the baseline response.
            &quot;candidateResponseFieldName&quot;: &quot;A String&quot;, # Optional. The field name of the candidate response.
            &quot;customOutputFormatConfig&quot;: { # Spec for custom output format configuration. # Optional. CustomOutputFormatConfig allows customization of metric output. When this config is set, the default output is replaced with the raw output string. If a custom format is chosen, the `pairwise_choice` and `explanation` fields in the corresponding metric result will be empty.
              &quot;returnRawOutput&quot;: True or False, # Optional. Whether to return raw output.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Metric prompt template for pairwise metric.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for pairwise metric.
          },
          &quot;pointwiseMetricSpec&quot;: { # Spec for pointwise metric. # Spec for pointwise metric.
            &quot;customOutputFormatConfig&quot;: { # Spec for custom output format configuration. # Optional. CustomOutputFormatConfig allows customization of metric output. By default, metrics return a score and explanation. When this config is set, the default output is replaced with either: - The raw output string. - A parsed output based on a user-defined schema. If a custom format is chosen, the `score` and `explanation` fields in the corresponding metric result will be empty.
              &quot;returnRawOutput&quot;: True or False, # Optional. Whether to return raw output.
            },
            &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Metric prompt template for pointwise metric.
            &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for pointwise metric.
          },
          &quot;predefinedMetricSpec&quot;: { # The spec for a pre-defined metric. # The spec for a pre-defined metric.
            &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
            &quot;metricSpecParameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
              &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
            },
          },
          &quot;rougeSpec&quot;: { # Spec for rouge score metric - calculates the recall of n-grams in prediction as compared to reference - returns a score ranging between 0 and 1. # Spec for rouge metric.
            &quot;rougeType&quot;: &quot;A String&quot;, # Optional. Supported rouge types are rougen[1-9], rougeL, and rougeLsum.
            &quot;splitSummaries&quot;: True or False, # Optional. Whether to split summaries while using rougeLsum.
            &quot;useStemmer&quot;: True or False, # Optional. Whether to use stemmer to compute rouge score.
          },
        },
        &quot;predefinedMetricSpec&quot;: { # Specification for a pre-defined metric. # Spec for a pre-defined metric.
          &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
          &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
        },
        &quot;rubricBasedMetricSpec&quot;: { # Specification for a metric that is based on rubrics. # Spec for rubric based metric.
          &quot;inlineRubrics&quot;: { # Defines a list of rubrics, used when providing rubrics inline. # Use rubrics provided directly in the spec.
            &quot;rubrics&quot;: [ # The list of rubrics.
              { # Message representing a single testable criterion for evaluation. One input prompt could have multiple rubrics.
                &quot;content&quot;: { # Content of the rubric, defining the testable criteria. # Required. The actual testable criteria for the rubric.
                  &quot;property&quot;: { # Defines criteria based on a specific property. # Evaluation criteria based on a specific property.
                    &quot;description&quot;: &quot;A String&quot;, # Description of the property being evaluated. Example: &quot;The model&#x27;s response is grammatically correct.&quot;
                  },
                },
                &quot;importance&quot;: &quot;A String&quot;, # Optional. The relative importance of this rubric.
                &quot;rubricId&quot;: &quot;A String&quot;, # Unique identifier for the rubric. This ID is used to refer to this rubric, e.g., in RubricVerdict.
                &quot;type&quot;: &quot;A String&quot;, # Optional. A type designator for the rubric, which can inform how it&#x27;s evaluated or interpreted by systems or users. It&#x27;s recommended to use consistent, well-defined, upper snake_case strings. Examples: &quot;SUMMARIZATION_QUALITY&quot;, &quot;SAFETY_HARMFUL_CONTENT&quot;, &quot;INSTRUCTION_ADHERENCE&quot;.
              },
            ],
          },
          &quot;judgeAutoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Optional configuration for the judge LLM (Autorater). The definition of AutoraterConfig needs to be provided.
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used by the judge model to evaluate against rubrics.
          &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics for evaluation using this specification.
            &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
              &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
              &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                  &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                  &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                    &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                    &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                  },
                  &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                },
                &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                  &quot;A String&quot;,
                ],
                &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                  &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                  &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                    # Object with schema name: GoogleCloudAiplatformV1Schema
                  ],
                  &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                  &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                  &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                    &quot;A String&quot;,
                  ],
                  &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                  &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                  &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                  &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                  &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                  &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                  &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                  &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                  &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                  &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                  &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                  &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                  &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                  &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                    &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                  },
                  &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                    &quot;A String&quot;,
                  ],
                  &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                  &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                    &quot;A String&quot;,
                  ],
                  &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                  &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                },
                &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                  &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                    &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                  },
                  &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                    &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                  },
                },
                &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                  &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                  &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                    &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                      { # Configuration for a single speaker in a multi-speaker setup.
                        &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                    ],
                  },
                  &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                    &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                      &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                    },
                  },
                },
                &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                  &quot;A String&quot;,
                ],
                &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                  &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                  &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                },
                &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
              },
              &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
            },
            &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
            &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
            &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
              &quot;A String&quot;,
            ],
          },
          &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input content. This refers to a key in the `rubric_groups` map of `RubricEnhancedContents`.
        },
      },
    ],
    &quot;outputConfig&quot;: { # The output config for the evaluation run. # Optional. The output config for the evaluation run.
      &quot;bigqueryDestination&quot;: { # The BigQuery location for the output content. # BigQuery destination for evaluation output.
        &quot;outputUri&quot;: &quot;A String&quot;, # Required. BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: * BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
      },
      &quot;gcsDestination&quot;: { # The Google Cloud Storage location where the output is to be written to. # Cloud Storage destination for evaluation output.
        &quot;outputUriPrefix&quot;: &quot;A String&quot;, # Required. Google Cloud Storage URI to output directory. If the uri doesn&#x27;t end with &#x27;/&#x27;, a &#x27;/&#x27; will be automatically appended. The directory is created if it doesn&#x27;t exist.
      },
    },
    &quot;promptTemplate&quot;: { # Prompt template used for inference. # The prompt template used for inference. The values for variables in the prompt template are defined in EvaluationItem.EvaluationPrompt.PromptTemplateData.values.
      &quot;gcsUri&quot;: &quot;A String&quot;, # Prompt template stored in Cloud Storage. Format: &quot;gs://my-bucket/file-name.txt&quot;.
      &quot;promptTemplate&quot;: &quot;A String&quot;, # Inline prompt template. Template variables should be in the format &quot;{var_name}&quot;. Example: &quot;Translate the following from {source_lang} to {target_lang}: {text}&quot;
    },
    &quot;rubricConfigs&quot;: [ # Optional. The rubric configs for the evaluation run. They are used to generate rubrics which can be used by rubric-based metrics. Multiple rubric configs can be specified for rubric generation but only one rubric config can be used for a rubric-based metric. If more than one rubric config is provided, the evaluation metric must specify a rubric group key. Note that if a generation spec is specified on both a rubric config and an evaluation metric, the rubrics generated for the metric will be used for evaluation.
      { # Configuration for a rubric group to be generated/saved for evaluation.
        &quot;predefinedRubricGenerationSpec&quot;: { # Specification for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
          &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
          &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
          },
        },
        &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
          &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
            &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
            &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
              &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
              &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
              &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
              &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
              &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                  &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                  &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                },
                &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
              },
              &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
              &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
              &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
              &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
              &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
              &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
              &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
              &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                &quot;A String&quot;,
              ],
              &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                  # Object with schema name: GoogleCloudAiplatformV1Schema
                ],
                &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                  &quot;A String&quot;,
                ],
                &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                  &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                },
                &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                  &quot;A String&quot;,
                ],
                &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                  &quot;A String&quot;,
                ],
                &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
              },
              &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                  &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                },
                &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                  &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                },
              },
              &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
              &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                  &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                    { # Configuration for a single speaker in a multi-speaker setup.
                      &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                  ],
                },
                &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
              &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                &quot;A String&quot;,
              ],
              &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
              &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
              },
              &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
              &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
            },
            &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
          },
          &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
          &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
          &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
            &quot;A String&quot;,
          ],
        },
        &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Required. The key used to save the generated rubrics. If a generation spec is provided, this key will be used for the name of the generated rubric group. Otherwise, this key will be used to look up the existing rubric group on the evaluation item. Note that if a rubric group key is specified on both a rubric config and an evaluation metric, the key from the metric will be used to select the rubrics for evaluation.
      },
    ],
  },
  &quot;evaluationResults&quot;: { # The results of the evaluation run. # Output only. The results of the evaluation run. Only populated when the evaluation run&#x27;s state is SUCCEEDED.
    &quot;evaluationSet&quot;: &quot;A String&quot;, # The evaluation set where item level results are stored.
    &quot;summaryMetrics&quot;: { # The summary metrics for the evaluation run. # Optional. The summary metrics for the evaluation run.
      &quot;failedItems&quot;: 42, # Optional. The number of items that failed to be evaluated.
      &quot;metrics&quot;: { # Optional. Map of metric name to metric value.
        &quot;a_key&quot;: &quot;&quot;,
      },
      &quot;totalItems&quot;: 42, # Optional. The total number of items that were evaluated.
    },
  },
  &quot;evaluationSetSnapshot&quot;: &quot;A String&quot;, # Output only. The specific evaluation set of the evaluation run. For runs with an evaluation set input, this will be that same set. For runs with BigQuery input, it&#x27;s the sampled BigQuery dataset.
  &quot;inferenceConfigs&quot;: { # Optional. The candidate to inference config map for the evaluation run. The candidate can be up to 128 characters long and can consist of any UTF-8 characters.
    &quot;a_key&quot;: { # An inference config used for model inference during the evaluation run.
      &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Generation config.
        &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
        &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
        &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
        &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
        &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
          &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
          &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
            &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
            &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
          },
          &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
        },
        &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
        &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
        &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
        &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
        &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
        &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
        &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
        &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
          &quot;A String&quot;,
        ],
        &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
          &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
          &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
            # Object with schema name: GoogleCloudAiplatformV1Schema
          ],
          &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
          &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
          &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
            &quot;A String&quot;,
          ],
          &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
          &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
          &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
          &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
          &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
          &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
          &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
          &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
          &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
          &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
          &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
          &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
          &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
          &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
            &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
          },
          &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
            &quot;A String&quot;,
          ],
          &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
          &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
            &quot;A String&quot;,
          ],
          &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
          &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
        },
        &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
          &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
            &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
          },
          &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
            &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
          },
        },
        &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
        &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
          &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
          &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
            &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
              { # Configuration for a single speaker in a multi-speaker setup.
                &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                  &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                    &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                  },
                },
              },
            ],
          },
          &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
            },
          },
        },
        &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
          &quot;A String&quot;,
        ],
        &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
        &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
          &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
          &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
        },
        &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
        &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
      },
      &quot;model&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
    },
  },
  &quot;labels&quot;: { # Optional. Labels for the evaluation run.
    &quot;a_key&quot;: &quot;A String&quot;,
  },
  &quot;metadata&quot;: &quot;&quot;, # Optional. Metadata about the evaluation run, can be used by the caller to store additional tracking information about the evaluation run.
  &quot;name&quot;: &quot;A String&quot;, # Identifier. The resource name of the EvaluationRun. This is a unique identifier. Format: `projects/{project}/locations/{location}/evaluationRuns/{evaluation_run}`
  &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the evaluation run.
}</pre>
</div>

<div class="method">
    <code class="details" id="list">list(parent, filter=None, orderBy=None, pageSize=None, pageToken=None, x__xgafv=None)</code>
  <pre>Lists Evaluation Runs.

Args:
  parent: string, Required. The resource name of the Location from which to list the Evaluation Runs. Format: `projects/{project}/locations/{location}` (required)
  filter: string, Optional. Filter expression that matches a subset of the EvaluationRuns to show. For field names both snake_case and camelCase are supported. For more information about filter syntax, see [AIP-160](https://google.aip.dev/160).
  orderBy: string, Optional. A comma-separated list of fields to order by, sorted in ascending order by default. Use `desc` after a field name for descending.
  pageSize: integer, Optional. The maximum number of Evaluation Runs to return.
  pageToken: string, Optional. A page token, received from a previous `ListEvaluationRuns` call. Provide this to retrieve the subsequent page.
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # Response message for EvaluationManagementService.ListEvaluationRuns.
  &quot;evaluationRuns&quot;: [ # List of EvaluationRuns in the requested page.
    { # EvaluationRun is a resource that represents a single evaluation run, which includes a set of prompts, model responses, evaluation configuration and the resulting metrics.
      &quot;completionTime&quot;: &quot;A String&quot;, # Output only. Time when the evaluation run was completed.
      &quot;createTime&quot;: &quot;A String&quot;, # Output only. Time when the evaluation run was created.
      &quot;dataSource&quot;: { # The data source for the evaluation run. # Required. The data source for the evaluation run.
        &quot;bigqueryRequestSet&quot;: { # The request set for the evaluation run. # Evaluation data in bigquery.
          &quot;candidateResponseColumns&quot;: { # Optional. Map of candidate name to candidate response column name. The column will be in evaluation_item.CandidateResponse format.
            &quot;a_key&quot;: &quot;A String&quot;,
          },
          &quot;promptColumn&quot;: &quot;A String&quot;, # Optional. The name of the column that contains the requests to evaluate. This will be in evaluation_item.EvalPrompt format.
          &quot;rubricsColumn&quot;: &quot;A String&quot;, # Optional. The name of the column that contains the rubrics. This is in evaluation_rubric.RubricGroup format.
          &quot;samplingConfig&quot;: { # The sampling config. # Optional. The sampling config for the bigquery resource.
            &quot;samplingCount&quot;: 42, # Optional. The total number of logged data to import. If available data is less than the sampling count, all data will be imported. Default is 100.
            &quot;samplingDuration&quot;: &quot;A String&quot;, # Optional. How long to wait before sampling data from the BigQuery table. If not specified, defaults to 0.
            &quot;samplingMethod&quot;: &quot;A String&quot;, # Optional. The sampling method to use.
          },
          &quot;uri&quot;: &quot;A String&quot;, # Required. The URI of a BigQuery table. e.g. bq://projectId.bqDatasetId.bqTableId
        },
        &quot;evaluationSet&quot;: &quot;A String&quot;, # The EvaluationSet resource name. Format: `projects/{project}/locations/{location}/evaluationSets/{evaluation_set}`
      },
      &quot;displayName&quot;: &quot;A String&quot;, # Required. The display name of the Evaluation Run.
      &quot;error&quot;: { # The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors). # Output only. Only populated when the evaluation run&#x27;s state is FAILED or CANCELLED.
        &quot;code&quot;: 42, # The status code, which should be an enum value of google.rpc.Code.
        &quot;details&quot;: [ # A list of messages that carry the error details. There is a common set of message types for APIs to use.
          {
            &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
          },
        ],
        &quot;message&quot;: &quot;A String&quot;, # A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
      },
      &quot;evaluationConfig&quot;: { # The Evalution configuration used for the evaluation run. # Required. The configuration used for the evaluation.
        &quot;autoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. The autorater config for the evaluation run.
          &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
          &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
            &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
            &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
            &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
            &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
            &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
              &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
              &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
              },
              &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
            },
            &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
            &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
            &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
            &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
            &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
            &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
            &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
            &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
              &quot;A String&quot;,
            ],
            &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
              &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
              &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                # Object with schema name: GoogleCloudAiplatformV1Schema
              ],
              &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
              &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
              },
              &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
              &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                &quot;A String&quot;,
              ],
              &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
              &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
              &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
              &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
              &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
              &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
              &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
              &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
              &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
              &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
              &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
              &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
              &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
              &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
              },
              &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                &quot;A String&quot;,
              ],
              &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
              &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                &quot;A String&quot;,
              ],
              &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
              &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
            },
            &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
              &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
              },
              &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
              },
            },
            &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
            &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
              &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
              &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                  { # Configuration for a single speaker in a multi-speaker setup.
                    &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                    &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                      &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                        &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                      },
                    },
                  },
                ],
              },
              &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                  &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                },
              },
            },
            &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
              &quot;A String&quot;,
            ],
            &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
            &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
              &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
              &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
            },
            &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
            &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
          },
          &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
        },
        &quot;metrics&quot;: [ # Required. The metrics to be calculated in the evaluation run.
          { # The metric used for evaluation runs.
            &quot;llmBasedMetricSpec&quot;: { # Specification for an LLM based metric. # Spec for an LLM based metric.
              &quot;additionalConfig&quot;: { # Optional. Optional additional configuration for the metric.
                &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
              },
              &quot;judgeAutoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Optional configuration for the judge LLM (Autorater).
                &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                  &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                  &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                  &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                  &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                  &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                    &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                    &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                      &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                      &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                    },
                    &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                  },
                  &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                  &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                  &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                  &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                  &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                  &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                  &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                  &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                    &quot;A String&quot;,
                  ],
                  &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                    &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                    &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                      # Object with schema name: GoogleCloudAiplatformV1Schema
                    ],
                    &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                    &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                    &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                      &quot;A String&quot;,
                    ],
                    &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                    &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                    &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                    &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                    &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                    &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                    &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                    &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                    &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                    &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                    &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                    &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                    &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                    &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                      &quot;A String&quot;,
                    ],
                    &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                    &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                      &quot;A String&quot;,
                    ],
                    &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                    &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                  },
                  &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                    &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                      &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                    },
                    &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                      &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                    },
                  },
                  &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                  &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                    &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                    &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                      &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                        { # Configuration for a single speaker in a multi-speaker setup.
                          &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                          &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                            },
                          },
                        },
                      ],
                    },
                    &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                      &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                        &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                      },
                    },
                  },
                  &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                    &quot;A String&quot;,
                  ],
                  &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                  &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                    &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                    &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                  },
                  &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                  &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                },
                &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
              },
              &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Template for the prompt sent to the judge model.
              &quot;predefinedRubricGenerationSpec&quot;: { # Specification for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
                &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
                &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
                  &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
                },
              },
              &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
                &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
                  &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                  &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                    &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                    &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                    &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                    &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                    &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                      &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                      &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                        &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                        &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                      },
                      &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                    },
                    &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                    &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                    &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                    &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                    &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                    &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                    &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                    &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                      &quot;A String&quot;,
                    ],
                    &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                      &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                      &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                        # Object with schema name: GoogleCloudAiplatformV1Schema
                      ],
                      &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                      &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                        &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                      },
                      &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                      &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                        &quot;A String&quot;,
                      ],
                      &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                      &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                      &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                      &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                      &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                      &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                      &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                      &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                      &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                      &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                      &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                      &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                      &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                      &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                        &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                      },
                      &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                        &quot;A String&quot;,
                      ],
                      &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                      &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                        &quot;A String&quot;,
                      ],
                      &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                      &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                    },
                    &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                      &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                        &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                      },
                      &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                        &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                      },
                    },
                    &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                    &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                      &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                      &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                        &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                          { # Configuration for a single speaker in a multi-speaker setup.
                            &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                            &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                              &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                                &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                              },
                            },
                          },
                        ],
                      },
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                    &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                      &quot;A String&quot;,
                    ],
                    &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                    &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                      &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                      &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                    },
                    &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                    &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                  },
                  &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
                },
                &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
                &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
                &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
                  &quot;A String&quot;,
                ],
              },
              &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input. Refers to a key in the rubric_groups map of EvaluationInstance.
              &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for the judge model.
            },
            &quot;metric&quot;: &quot;A String&quot;, # Required. The name of the metric.
            &quot;metricConfig&quot;: { # The metric used for running evaluations. # The metric config.
              &quot;aggregationMetrics&quot;: [ # Optional. The aggregation metrics to use.
                &quot;A String&quot;,
              ],
              &quot;bleuSpec&quot;: { # Spec for bleu score metric - calculates the precision of n-grams in the prediction as compared to reference - returns a score ranging between 0 to 1. # Spec for bleu metric.
                &quot;useEffectiveOrder&quot;: True or False, # Optional. Whether to use_effective_order to compute bleu score.
              },
              &quot;customCodeExecutionSpec&quot;: { # Specificies a metric that is populated by evaluating user-defined Python code. # Spec for Custom Code Execution metric.
                &quot;evaluationFunction&quot;: &quot;A String&quot;, # Required. Python function. Expected user to define the following function, e.g.: def evaluate(instance: dict[str, Any]) -&gt; float: Please include this function signature in the code snippet. Instance is the evaluation instance, any fields populated in the instance are available to the function as instance[field_name]. Example: Example input: ``` instance= EvaluationInstance( response=EvaluationInstance.InstanceData(text=&quot;The answer is 4.&quot;), reference=EvaluationInstance.InstanceData(text=&quot;4&quot;) ) ``` Example converted input: ``` { &#x27;response&#x27;: {&#x27;text&#x27;: &#x27;The answer is 4.&#x27;}, &#x27;reference&#x27;: {&#x27;text&#x27;: &#x27;4&#x27;} } ``` Example python function: ``` def evaluate(instance: dict[str, Any]) -&gt; float: if instance&#x27;response&#x27; == instance&#x27;reference&#x27;: return 1.0 return 0.0 ```
              },
              &quot;exactMatchSpec&quot;: { # Spec for exact match metric - returns 1 if prediction and reference exactly matches, otherwise 0. # Spec for exact match metric.
              },
              &quot;llmBasedMetricSpec&quot;: { # Specification for an LLM based metric. # Spec for an LLM based metric.
                &quot;additionalConfig&quot;: { # Optional. Optional additional configuration for the metric.
                  &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
                },
                &quot;judgeAutoraterConfig&quot;: { # The configs for autorater. This is applicable to both EvaluateInstances and EvaluateDataset. # Optional. Optional configuration for the judge LLM (Autorater).
                  &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                  &quot;flipEnabled&quot;: True or False, # Optional. Default is true. Whether to flip the candidate and baseline responses. This is only applicable to the pairwise metric. If enabled, also provide PairwiseMetricSpec.candidate_response_field_name and PairwiseMetricSpec.baseline_response_field_name. When rendering PairwiseMetricSpec.metric_prompt_template, the candidate and baseline fields will be flipped for half of the samples to reduce bias.
                  &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                    &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                    &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                    &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                    &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                    &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                      &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                      &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                        &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                        &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                      },
                      &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                    },
                    &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                    &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                    &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                    &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                    &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                    &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                    &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                    &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                      &quot;A String&quot;,
                    ],
                    &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                      &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                      &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                        # Object with schema name: GoogleCloudAiplatformV1Schema
                      ],
                      &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                      &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                        &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                      },
                      &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                      &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                        &quot;A String&quot;,
                      ],
                      &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                      &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                      &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                      &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                      &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                      &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                      &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                      &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                      &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                      &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                      &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                      &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                      &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                      &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                        &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                      },
                      &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                        &quot;A String&quot;,
                      ],
                      &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                      &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                        &quot;A String&quot;,
                      ],
                      &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                      &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                    },
                    &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                      &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                        &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                      },
                      &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                        &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                      },
                    },
                    &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                    &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                      &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                      &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                        &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                          { # Configuration for a single speaker in a multi-speaker setup.
                            &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                            &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                              &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                                &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                              },
                            },
                          },
                        ],
                      },
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                    &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                      &quot;A String&quot;,
                    ],
                    &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                    &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                      &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                      &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                    },
                    &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                    &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                  },
                  &quot;samplingCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
                },
                &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Template for the prompt sent to the judge model.
                &quot;predefinedRubricGenerationSpec&quot;: { # The spec for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
                  &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
                  &quot;metricSpecParameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
                    &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
                  },
                },
                &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
                  &quot;modelConfig&quot;: { # The configs for autorater. This is applicable to both EvaluateInstances and EvaluateDataset. # Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
                    &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                    &quot;flipEnabled&quot;: True or False, # Optional. Default is true. Whether to flip the candidate and baseline responses. This is only applicable to the pairwise metric. If enabled, also provide PairwiseMetricSpec.candidate_response_field_name and PairwiseMetricSpec.baseline_response_field_name. When rendering PairwiseMetricSpec.metric_prompt_template, the candidate and baseline fields will be flipped for half of the samples to reduce bias.
                    &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                      &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                      &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                      &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                      &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                      &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                        &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                        &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                          &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                          &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                        },
                        &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                      },
                      &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                      &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                      &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                      &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                      &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                      &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                      &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                      &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                        &quot;A String&quot;,
                      ],
                      &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                        &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                        &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                          # Object with schema name: GoogleCloudAiplatformV1Schema
                        ],
                        &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                        &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                          &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                        },
                        &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                        &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                          &quot;A String&quot;,
                        ],
                        &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                        &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                        &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                        &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                        &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                        &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                        &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                        &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                        &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                        &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                        &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                        &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                        &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                        &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                          &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                        },
                        &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                          &quot;A String&quot;,
                        ],
                        &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                        &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                          &quot;A String&quot;,
                        ],
                        &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                        &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                      },
                      &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                        &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                          &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                        },
                        &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                          &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                        },
                      },
                      &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                      &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                        &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                        &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                          &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                            { # Configuration for a single speaker in a multi-speaker setup.
                              &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                              &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                                &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                                  &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                                },
                              },
                            },
                          ],
                        },
                        &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                          &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                            &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                          },
                        },
                      },
                      &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                        &quot;A String&quot;,
                      ],
                      &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                      &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                        &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                        &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                      },
                      &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                      &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                    },
                    &quot;samplingCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
                  },
                  &quot;promptTemplate&quot;: &quot;A String&quot;, # Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
                  &quot;rubricContentType&quot;: &quot;A String&quot;, # The type of rubric content to be generated.
                  &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
                    &quot;A String&quot;,
                  ],
                },
                &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input. Refers to a key in the rubric_groups map of EvaluationInstance.
                &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for the judge model.
              },
              &quot;pairwiseMetricSpec&quot;: { # Spec for pairwise metric. # Spec for pairwise metric.
                &quot;baselineResponseFieldName&quot;: &quot;A String&quot;, # Optional. The field name of the baseline response.
                &quot;candidateResponseFieldName&quot;: &quot;A String&quot;, # Optional. The field name of the candidate response.
                &quot;customOutputFormatConfig&quot;: { # Spec for custom output format configuration. # Optional. CustomOutputFormatConfig allows customization of metric output. When this config is set, the default output is replaced with the raw output string. If a custom format is chosen, the `pairwise_choice` and `explanation` fields in the corresponding metric result will be empty.
                  &quot;returnRawOutput&quot;: True or False, # Optional. Whether to return raw output.
                },
                &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Metric prompt template for pairwise metric.
                &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for pairwise metric.
              },
              &quot;pointwiseMetricSpec&quot;: { # Spec for pointwise metric. # Spec for pointwise metric.
                &quot;customOutputFormatConfig&quot;: { # Spec for custom output format configuration. # Optional. CustomOutputFormatConfig allows customization of metric output. By default, metrics return a score and explanation. When this config is set, the default output is replaced with either: - The raw output string. - A parsed output based on a user-defined schema. If a custom format is chosen, the `score` and `explanation` fields in the corresponding metric result will be empty.
                  &quot;returnRawOutput&quot;: True or False, # Optional. Whether to return raw output.
                },
                &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Required. Metric prompt template for pointwise metric.
                &quot;systemInstruction&quot;: &quot;A String&quot;, # Optional. System instructions for pointwise metric.
              },
              &quot;predefinedMetricSpec&quot;: { # The spec for a pre-defined metric. # The spec for a pre-defined metric.
                &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
                &quot;metricSpecParameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
                  &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
                },
              },
              &quot;rougeSpec&quot;: { # Spec for rouge score metric - calculates the recall of n-grams in prediction as compared to reference - returns a score ranging between 0 and 1. # Spec for rouge metric.
                &quot;rougeType&quot;: &quot;A String&quot;, # Optional. Supported rouge types are rougen[1-9], rougeL, and rougeLsum.
                &quot;splitSummaries&quot;: True or False, # Optional. Whether to split summaries while using rougeLsum.
                &quot;useStemmer&quot;: True or False, # Optional. Whether to use stemmer to compute rouge score.
              },
            },
            &quot;predefinedMetricSpec&quot;: { # Specification for a pre-defined metric. # Spec for a pre-defined metric.
              &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
              &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
                &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
              },
            },
            &quot;rubricBasedMetricSpec&quot;: { # Specification for a metric that is based on rubrics. # Spec for rubric based metric.
              &quot;inlineRubrics&quot;: { # Defines a list of rubrics, used when providing rubrics inline. # Use rubrics provided directly in the spec.
                &quot;rubrics&quot;: [ # The list of rubrics.
                  { # Message representing a single testable criterion for evaluation. One input prompt could have multiple rubrics.
                    &quot;content&quot;: { # Content of the rubric, defining the testable criteria. # Required. The actual testable criteria for the rubric.
                      &quot;property&quot;: { # Defines criteria based on a specific property. # Evaluation criteria based on a specific property.
                        &quot;description&quot;: &quot;A String&quot;, # Description of the property being evaluated. Example: &quot;The model&#x27;s response is grammatically correct.&quot;
                      },
                    },
                    &quot;importance&quot;: &quot;A String&quot;, # Optional. The relative importance of this rubric.
                    &quot;rubricId&quot;: &quot;A String&quot;, # Unique identifier for the rubric. This ID is used to refer to this rubric, e.g., in RubricVerdict.
                    &quot;type&quot;: &quot;A String&quot;, # Optional. A type designator for the rubric, which can inform how it&#x27;s evaluated or interpreted by systems or users. It&#x27;s recommended to use consistent, well-defined, upper snake_case strings. Examples: &quot;SUMMARIZATION_QUALITY&quot;, &quot;SAFETY_HARMFUL_CONTENT&quot;, &quot;INSTRUCTION_ADHERENCE&quot;.
                  },
                ],
              },
              &quot;judgeAutoraterConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Optional configuration for the judge LLM (Autorater). The definition of AutoraterConfig needs to be provided.
                &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                  &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                  &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                  &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                  &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                  &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                    &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                    &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                      &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                      &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                    },
                    &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                  },
                  &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                  &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                  &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                  &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                  &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                  &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                  &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                  &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                    &quot;A String&quot;,
                  ],
                  &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                    &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                    &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                      # Object with schema name: GoogleCloudAiplatformV1Schema
                    ],
                    &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                    &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                    &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                      &quot;A String&quot;,
                    ],
                    &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                    &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                    &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                    &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                    &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                    &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                    &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                    &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                    &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                    &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                    &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                    &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                    &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                    &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                      &quot;A String&quot;,
                    ],
                    &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                    &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                      &quot;A String&quot;,
                    ],
                    &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                    &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                  },
                  &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                    &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                      &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                    },
                    &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                      &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                    },
                  },
                  &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                  &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                    &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                    &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                      &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                        { # Configuration for a single speaker in a multi-speaker setup.
                          &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                          &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                            },
                          },
                        },
                      ],
                    },
                    &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                      &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                        &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                      },
                    },
                  },
                  &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                    &quot;A String&quot;,
                  ],
                  &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                  &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                    &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                    &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                  },
                  &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                  &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                },
                &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
              },
              &quot;metricPromptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used by the judge model to evaluate against rubrics.
              &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics for evaluation using this specification.
                &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
                  &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                  &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                    &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                    &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                    &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                    &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                    &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                      &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                      &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                        &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                        &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                      },
                      &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                    },
                    &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                    &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                    &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                    &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                    &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                    &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                    &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                    &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                      &quot;A String&quot;,
                    ],
                    &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                      &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                      &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                        # Object with schema name: GoogleCloudAiplatformV1Schema
                      ],
                      &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                      &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                        &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                      },
                      &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                      &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                        &quot;A String&quot;,
                      ],
                      &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                      &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                      &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                      &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                      &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                      &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                      &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                      &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                      &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                      &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                      &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                      &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                      &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                      &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                        &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                      },
                      &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                        &quot;A String&quot;,
                      ],
                      &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                      &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                        &quot;A String&quot;,
                      ],
                      &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                      &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                    },
                    &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                      &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                        &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                      },
                      &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                        &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                      },
                    },
                    &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                    &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                      &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                      &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                        &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                          { # Configuration for a single speaker in a multi-speaker setup.
                            &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                            &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                              &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                                &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                              },
                            },
                          },
                        ],
                      },
                      &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                        &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                          &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                        },
                      },
                    },
                    &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                      &quot;A String&quot;,
                    ],
                    &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                    &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                      &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                      &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                    },
                    &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                    &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                  },
                  &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
                },
                &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
                &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
                &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
                  &quot;A String&quot;,
                ],
              },
              &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Use a pre-defined group of rubrics associated with the input content. This refers to a key in the `rubric_groups` map of `RubricEnhancedContents`.
            },
          },
        ],
        &quot;outputConfig&quot;: { # The output config for the evaluation run. # Optional. The output config for the evaluation run.
          &quot;bigqueryDestination&quot;: { # The BigQuery location for the output content. # BigQuery destination for evaluation output.
            &quot;outputUri&quot;: &quot;A String&quot;, # Required. BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: * BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
          },
          &quot;gcsDestination&quot;: { # The Google Cloud Storage location where the output is to be written to. # Cloud Storage destination for evaluation output.
            &quot;outputUriPrefix&quot;: &quot;A String&quot;, # Required. Google Cloud Storage URI to output directory. If the uri doesn&#x27;t end with &#x27;/&#x27;, a &#x27;/&#x27; will be automatically appended. The directory is created if it doesn&#x27;t exist.
          },
        },
        &quot;promptTemplate&quot;: { # Prompt template used for inference. # The prompt template used for inference. The values for variables in the prompt template are defined in EvaluationItem.EvaluationPrompt.PromptTemplateData.values.
          &quot;gcsUri&quot;: &quot;A String&quot;, # Prompt template stored in Cloud Storage. Format: &quot;gs://my-bucket/file-name.txt&quot;.
          &quot;promptTemplate&quot;: &quot;A String&quot;, # Inline prompt template. Template variables should be in the format &quot;{var_name}&quot;. Example: &quot;Translate the following from {source_lang} to {target_lang}: {text}&quot;
        },
        &quot;rubricConfigs&quot;: [ # Optional. The rubric configs for the evaluation run. They are used to generate rubrics which can be used by rubric-based metrics. Multiple rubric configs can be specified for rubric generation but only one rubric config can be used for a rubric-based metric. If more than one rubric config is provided, the evaluation metric must specify a rubric group key. Note that if a generation spec is specified on both a rubric config and an evaluation metric, the rubrics generated for the metric will be used for evaluation.
          { # Configuration for a rubric group to be generated/saved for evaluation.
            &quot;predefinedRubricGenerationSpec&quot;: { # Specification for a pre-defined metric. # Dynamically generate rubrics using a predefined spec.
              &quot;metricSpecName&quot;: &quot;A String&quot;, # Required. The name of a pre-defined metric, such as &quot;instruction_following_v1&quot; or &quot;text_quality_v1&quot;.
              &quot;parameters&quot;: { # Optional. The parameters needed to run the pre-defined metric.
                &quot;a_key&quot;: &quot;&quot;, # Properties of the object.
              },
            },
            &quot;rubricGenerationSpec&quot;: { # Specification for how rubrics should be generated. # Dynamically generate rubrics using this specification.
              &quot;modelConfig&quot;: { # The autorater config used for the evaluation run. # Optional. Configuration for the model used in rubric generation. Configs including sampling count and base model can be specified here. Flipping is not supported for rubric generation.
                &quot;autoraterModel&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or tuned autorater endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Tuned model endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
                &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Configuration options for model generation and outputs.
                  &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
                  &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
                  &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
                  &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
                  &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
                    &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
                    &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                      &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                      &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
                    },
                    &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
                  },
                  &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
                  &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
                  &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
                  &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
                  &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
                  &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
                  &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
                  &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
                    &quot;A String&quot;,
                  ],
                  &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
                    &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
                    &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                      # Object with schema name: GoogleCloudAiplatformV1Schema
                    ],
                    &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
                    &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
                    &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                      &quot;A String&quot;,
                    ],
                    &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
                    &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
                    &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
                    &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
                    &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
                    &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
                    &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
                    &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
                    &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
                    &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
                    &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
                    &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
                    &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
                    &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                      &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
                    },
                    &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                      &quot;A String&quot;,
                    ],
                    &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
                    &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                      &quot;A String&quot;,
                    ],
                    &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
                    &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
                  },
                  &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
                    &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                      &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
                    },
                    &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                      &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
                    },
                  },
                  &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
                  &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
                    &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
                    &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                      &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                        { # Configuration for a single speaker in a multi-speaker setup.
                          &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                          &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                            &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                              &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                            },
                          },
                        },
                      ],
                    },
                    &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                      &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                        &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                      },
                    },
                  },
                  &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
                    &quot;A String&quot;,
                  ],
                  &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
                  &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
                    &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
                    &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
                  },
                  &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
                  &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
                },
                &quot;sampleCount&quot;: 42, # Optional. Number of samples for each instance in the dataset. If not specified, the default is 4. Minimum value is 1, maximum value is 32.
              },
              &quot;promptTemplate&quot;: &quot;A String&quot;, # Optional. Template for the prompt used to generate rubrics. The details should be updated based on the most-recent recipe requirements.
              &quot;rubricContentType&quot;: &quot;A String&quot;, # Optional. The type of rubric content to be generated.
              &quot;rubricTypeOntology&quot;: [ # Optional. An optional, pre-defined list of allowed types for generated rubrics. If this field is provided, it implies `include_rubric_type` should be true, and the generated rubric types should be chosen from this ontology.
                &quot;A String&quot;,
              ],
            },
            &quot;rubricGroupKey&quot;: &quot;A String&quot;, # Required. The key used to save the generated rubrics. If a generation spec is provided, this key will be used for the name of the generated rubric group. Otherwise, this key will be used to look up the existing rubric group on the evaluation item. Note that if a rubric group key is specified on both a rubric config and an evaluation metric, the key from the metric will be used to select the rubrics for evaluation.
          },
        ],
      },
      &quot;evaluationResults&quot;: { # The results of the evaluation run. # Output only. The results of the evaluation run. Only populated when the evaluation run&#x27;s state is SUCCEEDED.
        &quot;evaluationSet&quot;: &quot;A String&quot;, # The evaluation set where item level results are stored.
        &quot;summaryMetrics&quot;: { # The summary metrics for the evaluation run. # Optional. The summary metrics for the evaluation run.
          &quot;failedItems&quot;: 42, # Optional. The number of items that failed to be evaluated.
          &quot;metrics&quot;: { # Optional. Map of metric name to metric value.
            &quot;a_key&quot;: &quot;&quot;,
          },
          &quot;totalItems&quot;: 42, # Optional. The total number of items that were evaluated.
        },
      },
      &quot;evaluationSetSnapshot&quot;: &quot;A String&quot;, # Output only. The specific evaluation set of the evaluation run. For runs with an evaluation set input, this will be that same set. For runs with BigQuery input, it&#x27;s the sampled BigQuery dataset.
      &quot;inferenceConfigs&quot;: { # Optional. The candidate to inference config map for the evaluation run. The candidate can be up to 128 characters long and can consist of any UTF-8 characters.
        &quot;a_key&quot;: { # An inference config used for model inference during the evaluation run.
          &quot;generationConfig&quot;: { # Configuration for content generation. This message contains all the parameters that control how the model generates content. It allows you to influence the randomness, length, and structure of the output. # Optional. Generation config.
            &quot;audioTimestamp&quot;: True or False, # Optional. If enabled, audio timestamps will be included in the request to the model. This can be useful for synchronizing audio with other modalities in the response.
            &quot;candidateCount&quot;: 42, # Optional. The number of candidate responses to generate. A higher `candidate_count` can provide more options to choose from, but it also consumes more resources. This can be useful for generating a variety of responses and selecting the best one.
            &quot;enableAffectiveDialog&quot;: True or False, # Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For example, if the model detects that the user is frustrated, it may provide a more empathetic response.
            &quot;frequencyPenalty&quot;: 3.14, # Optional. Penalizes tokens based on their frequency in the generated text. A positive value helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
            &quot;imageConfig&quot;: { # Configuration for image generation. This message allows you to control various aspects of image generation, such as the output format, aspect ratio, and whether the model can generate images of people. # Optional. Config for image generation features.
              &quot;aspectRatio&quot;: &quot;A String&quot;, # Optional. The desired aspect ratio for the generated images. The following aspect ratios are supported: &quot;1:1&quot; &quot;2:3&quot;, &quot;3:2&quot; &quot;3:4&quot;, &quot;4:3&quot; &quot;4:5&quot;, &quot;5:4&quot; &quot;9:16&quot;, &quot;16:9&quot; &quot;21:9&quot;
              &quot;imageOutputOptions&quot;: { # The image output format for generated images. # Optional. The image output format for generated images.
                &quot;compressionQuality&quot;: 42, # Optional. The compression quality of the output image.
                &quot;mimeType&quot;: &quot;A String&quot;, # Optional. The image format that the output should be saved as.
              },
              &quot;personGeneration&quot;: &quot;A String&quot;, # Optional. Controls whether the model can generate people.
            },
            &quot;logprobs&quot;: 42, # Optional. The number of top log probabilities to return for each token. This can be used to see which other tokens were considered likely candidates for a given position. A higher value will return more options, but it will also increase the size of the response.
            &quot;maxOutputTokens&quot;: 42, # Optional. The maximum number of tokens to generate in the response. A token is approximately four characters. The default value varies by model. This parameter can be used to control the length of the generated text and prevent overly long responses.
            &quot;mediaResolution&quot;: &quot;A String&quot;, # Optional. The token resolution at which input media content is sampled. This is used to control the trade-off between the quality of the response and the number of tokens used to represent the media. A higher resolution allows the model to perceive more detail, which can lead to a more nuanced response, but it will also use more tokens. This does not affect the image dimensions sent to the model.
            &quot;presencePenalty&quot;: 3.14, # Optional. Penalizes tokens that have already appeared in the generated text. A positive value encourages the model to generate more diverse and less repetitive text. Valid values can range from [-2.0, 2.0].
            &quot;responseJsonSchema&quot;: &quot;&quot;, # Optional. When this field is set, response_schema must be omitted and response_mime_type must be set to `application/json`.
            &quot;responseLogprobs&quot;: True or False, # Optional. If set to true, the log probabilities of the output tokens are returned. Log probabilities are the logarithm of the probability of a token appearing in the output. A higher log probability means the token is more likely to be generated. This can be useful for analyzing the model&#x27;s confidence in its own output and for debugging.
            &quot;responseMimeType&quot;: &quot;A String&quot;, # Optional. The IANA standard MIME type of the response. The model will generate output that conforms to this MIME type. Supported values include &#x27;text/plain&#x27; (default) and &#x27;application/json&#x27;. The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined. This is a preview feature.
            &quot;responseModalities&quot;: [ # Optional. The modalities of the response. The model will generate a response that includes all the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will include both text and an image.
              &quot;A String&quot;,
            ],
            &quot;responseSchema&quot;: { # Schema is used to define the format of input/output data. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema-object). More fields may be added in the future as needed. # Optional. Lets you to specify a schema for the model&#x27;s response, ensuring that the output conforms to a particular structure. This is useful for generating structured data such as JSON. The schema is a subset of the [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must also set the `response_mime_type` to `application/json`.
              &quot;additionalProperties&quot;: &quot;&quot;, # Optional. Can either be a boolean or an object; controls the presence of additional properties.
              &quot;anyOf&quot;: [ # Optional. The value should be validated against any (one or more) of the subschemas in the list.
                # Object with schema name: GoogleCloudAiplatformV1Schema
              ],
              &quot;default&quot;: &quot;&quot;, # Optional. Default value of the data.
              &quot;defs&quot;: { # Optional. A map of definitions for use by `ref` Only allowed at the root of the schema.
                &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
              },
              &quot;description&quot;: &quot;A String&quot;, # Optional. The description of the data.
              &quot;enum&quot;: [ # Optional. Possible values of the element of primitive type with enum format. Examples: 1. We can define direction as : {type:STRING, format:enum, enum:[&quot;EAST&quot;, NORTH&quot;, &quot;SOUTH&quot;, &quot;WEST&quot;]} 2. We can define apartment number as : {type:INTEGER, format:enum, enum:[&quot;101&quot;, &quot;201&quot;, &quot;301&quot;]}
                &quot;A String&quot;,
              ],
              &quot;example&quot;: &quot;&quot;, # Optional. Example of the object. Will only populated when the object is the root.
              &quot;format&quot;: &quot;A String&quot;, # Optional. The format of the data. Supported formats: for NUMBER type: &quot;float&quot;, &quot;double&quot; for INTEGER type: &quot;int32&quot;, &quot;int64&quot; for STRING type: &quot;email&quot;, &quot;byte&quot;, etc
              &quot;items&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema # Optional. SCHEMA FIELDS FOR TYPE ARRAY Schema of the elements of Type.ARRAY.
              &quot;maxItems&quot;: &quot;A String&quot;, # Optional. Maximum number of the elements for Type.ARRAY.
              &quot;maxLength&quot;: &quot;A String&quot;, # Optional. Maximum length of the Type.STRING
              &quot;maxProperties&quot;: &quot;A String&quot;, # Optional. Maximum number of the properties for Type.OBJECT.
              &quot;maximum&quot;: 3.14, # Optional. Maximum value of the Type.INTEGER and Type.NUMBER
              &quot;minItems&quot;: &quot;A String&quot;, # Optional. Minimum number of the elements for Type.ARRAY.
              &quot;minLength&quot;: &quot;A String&quot;, # Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
              &quot;minProperties&quot;: &quot;A String&quot;, # Optional. Minimum number of the properties for Type.OBJECT.
              &quot;minimum&quot;: 3.14, # Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
              &quot;nullable&quot;: True or False, # Optional. Indicates if the value may be null.
              &quot;pattern&quot;: &quot;A String&quot;, # Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
              &quot;properties&quot;: { # Optional. SCHEMA FIELDS FOR TYPE OBJECT Properties of Type.OBJECT.
                &quot;a_key&quot;: # Object with schema name: GoogleCloudAiplatformV1Schema
              },
              &quot;propertyOrdering&quot;: [ # Optional. The order of the properties. Not a standard field in open api spec. Only used to support the order of the properties.
                &quot;A String&quot;,
              ],
              &quot;ref&quot;: &quot;A String&quot;, # Optional. Allows indirect references between schema nodes. The value should be a valid reference to a child of the root `defs`. For example, the following schema defines a reference to a schema node named &quot;Pet&quot;: type: object properties: pet: ref: #/defs/Pet defs: Pet: type: object properties: name: type: string The value of the &quot;pet&quot; property is a reference to the schema node named &quot;Pet&quot;. See details in https://json-schema.org/understanding-json-schema/structuring
              &quot;required&quot;: [ # Optional. Required properties of Type.OBJECT.
                &quot;A String&quot;,
              ],
              &quot;title&quot;: &quot;A String&quot;, # Optional. The title of the Schema.
              &quot;type&quot;: &quot;A String&quot;, # Optional. The type of the data.
            },
            &quot;routingConfig&quot;: { # The configuration for routing the request to a specific model. This can be used to control which model is used for the generation, either automatically or by specifying a model name. # Optional. Routing configuration.
              &quot;autoMode&quot;: { # The configuration for automated routing. When automated routing is specified, the routing will be determined by the pretrained routing model and customer provided model routing preference. # In this mode, the model is selected automatically based on the content of the request.
                &quot;modelRoutingPreference&quot;: &quot;A String&quot;, # The model routing preference.
              },
              &quot;manualMode&quot;: { # The configuration for manual routing. When manual routing is specified, the model will be selected based on the model name provided. # In this mode, the model is specified manually.
                &quot;modelName&quot;: &quot;A String&quot;, # The name of the model to use. Only public LLM models are accepted.
              },
            },
            &quot;seed&quot;: 42, # Optional. A seed for the random number generator. By setting a seed, you can make the model&#x27;s output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.), the model will produce the same response every time. However, it&#x27;s not a guaranteed absolute deterministic behavior. This is different from parameters like `temperature`, which control the *level* of randomness. `seed` ensures that the &quot;random&quot; choices the model makes are the same on every run, making it essential for testing and ensuring reproducible results.
            &quot;speechConfig&quot;: { # Configuration for speech generation. # Optional. The speech generation config.
              &quot;languageCode&quot;: &quot;A String&quot;, # Optional. The language code (ISO 639-1) for the speech synthesis.
              &quot;multiSpeakerVoiceConfig&quot;: { # Configuration for a multi-speaker text-to-speech request. # The configuration for a multi-speaker text-to-speech request. This field is mutually exclusive with `voice_config`.
                &quot;speakerVoiceConfigs&quot;: [ # Required. A list of configurations for the voices of the speakers. Exactly two speaker voice configurations must be provided.
                  { # Configuration for a single speaker in a multi-speaker setup.
                    &quot;speaker&quot;: &quot;A String&quot;, # Required. The name of the speaker. This should be the same as the speaker name used in the prompt.
                    &quot;voiceConfig&quot;: { # Configuration for a voice. # Required. The configuration for the voice of this speaker.
                      &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                        &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                      },
                    },
                  },
                ],
              },
              &quot;voiceConfig&quot;: { # Configuration for a voice. # The configuration for the voice to use.
                &quot;prebuiltVoiceConfig&quot;: { # Configuration for a prebuilt voice. # The configuration for a prebuilt voice.
                  &quot;voiceName&quot;: &quot;A String&quot;, # The name of the prebuilt voice to use.
                },
              },
            },
            &quot;stopSequences&quot;: [ # Optional. A list of character sequences that will stop the model from generating further tokens. If a stop sequence is generated, the output will end at that point. This is useful for controlling the length and structure of the output. For example, you can use [&quot;\n&quot;, &quot;###&quot;] to stop generation at a new line or a specific marker.
              &quot;A String&quot;,
            ],
            &quot;temperature&quot;: 3.14, # Optional. Controls the randomness of the output. A higher temperature results in more creative and diverse responses, while a lower temperature makes the output more predictable and focused. The valid range is (0.0, 2.0].
            &quot;thinkingConfig&quot;: { # Configuration for the model&#x27;s thinking features. &quot;Thinking&quot; is a process where the model breaks down a complex task into smaller, manageable steps. This allows the model to reason about the task, plan its approach, and execute the plan to generate a high-quality response. # Optional. Configuration for thinking features. An error will be returned if this field is set for models that don&#x27;t support thinking.
              &quot;includeThoughts&quot;: True or False, # Optional. If true, the model will include its thoughts in the response. &quot;Thoughts&quot; are the intermediate steps the model takes to arrive at the final response. They can provide insights into the model&#x27;s reasoning process and help with debugging. If this is true, thoughts are returned only when available.
              &quot;thinkingBudget&quot;: 42, # Optional. The token budget for the model&#x27;s thinking process. The model will make a best effort to stay within this budget. This can be used to control the trade-off between response quality and latency.
            },
            &quot;topK&quot;: 3.14, # Optional. Specifies the top-k sampling threshold. The model considers only the top k most probable tokens for the next token. This can be useful for generating more coherent and less random text. For example, a `top_k` of 40 means the model will choose the next word from the 40 most likely words.
            &quot;topP&quot;: 3.14, # Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens until the cumulative probability of the tokens to select from reaches 0.9. It&#x27;s recommended to adjust either temperature or `top_p`, but not both.
          },
          &quot;model&quot;: &quot;A String&quot;, # Optional. The fully qualified name of the publisher model or endpoint to use. Publisher model format: `projects/{project}/locations/{location}/publishers/*/models/*` Endpoint format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
        },
      },
      &quot;labels&quot;: { # Optional. Labels for the evaluation run.
        &quot;a_key&quot;: &quot;A String&quot;,
      },
      &quot;metadata&quot;: &quot;&quot;, # Optional. Metadata about the evaluation run, can be used by the caller to store additional tracking information about the evaluation run.
      &quot;name&quot;: &quot;A String&quot;, # Identifier. The resource name of the EvaluationRun. This is a unique identifier. Format: `projects/{project}/locations/{location}/evaluationRuns/{evaluation_run}`
      &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the evaluation run.
    },
  ],
  &quot;nextPageToken&quot;: &quot;A String&quot;, # A token to retrieve the next page of results.
}</pre>
</div>

<div class="method">
    <code class="details" id="list_next">list_next()</code>
  <pre>Retrieves the next page of results.

        Args:
          previous_request: The request for the previous page. (required)
          previous_response: The response from the request for the previous page. (required)

        Returns:
          A request object that you can call &#x27;execute()&#x27; on to request the next
          page. Returns None if there are no more items in the collection.
        </pre>
</div>

</body></html>