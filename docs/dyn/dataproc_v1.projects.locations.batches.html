<html><body>
<style>

body, h1, h2, h3, div, span, p, pre, a {
  margin: 0;
  padding: 0;
  border: 0;
  font-weight: inherit;
  font-style: inherit;
  font-size: 100%;
  font-family: inherit;
  vertical-align: baseline;
}

body {
  font-size: 13px;
  padding: 1em;
}

h1 {
  font-size: 26px;
  margin-bottom: 1em;
}

h2 {
  font-size: 24px;
  margin-bottom: 1em;
}

h3 {
  font-size: 20px;
  margin-bottom: 1em;
  margin-top: 1em;
}

pre, code {
  line-height: 1.5;
  font-family: Monaco, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console', monospace;
}

pre {
  margin-top: 0.5em;
}

h1, h2, h3, p {
  font-family: Arial, sans serif;
}

h1, h2, h3 {
  border-bottom: solid #CCC 1px;
}

.toc_element {
  margin-top: 0.5em;
}

.firstline {
  margin-left: 2 em;
}

.method  {
  margin-top: 1em;
  border: solid 1px #CCC;
  padding: 1em;
  background: #EEE;
}

.details {
  font-weight: bold;
  font-size: 14px;
}

</style>

<h1><a href="dataproc_v1.html">Cloud Dataproc API</a> . <a href="dataproc_v1.projects.html">projects</a> . <a href="dataproc_v1.projects.locations.html">locations</a> . <a href="dataproc_v1.projects.locations.batches.html">batches</a></h1>
<h2>Instance Methods</h2>
<p class="toc_element">
  <code><a href="#close">close()</a></code></p>
<p class="firstline">Close httplib2 connections.</p>
<p class="toc_element">
  <code><a href="#create">create(parent, batchId=None, body=None, requestId=None, x__xgafv=None)</a></code></p>
<p class="firstline">Creates a batch workload that executes asynchronously.</p>
<p class="toc_element">
  <code><a href="#delete">delete(name, x__xgafv=None)</a></code></p>
<p class="firstline">Deletes the batch workload resource. If the batch is not in terminal state, the delete fails and the response returns FAILED_PRECONDITION.</p>
<p class="toc_element">
  <code><a href="#get">get(name, x__xgafv=None)</a></code></p>
<p class="firstline">Gets the batch workload resource representation.</p>
<p class="toc_element">
  <code><a href="#list">list(parent, filter=None, orderBy=None, pageSize=None, pageToken=None, x__xgafv=None)</a></code></p>
<p class="firstline">Lists batch workloads.</p>
<p class="toc_element">
  <code><a href="#list_next">list_next()</a></code></p>
<p class="firstline">Retrieves the next page of results.</p>
<h3>Method Details</h3>
<div class="method">
    <code class="details" id="close">close()</code>
  <pre>Close httplib2 connections.</pre>
</div>

<div class="method">
    <code class="details" id="create">create(parent, batchId=None, body=None, requestId=None, x__xgafv=None)</code>
  <pre>Creates a batch workload that executes asynchronously.

Args:
  parent: string, Required. The parent resource where this batch will be created. (required)
  body: object, The request body.
    The object takes the form of:

{ # A representation of a batch workload in the service.
  &quot;createTime&quot;: &quot;A String&quot;, # Output only. The time when the batch was created.
  &quot;creator&quot;: &quot;A String&quot;, # Output only. The email address of the user who created the batch.
  &quot;environmentConfig&quot;: { # Environment configuration for a workload. # Optional. Environment configuration for the batch execution.
    &quot;executionConfig&quot;: { # Execution configuration for a workload. # Optional. Execution configuration for a workload.
      &quot;idleTtl&quot;: &quot;A String&quot;, # Optional. The duration to keep the session alive while it&#x27;s idling. Passing this threshold will cause the session to be terminated. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified, the conditions are treated as and OR: the workload will be terminated when it has been idle for idle_ttl or when the ttl has passed, whichever comes first.
      &quot;kmsKey&quot;: &quot;A String&quot;, # Optional. The Cloud KMS key to use for encryption.
      &quot;networkTags&quot;: [ # Optional. Tags used for network traffic control.
        &quot;A String&quot;,
      ],
      &quot;networkUri&quot;: &quot;A String&quot;, # Optional. Network URI to connect workload to.
      &quot;serviceAccount&quot;: &quot;A String&quot;, # Optional. Service account that used to execute workload.
      &quot;stagingBucket&quot;: &quot;A String&quot;, # Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
      &quot;subnetworkUri&quot;: &quot;A String&quot;, # Optional. Subnetwork URI to connect workload to.
      &quot;ttl&quot;: &quot;A String&quot;, # Optional. The duration after which the workload will be terminated. When the workload passes this ttl, it will be unconditionally killed without waiting for ongoing work to finish. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified, the conditions are treated as and OR: the workload will be terminated when it has been idle for idle_ttl or when the ttl has passed, whichever comes first. If ttl is not specified for a session, it defaults to 24h.
    },
    &quot;peripheralsConfig&quot;: { # Auxiliary services configuration for a workload. # Optional. Peripherals configuration that workload has access to.
      &quot;metastoreService&quot;: &quot;A String&quot;, # Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
      &quot;sparkHistoryServerConfig&quot;: { # Spark History Server configuration for the workload. # Optional. The Spark History Server configuration for the workload.
        &quot;dataprocCluster&quot;: &quot;A String&quot;, # Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
      },
    },
  },
  &quot;labels&quot;: { # Optional. The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch.
    &quot;a_key&quot;: &quot;A String&quot;,
  },
  &quot;name&quot;: &quot;A String&quot;, # Output only. The resource name of the batch.
  &quot;operation&quot;: &quot;A String&quot;, # Output only. The resource name of the operation associated with this batch.
  &quot;pysparkBatch&quot;: { # A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload. # Optional. PySpark batch config.
    &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
      &quot;A String&quot;,
    ],
    &quot;args&quot;: [ # Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
      &quot;A String&quot;,
    ],
    &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
      &quot;A String&quot;,
    ],
    &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
      &quot;A String&quot;,
    ],
    &quot;mainPythonFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
    &quot;pythonFileUris&quot;: [ # Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
      &quot;A String&quot;,
    ],
  },
  &quot;runtimeConfig&quot;: { # Runtime configuration for a workload. # Optional. Runtime configuration for the batch execution.
    &quot;containerImage&quot;: &quot;A String&quot;, # Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
    &quot;properties&quot;: { # Optional. A mapping of property names to values, which are used to configure workload execution.
      &quot;a_key&quot;: &quot;A String&quot;,
    },
    &quot;version&quot;: &quot;A String&quot;, # Optional. Version of the batch runtime.
  },
  &quot;runtimeInfo&quot;: { # Runtime information about workload execution. # Output only. Runtime information about batch execution.
    &quot;approximateUsage&quot;: { # Usage metrics represent approximate total resources consumed by a workload. # Output only. Approximate workload resource usage calculated after workload finishes (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
      &quot;milliDcuSeconds&quot;: &quot;A String&quot;, # Optional. DCU (Dataproc Compute Units) usage in (milliDCU x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
      &quot;shuffleStorageGbSeconds&quot;: &quot;A String&quot;, # Optional. Shuffle storage usage in (GB x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
    },
    &quot;currentUsage&quot;: { # The usage snaphot represents the resources consumed by a workload at a specified time. # Output only. Snapshot of current workload resource usage.
      &quot;milliDcu&quot;: &quot;A String&quot;, # Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
      &quot;shuffleStorageGb&quot;: &quot;A String&quot;, # Optional. Shuffle Storage in gigabytes (GB). (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing))
      &quot;snapshotTime&quot;: &quot;A String&quot;, # Optional. The timestamp of the usage snapshot.
    },
    &quot;diagnosticOutputUri&quot;: &quot;A String&quot;, # Output only. A URI pointing to the location of the diagnostics tarball.
    &quot;endpoints&quot;: { # Output only. Map of remote access endpoints (such as web interfaces and APIs) to their URIs.
      &quot;a_key&quot;: &quot;A String&quot;,
    },
    &quot;outputUri&quot;: &quot;A String&quot;, # Output only. A URI pointing to the location of the stdout and stderr of the workload.
  },
  &quot;sparkBatch&quot;: { # A configuration for running an Apache Spark (https://spark.apache.org/) batch workload. # Optional. Spark batch config.
    &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
      &quot;A String&quot;,
    ],
    &quot;args&quot;: [ # Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
      &quot;A String&quot;,
    ],
    &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
      &quot;A String&quot;,
    ],
    &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
      &quot;A String&quot;,
    ],
    &quot;mainClass&quot;: &quot;A String&quot;, # Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
    &quot;mainJarFileUri&quot;: &quot;A String&quot;, # Optional. The HCFS URI of the jar file that contains the main class.
  },
  &quot;sparkRBatch&quot;: { # A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload. # Optional. SparkR batch config.
    &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
      &quot;A String&quot;,
    ],
    &quot;args&quot;: [ # Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
      &quot;A String&quot;,
    ],
    &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
      &quot;A String&quot;,
    ],
    &quot;mainRFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
  },
  &quot;sparkSqlBatch&quot;: { # A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload. # Optional. SparkSql batch config.
    &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
      &quot;A String&quot;,
    ],
    &quot;queryFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the script that contains Spark SQL queries to execute.
    &quot;queryVariables&quot;: { # Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name=&quot;value&quot;;).
      &quot;a_key&quot;: &quot;A String&quot;,
    },
  },
  &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the batch.
  &quot;stateHistory&quot;: [ # Output only. Historical state information for the batch.
    { # Historical state information.
      &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the batch at this point in history.
      &quot;stateMessage&quot;: &quot;A String&quot;, # Output only. Details about the state at this point in history.
      &quot;stateStartTime&quot;: &quot;A String&quot;, # Output only. The time when the batch entered the historical state.
    },
  ],
  &quot;stateMessage&quot;: &quot;A String&quot;, # Output only. Batch state details, such as a failure description if the state is FAILED.
  &quot;stateTime&quot;: &quot;A String&quot;, # Output only. The time when the batch entered a current state.
  &quot;uuid&quot;: &quot;A String&quot;, # Output only. A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
}

  batchId: string, Optional. The ID to use for the batch, which will become the final component of the batch&#x27;s resource name.This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
  requestId: string, Optional. A unique ID used to identify the request. If the service receives two CreateBatchRequest (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.CreateBatchRequest)s with the same request_id, the second request is ignored and the Operation that corresponds to the first Batch created and stored in the backend is returned.Recommendation: Set this value to a UUID (https://en.wikipedia.org/wiki/Universally_unique_identifier).The value must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). The maximum length is 40 characters.
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # This resource represents a long-running operation that is the result of a network API call.
  &quot;done&quot;: True or False, # If the value is false, it means the operation is still in progress. If true, the operation is completed, and either error or response is available.
  &quot;error&quot;: { # The Status type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by gRPC (https://github.com/grpc). Each Status message contains three pieces of data: error code, error message, and error details.You can find out more about this error model and how to work with it in the API Design Guide (https://cloud.google.com/apis/design/errors). # The error result of the operation in case of failure or cancellation.
    &quot;code&quot;: 42, # The status code, which should be an enum value of google.rpc.Code.
    &quot;details&quot;: [ # A list of messages that carry the error details. There is a common set of message types for APIs to use.
      {
        &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
      },
    ],
    &quot;message&quot;: &quot;A String&quot;, # A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
  },
  &quot;metadata&quot;: { # Service-specific metadata associated with the operation. It typically contains progress information and common metadata such as create time. Some services might not provide such metadata. Any method that returns a long-running operation should document the metadata type, if any.
    &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
  },
  &quot;name&quot;: &quot;A String&quot;, # The server-assigned name, which is only unique within the same service that originally returns it. If you use the default HTTP mapping, the name should be a resource name ending with operations/{unique_id}.
  &quot;response&quot;: { # The normal response of the operation in case of success. If the original method returns no data on success, such as Delete, the response is google.protobuf.Empty. If the original method is standard Get/Create/Update, the response should be the resource. For other methods, the response should have the type XxxResponse, where Xxx is the original method name. For example, if the original method name is TakeSnapshot(), the inferred response type is TakeSnapshotResponse.
    &quot;a_key&quot;: &quot;&quot;, # Properties of the object. Contains field @type with type URL.
  },
}</pre>
</div>

<div class="method">
    <code class="details" id="delete">delete(name, x__xgafv=None)</code>
  <pre>Deletes the batch workload resource. If the batch is not in terminal state, the delete fails and the response returns FAILED_PRECONDITION.

Args:
  name: string, Required. The fully qualified name of the batch to retrieve in the format &quot;projects/PROJECT_ID/locations/DATAPROC_REGION/batches/BATCH_ID&quot; (required)
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # A generic empty message that you can re-use to avoid defining duplicated empty messages in your APIs. A typical example is to use it as the request or the response type of an API method. For instance: service Foo { rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty); }
}</pre>
</div>

<div class="method">
    <code class="details" id="get">get(name, x__xgafv=None)</code>
  <pre>Gets the batch workload resource representation.

Args:
  name: string, Required. The fully qualified name of the batch to retrieve in the format &quot;projects/PROJECT_ID/locations/DATAPROC_REGION/batches/BATCH_ID&quot; (required)
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # A representation of a batch workload in the service.
  &quot;createTime&quot;: &quot;A String&quot;, # Output only. The time when the batch was created.
  &quot;creator&quot;: &quot;A String&quot;, # Output only. The email address of the user who created the batch.
  &quot;environmentConfig&quot;: { # Environment configuration for a workload. # Optional. Environment configuration for the batch execution.
    &quot;executionConfig&quot;: { # Execution configuration for a workload. # Optional. Execution configuration for a workload.
      &quot;idleTtl&quot;: &quot;A String&quot;, # Optional. The duration to keep the session alive while it&#x27;s idling. Passing this threshold will cause the session to be terminated. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified, the conditions are treated as and OR: the workload will be terminated when it has been idle for idle_ttl or when the ttl has passed, whichever comes first.
      &quot;kmsKey&quot;: &quot;A String&quot;, # Optional. The Cloud KMS key to use for encryption.
      &quot;networkTags&quot;: [ # Optional. Tags used for network traffic control.
        &quot;A String&quot;,
      ],
      &quot;networkUri&quot;: &quot;A String&quot;, # Optional. Network URI to connect workload to.
      &quot;serviceAccount&quot;: &quot;A String&quot;, # Optional. Service account that used to execute workload.
      &quot;stagingBucket&quot;: &quot;A String&quot;, # Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
      &quot;subnetworkUri&quot;: &quot;A String&quot;, # Optional. Subnetwork URI to connect workload to.
      &quot;ttl&quot;: &quot;A String&quot;, # Optional. The duration after which the workload will be terminated. When the workload passes this ttl, it will be unconditionally killed without waiting for ongoing work to finish. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified, the conditions are treated as and OR: the workload will be terminated when it has been idle for idle_ttl or when the ttl has passed, whichever comes first. If ttl is not specified for a session, it defaults to 24h.
    },
    &quot;peripheralsConfig&quot;: { # Auxiliary services configuration for a workload. # Optional. Peripherals configuration that workload has access to.
      &quot;metastoreService&quot;: &quot;A String&quot;, # Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
      &quot;sparkHistoryServerConfig&quot;: { # Spark History Server configuration for the workload. # Optional. The Spark History Server configuration for the workload.
        &quot;dataprocCluster&quot;: &quot;A String&quot;, # Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
      },
    },
  },
  &quot;labels&quot;: { # Optional. The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch.
    &quot;a_key&quot;: &quot;A String&quot;,
  },
  &quot;name&quot;: &quot;A String&quot;, # Output only. The resource name of the batch.
  &quot;operation&quot;: &quot;A String&quot;, # Output only. The resource name of the operation associated with this batch.
  &quot;pysparkBatch&quot;: { # A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload. # Optional. PySpark batch config.
    &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
      &quot;A String&quot;,
    ],
    &quot;args&quot;: [ # Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
      &quot;A String&quot;,
    ],
    &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
      &quot;A String&quot;,
    ],
    &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
      &quot;A String&quot;,
    ],
    &quot;mainPythonFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
    &quot;pythonFileUris&quot;: [ # Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
      &quot;A String&quot;,
    ],
  },
  &quot;runtimeConfig&quot;: { # Runtime configuration for a workload. # Optional. Runtime configuration for the batch execution.
    &quot;containerImage&quot;: &quot;A String&quot;, # Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
    &quot;properties&quot;: { # Optional. A mapping of property names to values, which are used to configure workload execution.
      &quot;a_key&quot;: &quot;A String&quot;,
    },
    &quot;version&quot;: &quot;A String&quot;, # Optional. Version of the batch runtime.
  },
  &quot;runtimeInfo&quot;: { # Runtime information about workload execution. # Output only. Runtime information about batch execution.
    &quot;approximateUsage&quot;: { # Usage metrics represent approximate total resources consumed by a workload. # Output only. Approximate workload resource usage calculated after workload finishes (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
      &quot;milliDcuSeconds&quot;: &quot;A String&quot;, # Optional. DCU (Dataproc Compute Units) usage in (milliDCU x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
      &quot;shuffleStorageGbSeconds&quot;: &quot;A String&quot;, # Optional. Shuffle storage usage in (GB x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
    },
    &quot;currentUsage&quot;: { # The usage snaphot represents the resources consumed by a workload at a specified time. # Output only. Snapshot of current workload resource usage.
      &quot;milliDcu&quot;: &quot;A String&quot;, # Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
      &quot;shuffleStorageGb&quot;: &quot;A String&quot;, # Optional. Shuffle Storage in gigabytes (GB). (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing))
      &quot;snapshotTime&quot;: &quot;A String&quot;, # Optional. The timestamp of the usage snapshot.
    },
    &quot;diagnosticOutputUri&quot;: &quot;A String&quot;, # Output only. A URI pointing to the location of the diagnostics tarball.
    &quot;endpoints&quot;: { # Output only. Map of remote access endpoints (such as web interfaces and APIs) to their URIs.
      &quot;a_key&quot;: &quot;A String&quot;,
    },
    &quot;outputUri&quot;: &quot;A String&quot;, # Output only. A URI pointing to the location of the stdout and stderr of the workload.
  },
  &quot;sparkBatch&quot;: { # A configuration for running an Apache Spark (https://spark.apache.org/) batch workload. # Optional. Spark batch config.
    &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
      &quot;A String&quot;,
    ],
    &quot;args&quot;: [ # Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
      &quot;A String&quot;,
    ],
    &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
      &quot;A String&quot;,
    ],
    &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
      &quot;A String&quot;,
    ],
    &quot;mainClass&quot;: &quot;A String&quot;, # Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
    &quot;mainJarFileUri&quot;: &quot;A String&quot;, # Optional. The HCFS URI of the jar file that contains the main class.
  },
  &quot;sparkRBatch&quot;: { # A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload. # Optional. SparkR batch config.
    &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
      &quot;A String&quot;,
    ],
    &quot;args&quot;: [ # Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
      &quot;A String&quot;,
    ],
    &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
      &quot;A String&quot;,
    ],
    &quot;mainRFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
  },
  &quot;sparkSqlBatch&quot;: { # A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload. # Optional. SparkSql batch config.
    &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
      &quot;A String&quot;,
    ],
    &quot;queryFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the script that contains Spark SQL queries to execute.
    &quot;queryVariables&quot;: { # Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name=&quot;value&quot;;).
      &quot;a_key&quot;: &quot;A String&quot;,
    },
  },
  &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the batch.
  &quot;stateHistory&quot;: [ # Output only. Historical state information for the batch.
    { # Historical state information.
      &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the batch at this point in history.
      &quot;stateMessage&quot;: &quot;A String&quot;, # Output only. Details about the state at this point in history.
      &quot;stateStartTime&quot;: &quot;A String&quot;, # Output only. The time when the batch entered the historical state.
    },
  ],
  &quot;stateMessage&quot;: &quot;A String&quot;, # Output only. Batch state details, such as a failure description if the state is FAILED.
  &quot;stateTime&quot;: &quot;A String&quot;, # Output only. The time when the batch entered a current state.
  &quot;uuid&quot;: &quot;A String&quot;, # Output only. A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
}</pre>
</div>

<div class="method">
    <code class="details" id="list">list(parent, filter=None, orderBy=None, pageSize=None, pageToken=None, x__xgafv=None)</code>
  <pre>Lists batch workloads.

Args:
  parent: string, Required. The parent, which owns this collection of batches. (required)
  filter: string, Optional. A filter for the batches to return in the response.A filter is a logical expression constraining the values of various fields in each batch resource. Filters are case sensitive, and may contain multiple clauses combined with logical operators (AND/OR). Supported fields are batch_id, batch_uuid, state, and create_time.e.g. state = RUNNING and create_time &lt; &quot;2023-01-01T00:00:00Z&quot; filters for batches in state RUNNING that were created before 2023-01-01See https://google.aip.dev/assets/misc/ebnf-filtering.txt for a detailed description of the filter syntax and a list of supported comparisons.
  orderBy: string, Optional. Field(s) on which to sort the list of batches.Currently the only supported sort orders are unspecified (empty) and create_time desc to sort by most recently created batches first.See https://google.aip.dev/132#ordering for more details.
  pageSize: integer, Optional. The maximum number of batches to return in each response. The service may return fewer than this value. The default page size is 20; the maximum page size is 1000.
  pageToken: string, Optional. A page token received from a previous ListBatches call. Provide this token to retrieve the subsequent page.
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # A list of batch workloads.
  &quot;batches&quot;: [ # The batches from the specified collection.
    { # A representation of a batch workload in the service.
      &quot;createTime&quot;: &quot;A String&quot;, # Output only. The time when the batch was created.
      &quot;creator&quot;: &quot;A String&quot;, # Output only. The email address of the user who created the batch.
      &quot;environmentConfig&quot;: { # Environment configuration for a workload. # Optional. Environment configuration for the batch execution.
        &quot;executionConfig&quot;: { # Execution configuration for a workload. # Optional. Execution configuration for a workload.
          &quot;idleTtl&quot;: &quot;A String&quot;, # Optional. The duration to keep the session alive while it&#x27;s idling. Passing this threshold will cause the session to be terminated. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified, the conditions are treated as and OR: the workload will be terminated when it has been idle for idle_ttl or when the ttl has passed, whichever comes first.
          &quot;kmsKey&quot;: &quot;A String&quot;, # Optional. The Cloud KMS key to use for encryption.
          &quot;networkTags&quot;: [ # Optional. Tags used for network traffic control.
            &quot;A String&quot;,
          ],
          &quot;networkUri&quot;: &quot;A String&quot;, # Optional. Network URI to connect workload to.
          &quot;serviceAccount&quot;: &quot;A String&quot;, # Optional. Service account that used to execute workload.
          &quot;stagingBucket&quot;: &quot;A String&quot;, # Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
          &quot;subnetworkUri&quot;: &quot;A String&quot;, # Optional. Subnetwork URI to connect workload to.
          &quot;ttl&quot;: &quot;A String&quot;, # Optional. The duration after which the workload will be terminated. When the workload passes this ttl, it will be unconditionally killed without waiting for ongoing work to finish. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified, the conditions are treated as and OR: the workload will be terminated when it has been idle for idle_ttl or when the ttl has passed, whichever comes first. If ttl is not specified for a session, it defaults to 24h.
        },
        &quot;peripheralsConfig&quot;: { # Auxiliary services configuration for a workload. # Optional. Peripherals configuration that workload has access to.
          &quot;metastoreService&quot;: &quot;A String&quot;, # Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
          &quot;sparkHistoryServerConfig&quot;: { # Spark History Server configuration for the workload. # Optional. The Spark History Server configuration for the workload.
            &quot;dataprocCluster&quot;: &quot;A String&quot;, # Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
          },
        },
      },
      &quot;labels&quot;: { # Optional. The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch.
        &quot;a_key&quot;: &quot;A String&quot;,
      },
      &quot;name&quot;: &quot;A String&quot;, # Output only. The resource name of the batch.
      &quot;operation&quot;: &quot;A String&quot;, # Output only. The resource name of the operation associated with this batch.
      &quot;pysparkBatch&quot;: { # A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload. # Optional. PySpark batch config.
        &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
          &quot;A String&quot;,
        ],
        &quot;args&quot;: [ # Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
          &quot;A String&quot;,
        ],
        &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
          &quot;A String&quot;,
        ],
        &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
          &quot;A String&quot;,
        ],
        &quot;mainPythonFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
        &quot;pythonFileUris&quot;: [ # Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
          &quot;A String&quot;,
        ],
      },
      &quot;runtimeConfig&quot;: { # Runtime configuration for a workload. # Optional. Runtime configuration for the batch execution.
        &quot;containerImage&quot;: &quot;A String&quot;, # Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
        &quot;properties&quot;: { # Optional. A mapping of property names to values, which are used to configure workload execution.
          &quot;a_key&quot;: &quot;A String&quot;,
        },
        &quot;version&quot;: &quot;A String&quot;, # Optional. Version of the batch runtime.
      },
      &quot;runtimeInfo&quot;: { # Runtime information about workload execution. # Output only. Runtime information about batch execution.
        &quot;approximateUsage&quot;: { # Usage metrics represent approximate total resources consumed by a workload. # Output only. Approximate workload resource usage calculated after workload finishes (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
          &quot;milliDcuSeconds&quot;: &quot;A String&quot;, # Optional. DCU (Dataproc Compute Units) usage in (milliDCU x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
          &quot;shuffleStorageGbSeconds&quot;: &quot;A String&quot;, # Optional. Shuffle storage usage in (GB x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
        },
        &quot;currentUsage&quot;: { # The usage snaphot represents the resources consumed by a workload at a specified time. # Output only. Snapshot of current workload resource usage.
          &quot;milliDcu&quot;: &quot;A String&quot;, # Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
          &quot;shuffleStorageGb&quot;: &quot;A String&quot;, # Optional. Shuffle Storage in gigabytes (GB). (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing))
          &quot;snapshotTime&quot;: &quot;A String&quot;, # Optional. The timestamp of the usage snapshot.
        },
        &quot;diagnosticOutputUri&quot;: &quot;A String&quot;, # Output only. A URI pointing to the location of the diagnostics tarball.
        &quot;endpoints&quot;: { # Output only. Map of remote access endpoints (such as web interfaces and APIs) to their URIs.
          &quot;a_key&quot;: &quot;A String&quot;,
        },
        &quot;outputUri&quot;: &quot;A String&quot;, # Output only. A URI pointing to the location of the stdout and stderr of the workload.
      },
      &quot;sparkBatch&quot;: { # A configuration for running an Apache Spark (https://spark.apache.org/) batch workload. # Optional. Spark batch config.
        &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
          &quot;A String&quot;,
        ],
        &quot;args&quot;: [ # Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
          &quot;A String&quot;,
        ],
        &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
          &quot;A String&quot;,
        ],
        &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
          &quot;A String&quot;,
        ],
        &quot;mainClass&quot;: &quot;A String&quot;, # Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
        &quot;mainJarFileUri&quot;: &quot;A String&quot;, # Optional. The HCFS URI of the jar file that contains the main class.
      },
      &quot;sparkRBatch&quot;: { # A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload. # Optional. SparkR batch config.
        &quot;archiveUris&quot;: [ # Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
          &quot;A String&quot;,
        ],
        &quot;args&quot;: [ # Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
          &quot;A String&quot;,
        ],
        &quot;fileUris&quot;: [ # Optional. HCFS URIs of files to be placed in the working directory of each executor.
          &quot;A String&quot;,
        ],
        &quot;mainRFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
      },
      &quot;sparkSqlBatch&quot;: { # A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload. # Optional. SparkSql batch config.
        &quot;jarFileUris&quot;: [ # Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
          &quot;A String&quot;,
        ],
        &quot;queryFileUri&quot;: &quot;A String&quot;, # Required. The HCFS URI of the script that contains Spark SQL queries to execute.
        &quot;queryVariables&quot;: { # Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name=&quot;value&quot;;).
          &quot;a_key&quot;: &quot;A String&quot;,
        },
      },
      &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the batch.
      &quot;stateHistory&quot;: [ # Output only. Historical state information for the batch.
        { # Historical state information.
          &quot;state&quot;: &quot;A String&quot;, # Output only. The state of the batch at this point in history.
          &quot;stateMessage&quot;: &quot;A String&quot;, # Output only. Details about the state at this point in history.
          &quot;stateStartTime&quot;: &quot;A String&quot;, # Output only. The time when the batch entered the historical state.
        },
      ],
      &quot;stateMessage&quot;: &quot;A String&quot;, # Output only. Batch state details, such as a failure description if the state is FAILED.
      &quot;stateTime&quot;: &quot;A String&quot;, # Output only. The time when the batch entered a current state.
      &quot;uuid&quot;: &quot;A String&quot;, # Output only. A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
    },
  ],
  &quot;nextPageToken&quot;: &quot;A String&quot;, # A token, which can be sent as page_token to retrieve the next page. If this field is omitted, there are no subsequent pages.
}</pre>
</div>

<div class="method">
    <code class="details" id="list_next">list_next()</code>
  <pre>Retrieves the next page of results.

        Args:
          previous_request: The request for the previous page. (required)
          previous_response: The response from the request for the previous page. (required)

        Returns:
          A request object that you can call &#x27;execute()&#x27; on to request the next
          page. Returns None if there are no more items in the collection.
        </pre>
</div>

</body></html>